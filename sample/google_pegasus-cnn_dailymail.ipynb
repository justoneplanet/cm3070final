{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[\"California's largest electricity provider has turned off power to hundreds of thousands of customers.\"]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "@see https://huggingface.co/docs/transformers/main/model_doc/pegasus\n",
        "\"\"\"\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "\n",
        "src_text = [\n",
        "    \"\"\"PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\"\"\n",
        "]\n",
        "\n",
        "if torch.cuda.is_available(): # for NVIDIA GPU\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available(): #  for Apple Silicon\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "batch = tokenizer(\n",
        "    src_text,\n",
        "    truncation=True,\n",
        "    padding=\"longest\",\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "translated = model.generate(**batch)\n",
        "tgt_text = tokenizer.batch_decode(\n",
        "    translated,\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "tgt_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "s_qNSzzyaCbD"
      ],
      "name": "transformer.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
