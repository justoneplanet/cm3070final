{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras-nlp/example/machine_translation\n",
    "\n",
    "- https://github.com/keras-team/keras-nlp/tree/master/examples/machine_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.15.1\n",
      "  Using cached tensorflow-2.15.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting tensorflow-macos==2.15.1\n",
      "  Using cached tensorflow_macos-2.15.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.4 kB)\n",
      "Collecting keras==2.15.0\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (16.0.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.3.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.26.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (4.11.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.56.0)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.1)\n",
      "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.19.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.30.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n",
      "Using cached tensorflow-2.15.1-cp310-cp310-macosx_12_0_arm64.whl (205.7 MB)\n",
      "Using cached tensorflow_macos-2.15.1-cp310-cp310-macosx_12_0_arm64.whl (2.2 kB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Installing collected packages: keras, tensorboard, tensorflow, tensorflow-macos\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.4.1\n",
      "    Uninstalling keras-3.4.1:\n",
      "      Successfully uninstalled keras-3.4.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.16.2\n",
      "    Uninstalling tensorflow-2.16.2:\n",
      "      Successfully uninstalled tensorflow-2.16.2\n",
      "  Attempting uninstall: tensorflow-macos\n",
      "    Found existing installation: tensorflow-macos 2.16.2\n",
      "    Uninstalling tensorflow-macos-2.16.2:\n",
      "      Successfully uninstalled tensorflow-macos-2.16.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.15.0 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-macos-2.15.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# The hyperparameters, especially of the optimizer, seem picky.\n",
    "# Currently, only the following versions go well only on mac with RMSprop.\n",
    "%pip install -U tensorflow==2.15.1 tensorflow-macos==2.15.1 keras==2.15.0\n",
    "#%pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version can not use keras.optimizers.legacy.RMSprop.\n",
    "# And the model does not converge.\n",
    "%pip install -U tensorflow==2.16.2 tensorflow-macos==2.16.2 keras==3.4.1\n",
    "#%pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "2.15.1\n",
      "2.15.0\n",
      "0.12.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "def is_running_on_apple_sillicon():\n",
    "    return platform.system() == \"Darwin\" and platform.processor() == \"arm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker cannot use @keras.saving\n",
    "from keras import saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def download_data():\n",
    "    text_file = keras.utils.get_file(\n",
    "        fname=\"spa-eng.zip\",\n",
    "        origin=(\n",
    "            \"http://storage.googleapis.com/download.tensorflow.org/data/\"\n",
    "            + \"spa-eng.zip\"\n",
    "        ),\n",
    "        extract=True,\n",
    "    )\n",
    "    return pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n",
    "\n",
    "\n",
    "def read_data(filepath):\n",
    "    with open(filepath) as f:\n",
    "        lines = f.read().split(\"\\n\")[:-1]\n",
    "        text_pairs = []\n",
    "        for line in lines:\n",
    "            eng, spa = line.split(\"\\t\")\n",
    "            spa = \"[start] \" + spa + \" [end]\"\n",
    "            text_pairs.append((eng, spa))\n",
    "    return text_pairs\n",
    "\n",
    "\n",
    "def split_train_val_test(text_pairs):\n",
    "    random.shuffle(text_pairs)\n",
    "    num_val_samples = int(0.15 * len(text_pairs))\n",
    "    num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "    train_pairs = text_pairs[:num_train_samples]\n",
    "    val_end_index = num_train_samples + num_val_samples\n",
    "    val_pairs = text_pairs[num_train_samples:val_end_index]\n",
    "    test_pairs = text_pairs[val_end_index:]\n",
    "    return train_pairs, val_pairs, test_pairs\n",
    "\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase,\n",
    "        \"[%s]\" % re.escape(strip_chars),\n",
    "        \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_tokenizer(train_pairs, sequence_length, vocab_size):\n",
    "    \"\"\"Preapare English and Spanish tokenizer.\"\"\"\n",
    "    eng_tokenizer = keras.layers.TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=sequence_length,\n",
    "    )\n",
    "    spa_tokenizer = keras.layers.TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=sequence_length + 1,\n",
    "        standardize=custom_standardization,\n",
    "    )\n",
    "    eng_texts, spa_texts = zip(*train_pairs)\n",
    "    eng_tokenizer.adapt(eng_texts)\n",
    "    spa_tokenizer.adapt(spa_texts)\n",
    "    return eng_tokenizer, spa_tokenizer\n",
    "\n",
    "\n",
    "def prepare_datasets(text_pairs, batch_size, eng_tokenizer, spa_tokenizer):\n",
    "    \"\"\"Transform raw text pairs to tf datasets.\"\"\"\n",
    "    eng_texts, spa_texts = zip(*text_pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "\n",
    "    def format_dataset(eng, spa):\n",
    "        \"\"\"Format the dataset given input English and Spanish text.\n",
    "\n",
    "        The output format is:\n",
    "            x: a pair of English and Spanish sentence.\n",
    "            y: The Spanish sentence in x shifts 1 token towards right, because\n",
    "                we are predicting the next token.\n",
    "        \"\"\"\n",
    "        eng = eng_tokenizer(eng)\n",
    "        spa = spa_tokenizer(spa)\n",
    "        return (\n",
    "            {\n",
    "                \"encoder_inputs\": eng,\n",
    "                \"decoder_inputs\": spa[:, :-1],\n",
    "            },\n",
    "            spa[:, 1:],\n",
    "            tf.cast((spa[:, 1:] != 0), \"float32\"),  # mask as sample weights\n",
    "        )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "\n",
    "def get_dataset_and_tokenizer(sequence_length, vocab_size, batch_size):\n",
    "    \"\"\"Main method to get the formatted machine translation dataset.\"\"\"\n",
    "    filepath = download_data()\n",
    "    text_pairs = read_data(filepath)\n",
    "    train_pairs, val_pairs, test_pairs = split_train_val_test(text_pairs)\n",
    "    eng_tokenizer, spa_tokenizer = prepare_tokenizer(\n",
    "        train_pairs, sequence_length, vocab_size\n",
    "    )\n",
    "    train_ds = prepare_datasets(\n",
    "        train_pairs,\n",
    "        batch_size,\n",
    "        eng_tokenizer,\n",
    "        spa_tokenizer,\n",
    "    )\n",
    "    val_ds = prepare_datasets(\n",
    "        val_pairs,\n",
    "        batch_size,\n",
    "        eng_tokenizer,\n",
    "        spa_tokenizer,\n",
    "    )\n",
    "    test_ds = prepare_datasets(\n",
    "        test_pairs,\n",
    "        batch_size,\n",
    "        eng_tokenizer,\n",
    "        spa_tokenizer,\n",
    "    )\n",
    "    return (train_ds, val_ds, test_ds), (eng_tokenizer, spa_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "class TranslationModel(keras.Model):\n",
    "    \"\"\"The machine translation model.\n",
    "\n",
    "    The model is an encoder-decoder structure model. The encoder is a stack of\n",
    "    `keras_nlp.TransformerEncoder`, and the decoder is a stack of\n",
    "    `keras_nlp.TransformerDecoder`. We also pass in the tokenizer for encoder\n",
    "    and decoder so that during save/load, the tokenizer is also kept.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_tokenizer,\n",
    "        decoder_tokenizer,\n",
    "        num_encoders,\n",
    "        num_decoders,\n",
    "        num_heads,\n",
    "        transformer_intermediate_dim,\n",
    "        encoder_vocabulary_size,\n",
    "        decoder_vocabulary_size,\n",
    "        embed_dim,\n",
    "        sequence_length,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoders = []\n",
    "        self.decoders = []\n",
    "        for _ in range(num_encoders):\n",
    "            self.encoders.append(\n",
    "                keras_nlp.layers.TransformerEncoder(\n",
    "                    num_heads=num_heads,\n",
    "                    intermediate_dim=transformer_intermediate_dim,\n",
    "                )\n",
    "            )\n",
    "        for _ in range(num_decoders):\n",
    "            self.decoders.append(\n",
    "                keras_nlp.layers.TransformerDecoder(\n",
    "                    num_heads=num_heads,\n",
    "                    intermediate_dim=transformer_intermediate_dim,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.encoder_tokenizer = encoder_tokenizer\n",
    "        self.decoder_tokenizer = decoder_tokenizer\n",
    "\n",
    "        self.encoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "            vocabulary_size=encoder_vocabulary_size,\n",
    "            sequence_length=sequence_length,\n",
    "            embedding_dim=embed_dim,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "\n",
    "        self.decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "            vocabulary_size=decoder_vocabulary_size,\n",
    "            sequence_length=sequence_length,\n",
    "            embedding_dim=embed_dim,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "\n",
    "        self.dense = keras.layers.Dense(\n",
    "            decoder_vocabulary_size,\n",
    "            activation=\"softmax\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_input, decoder_input = (\n",
    "            inputs[\"encoder_inputs\"],\n",
    "            inputs[\"decoder_inputs\"],\n",
    "        )\n",
    "        encoded = self.encoder_embedding(encoder_input)\n",
    "        for encoder in self.encoders:\n",
    "            encoded = encoder(inputs=encoded)\n",
    "\n",
    "        decoded = self.decoder_embedding(decoder_input)\n",
    "        for decoder in self.decoders:\n",
    "            decoded = decoder(\n",
    "                decoder_sequence=decoded,\n",
    "                encoder_sequence=encoded,\n",
    "                use_causal_mask=True,\n",
    "            )\n",
    "\n",
    "        output = self.dense(decoded)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            \"encoder_tokenizer\": self.encoder_tokenizer.get_config(),\n",
    "            \"decoder_tokenizer\": self.decoder_tokenizer.get_config(),\n",
    "            \"num_encoders\": len(self.encoders),\n",
    "            \"num_decoders\": len(self.decoders),\n",
    "            \"num_heads\": self.encoders[0].num_heads,\n",
    "            \"transformer_intermediate_dim\": self.encoders[0].intermediate_dim,\n",
    "            \"encoder_vocabulary_size\": self.encoder_embedding.vocabulary_size,\n",
    "            \"decoder_vocabulary_size\": self.decoder_embedding.vocabulary_size,\n",
    "            \"embedding_dim\": self.encoder_embedding.embedding_dim,\n",
    "            \"sequence_length\": self.encoder_embedding.sequence_length,\n",
    "        })\n",
    "        return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        encoder_tokenizer_config = config.pop(\"encoder_tokenizer\")\n",
    "        decoder_tokenizer_config = config.pop(\"decoder_tokenizer\")\n",
    "        encoder_tokenizer = keras.layers.TextVectorization.from_config(encoder_tokenizer_config)\n",
    "        decoder_tokenizer = keras.layers.TextVectorization.from_config(decoder_tokenizer_config)\n",
    "        return cls(encoder_tokenizer=encoder_tokenizer, decoder_tokenizer=decoder_tokenizer, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "        model,\n",
    "        train_ds,\n",
    "        val_ds,\n",
    "        initial_learning_rate,\n",
    "        decay_steps,\n",
    "        decay_rate,\n",
    "        epochs,\n",
    "        steps_per_epoch):\n",
    "    if decay_rate < 1.0:\n",
    "        learning_rate = keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate=initial_learning_rate,\n",
    "            decay_steps=decay_steps,\n",
    "            decay_rate=decay_rate,\n",
    "        )\n",
    "    else:\n",
    "        learning_rate = initial_learning_rate\n",
    "    if f\"{keras.__version__}\".startswith(\"2.\") and is_running_on_apple_sillicon():\n",
    "        optimizer = keras.optimizers.legacy.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "        )\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "        )\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "        reduction=keras.losses.Reduction.NONE\n",
    "    )\n",
    "    metrics = [\n",
    "        keras.metrics.SparseCategoricalAccuracy(),\n",
    "        #keras.metrics.SparseCategoricalCrossentropy(),\n",
    "        #keras_nlp.metrics.Bleu(), #  This cannot be used here\n",
    "    ]\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics,\n",
    "        loss=loss_fn,\n",
    "        weighted_metrics=[],\n",
    "    )\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "        num_encoders,\n",
    "        num_decoders,\n",
    "        num_heads,\n",
    "        transformer_intermediate_dim,\n",
    "        sequence_length,\n",
    "        vocab_size,\n",
    "        batch_size,\n",
    "        embed_dim,\n",
    "        initial_learning_rate,\n",
    "        decay_steps,\n",
    "        decay_rate,\n",
    "        epochs,\n",
    "        steps_per_epoch):\n",
    "    (\n",
    "        (train_ds, val_ds, test_ds),\n",
    "        (\n",
    "            eng_tokenizer,\n",
    "            spa_tokenizer,\n",
    "        ),\n",
    "    ) = get_dataset_and_tokenizer(\n",
    "        sequence_length, vocab_size, batch_size\n",
    "    )\n",
    "    english_vocab_size = eng_tokenizer.vocabulary_size()\n",
    "    spanish_vocab_size = spa_tokenizer.vocabulary_size()\n",
    "    model = TranslationModel(\n",
    "        encoder_tokenizer=eng_tokenizer,\n",
    "        decoder_tokenizer=spa_tokenizer,\n",
    "        num_encoders=num_encoders,\n",
    "        num_decoders=num_decoders,\n",
    "        num_heads=num_heads,\n",
    "        transformer_intermediate_dim=transformer_intermediate_dim,\n",
    "        encoder_vocabulary_size=english_vocab_size,\n",
    "        decoder_vocabulary_size=spanish_vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        sequence_length=sequence_length,\n",
    "    )\n",
    "\n",
    "    run_training(\n",
    "        model,\n",
    "        train_ds,\n",
    "        val_ds,\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "    )\n",
    "\n",
    "    if is_running_on_apple_sillicon():\n",
    "        filepath = 'machine_translation_model.keras/machine_translation_model'\n",
    "    else:\n",
    "        filepath = 'machine_translation_model.keras'\n",
    "    print(f\"Saving to {filepath}\")\n",
    "    model.save(filepath=filepath)\n",
    "\n",
    "    print(f\"Successfully saved model to {filepath}\")\n",
    "    return model, filepath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "EXAMPLES = [\n",
    "    (\n",
    "        \"Tom doesn't listen to anyone.\",\n",
    "        \"[start] Tomás no escucha a nadie. [end]\",\n",
    "    ),\n",
    "    (\"I got soaked to the skin.\", \"[start] Estoy chorreando. [end]\"),\n",
    "    (\"I imagined that.\", \"[start] Me imaginé eso. [end]\"),\n",
    "    (\"The baby is crying.\", \"[start] El bebé está llorando. [end]\"),\n",
    "    (\n",
    "        \"I've never felt so exhilarated.\",\n",
    "        \"[start] Nunca me he sentido tan animado. [end]\",\n",
    "    ),\n",
    "    (\n",
    "        \"Please forgive me for not having written sooner.\",\n",
    "        \"[start] Perdóname por no haberte escrito antes, por favor. [end]\",\n",
    "    ),\n",
    "    (\"I expected more from you.\", \"[start] Esperaba más de vos. [end]\"),\n",
    "    (\"I have a computer.\", \"[start] Tengo un computador. [end]\"),\n",
    "    (\"Dinner's ready!\", \"[start] ¡La cena está lista! [end]\"),\n",
    "    (\"Let me finish.\", \"[start] Déjame terminar. [end]\"),\n",
    "    (\"I trust her.\", \"[start] Yo confío en ella. [end]\"),\n",
    "    (\"I trust him.\", \"[start] Yo confío en él. [end]\"),\n",
    "]\n",
    "\n",
    "def decode_sequence(input_sentence, model, max_sequence_length, lookup_table):\n",
    "    encoder_tokenizer = model.encoder_tokenizer\n",
    "    decoder_tokenizer = model.decoder_tokenizer\n",
    "    tokenized_input = encoder_tokenizer([input_sentence])\n",
    "\n",
    "    start_token = decoder_tokenizer(\"[start]\")[0].numpy()\n",
    "    end_token = decoder_tokenizer(\"[end]\")[0].numpy()\n",
    "\n",
    "    decoded_sentence = [start_token]\n",
    "    for i in range(max_sequence_length):\n",
    "        decoder_inputs = tf.convert_to_tensor(\n",
    "            [decoded_sentence],\n",
    "            dtype=\"int64\",\n",
    "        )\n",
    "        decoder_inputs = tf.concat(\n",
    "            [\n",
    "                decoder_inputs,\n",
    "                tf.zeros(\n",
    "                    [1, max_sequence_length - i - 1],\n",
    "                    dtype=\"int64\",\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        input = {\n",
    "            \"encoder_inputs\": tokenized_input,\n",
    "            \"decoder_inputs\": decoder_inputs,\n",
    "        }\n",
    "        predictions = model(input)\n",
    "        predicted_token = np.argmax(predictions[0, i, :])\n",
    "        decoded_sentence.append(predicted_token)\n",
    "        if predicted_token == end_token:\n",
    "            break\n",
    "\n",
    "    detokenized_output = []\n",
    "    for token in decoded_sentence:\n",
    "        detokenized_output.append(lookup_table[token])\n",
    "    return \" \".join(detokenized_output)\n",
    "\n",
    "\n",
    "def predict_main(filepath, examples, sequence_length):\n",
    "    loaded_model = keras.models.load_model(filepath)\n",
    "\n",
    "    decoder_tokenizer = loaded_model.decoder_tokenizer\n",
    "    vocab = decoder_tokenizer.get_vocabulary()\n",
    "    index_lookup_table = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "    translated = []\n",
    "    for example in examples:\n",
    "        translated.append(\n",
    "            decode_sequence(\n",
    "                example[0],\n",
    "                loaded_model,\n",
    "                sequence_length,\n",
    "                index_lookup_table,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        print(\"ENGLISH SENTENCE: \", examples[i][0])\n",
    "        print(\"MACHINE TRANSLATED RESULT: \", translated[i])\n",
    "        print(\"GOLDEN: \", examples[i][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default params\n",
    "FLAGS_learning_rate = 0.001\n",
    "FLAGS_num_epochs = 50 # default=1, but too small\n",
    "FLAGS_steps_per_epoch = None\n",
    "FLAGS_sequence_length = 20 # Input and output sequence length.\n",
    "FLAGS_vocab_size = 15000 # Vocabulary size, required by tokenizer.\n",
    "FLAGS_batch_size = 64\n",
    "FLAGS_num_encoders = 2\n",
    "FLAGS_num_decoders = 2\n",
    "FLAGS_num_heads = 8 # Number of head of the multihead attention.\n",
    "FLAGS_intermediate_dim = 128 # Intermediate dimension (feedforward network) of transformer.\n",
    "FLAGS_model_dim = 64\n",
    "FLAGS_decay_steps = 20\n",
    "FLAGS_decay_rate = 0.98\n",
    "\n",
    "model, filepath = build_model(\n",
    "    num_encoders=FLAGS_num_encoders,\n",
    "    num_decoders=FLAGS_num_decoders,\n",
    "    num_heads=FLAGS_num_heads,\n",
    "    transformer_intermediate_dim=FLAGS_intermediate_dim,\n",
    "    sequence_length=FLAGS_sequence_length,\n",
    "    vocab_size=FLAGS_vocab_size,\n",
    "    batch_size=FLAGS_batch_size,\n",
    "    embed_dim=FLAGS_model_dim,\n",
    "    initial_learning_rate=FLAGS_learning_rate,\n",
    "    decay_steps=FLAGS_decay_steps,\n",
    "    decay_rate=FLAGS_decay_rate,\n",
    "    epochs=FLAGS_num_epochs,\n",
    "    steps_per_epoch=FLAGS_steps_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_main(\n",
    "    filepath=filepath,\n",
    "    examples=EXAMPLES,\n",
    "    sequence_length=FLAGS_sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 13:05:46.260740: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-07-01 13:05:46.260765: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-07-01 13:05:46.260773: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-07-01 13:05:46.260804: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-07-01 13:05:46.260820: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-07-01 13:05:46.604499: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 77s 55ms/step - loss: 1.4819 - sparse_categorical_accuracy: 0.3234 - val_loss: 0.9888 - val_sparse_categorical_accuracy: 0.4646\n",
      "Saving to machine_translation_model.keras/machine_translation_model\n",
      "INFO:tensorflow:Assets written to: machine_translation_model.keras/machine_translation_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: machine_translation_model.keras/machine_translation_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model to machine_translation_model.keras/machine_translation_model\n"
     ]
    }
   ],
   "source": [
    "# minimum viable params\n",
    "# Epoch 24/50 - loss: 0.1224 - sparse_categorical_accuracy: 0.8007 - val_loss: 0.8010 - val_sparse_categorical_accuracy: 0.5755\n",
    "FLAGS_learning_rate = 0.001\n",
    "FLAGS_num_epochs = 50\n",
    "FLAGS_steps_per_epoch = None\n",
    "FLAGS_sequence_length = 20\n",
    "FLAGS_vocab_size = 15000\n",
    "FLAGS_batch_size = 64\n",
    "FLAGS_num_encoders = 1\n",
    "FLAGS_num_decoders = 1\n",
    "FLAGS_num_heads = 6\n",
    "FLAGS_intermediate_dim = 512\n",
    "FLAGS_model_dim = 64\n",
    "FLAGS_decay_steps = 100\n",
    "FLAGS_decay_rate = 0.99\n",
    "\n",
    "model, filepath = build_model(\n",
    "    num_encoders=FLAGS_num_encoders,\n",
    "    num_decoders=FLAGS_num_decoders,\n",
    "    num_heads=FLAGS_num_heads,\n",
    "    transformer_intermediate_dim=FLAGS_intermediate_dim,\n",
    "    sequence_length=FLAGS_sequence_length,\n",
    "    vocab_size=FLAGS_vocab_size,\n",
    "    batch_size=FLAGS_batch_size,\n",
    "    embed_dim=FLAGS_model_dim,\n",
    "    initial_learning_rate=FLAGS_learning_rate,\n",
    "    decay_steps=FLAGS_decay_steps,\n",
    "    decay_rate=FLAGS_decay_rate,\n",
    "    epochs=FLAGS_num_epochs,\n",
    "    steps_per_epoch=FLAGS_steps_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH SENTENCE:  Tom doesn't listen to anyone.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no le dio a alguien que alguien en el mundo se [UNK] a la estación de los demás [end]\n",
      "GOLDEN:  [start] Tomás no escucha a nadie. [end]\n",
      "ENGLISH SENTENCE:  I got soaked to the skin.\n",
      "MACHINE TRANSLATED RESULT:  [start] me he decidido a la [UNK] el [UNK] [UNK] de [UNK] [UNK] a la cabeza de la playa [end]\n",
      "GOLDEN:  [start] Estoy chorreando. [end]\n",
      "ENGLISH SENTENCE:  I imagined that.\n",
      "MACHINE TRANSLATED RESULT:  [start] me [UNK] que [UNK] [UNK] [UNK] [UNK] a la [UNK] [UNK] [UNK] a la cabeza en el suelo que [UNK]\n",
      "GOLDEN:  [start] Me imaginé eso. [end]\n",
      "ENGLISH SENTENCE:  The baby is crying.\n",
      "MACHINE TRANSLATED RESULT:  [start] el bebé está acostumbrado a [UNK] [UNK] en el [UNK] [UNK] de [UNK] en la estación de la estación [end]\n",
      "GOLDEN:  [start] El bebé está llorando. [end]\n",
      "ENGLISH SENTENCE:  I've never felt so exhilarated.\n",
      "MACHINE TRANSLATED RESULT:  [start] nunca he podido que [UNK] por el [UNK] [UNK] a la [UNK] en el suelo que la estación de [UNK]\n",
      "GOLDEN:  [start] Nunca me he sentido tan animado. [end]\n",
      "ENGLISH SENTENCE:  Please forgive me for not having written sooner.\n",
      "MACHINE TRANSLATED RESULT:  [start] por favor dime por no se [UNK] a la que [UNK] [UNK] a la estación de la playa [end]\n",
      "GOLDEN:  [start] Perdóname por no haberte escrito antes, por favor. [end]\n",
      "ENGLISH SENTENCE:  I expected more from you.\n",
      "MACHINE TRANSLATED RESULT:  [start] me he hecho más de que te [UNK] [UNK] a la [UNK] de la estación de la playa en el\n",
      "GOLDEN:  [start] Esperaba más de vos. [end]\n",
      "ENGLISH SENTENCE:  I have a computer.\n",
      "MACHINE TRANSLATED RESULT:  [start] tengo un consejo de [UNK] [UNK] [UNK] en el [UNK] [UNK] [UNK] de la cabeza [end]\n",
      "GOLDEN:  [start] Tengo un computador. [end]\n",
      "ENGLISH SENTENCE:  Dinner's ready!\n",
      "MACHINE TRANSLATED RESULT:  [start] la primera vez [UNK] [UNK] [UNK] de [UNK] en el [UNK] de [UNK] en la estación de la estación [end]\n",
      "GOLDEN:  [start] ¡La cena está lista! [end]\n",
      "ENGLISH SENTENCE:  Let me finish.\n",
      "MACHINE TRANSLATED RESULT:  [start] cuándo me [UNK] [UNK] [UNK] a la [UNK] [UNK] a la [UNK] de la cabeza de la estación de la\n",
      "GOLDEN:  [start] Déjame terminar. [end]\n",
      "ENGLISH SENTENCE:  I trust her.\n",
      "MACHINE TRANSLATED RESULT:  [start] me fui a su [UNK] [UNK] [UNK] en el [UNK] [UNK] [UNK] de la cabeza [end]\n",
      "GOLDEN:  [start] Yo confío en ella. [end]\n",
      "ENGLISH SENTENCE:  I trust him.\n",
      "MACHINE TRANSLATED RESULT:  [start] me voy a [UNK] [UNK] [UNK] a la [UNK] en el [UNK] de la estación de que se [UNK] [end]\n",
      "GOLDEN:  [start] Yo confío en él. [end]\n"
     ]
    }
   ],
   "source": [
    "predict_main(\n",
    "    filepath=filepath,\n",
    "    examples=EXAMPLES,\n",
    "    sequence_length=FLAGS_sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS_learning_rate = 0.001 # pegasus: 0.0005 > text generation: 2e-6\n",
    "# minimum viable params\n",
    "FLAGS_learning_rate = 0.001\n",
    "FLAGS_num_epochs = 50\n",
    "FLAGS_steps_per_epoch = None\n",
    "FLAGS_sequence_length = 20\n",
    "FLAGS_vocab_size = 15000\n",
    "FLAGS_batch_size = 64\n",
    "FLAGS_num_encoders = 1\n",
    "FLAGS_num_decoders = 1\n",
    "FLAGS_num_heads = 6\n",
    "FLAGS_intermediate_dim = 512\n",
    "FLAGS_model_dim = 64\n",
    "FLAGS_decay_steps = 100\n",
    "FLAGS_decay_rate = 0.99\n",
    "\n",
    "model, filepath = build_model(\n",
    "    num_encoders=FLAGS_num_encoders,\n",
    "    num_decoders=FLAGS_num_decoders,\n",
    "    num_heads=FLAGS_num_heads,\n",
    "    transformer_intermediate_dim=FLAGS_intermediate_dim,\n",
    "    sequence_length=FLAGS_sequence_length,\n",
    "    vocab_size=FLAGS_vocab_size,\n",
    "    batch_size=FLAGS_batch_size,\n",
    "    embed_dim=FLAGS_model_dim,\n",
    "    initial_learning_rate=FLAGS_learning_rate,\n",
    "    decay_steps=FLAGS_decay_steps,\n",
    "    decay_rate=FLAGS_decay_rate,\n",
    "    epochs=FLAGS_num_epochs,\n",
    "    steps_per_epoch=FLAGS_steps_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_main(\n",
    "    filepath=filepath,\n",
    "    examples=EXAMPLES,\n",
    "    sequence_length=FLAGS_sequence_length,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
