{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.15.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (2.15.1)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (2.15.1)\n",
      "Requirement already satisfied: keras==2.15.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (16.0.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.3.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.26.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (4.11.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.56.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow==2.15.1) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.19.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.30.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# The hyperparameters, especially of the optimizer, seem picky.\n",
    "# Currently, only the following versions go well only on mac with RMSprop.\n",
    "%pip install -U tensorflow==2.15.1 tensorflow-macos==2.15.1 keras==2.15.0\n",
    "#%pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version can not use keras.optimizers.legacy.RMSprop.\n",
    "# And the model does not converge.\n",
    "%pip install -U tensorflow==2.16.2 tensorflow-macos==2.16.2 keras==3.4.1\n",
    "#%pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "2.15.1\n",
      "2.15.0\n",
      "0.12.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "def is_running_on_apple_sillicon():\n",
    "    return platform.system() == \"Darwin\" and platform.processor() == \"arm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker cannot use @keras.saving\n",
    "from keras import saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def download_data():\n",
    "    text_file = keras.utils.get_file(\n",
    "        fname=\"spa-eng.zip\",\n",
    "        origin=(\n",
    "            \"http://storage.googleapis.com/download.tensorflow.org/data/\"\n",
    "            + \"spa-eng.zip\"\n",
    "        ),\n",
    "        extract=True,\n",
    "    )\n",
    "    return pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n",
    "\n",
    "\n",
    "def read_data(filepath):\n",
    "    with open(filepath) as f:\n",
    "        lines = f.read().split(\"\\n\")[:-1]\n",
    "        text_pairs = []\n",
    "        for line in lines:\n",
    "            eng, spa = line.split(\"\\t\")\n",
    "            spa = \"[start] \" + spa + \" [end]\"\n",
    "            text_pairs.append((eng, spa))\n",
    "    return text_pairs\n",
    "\n",
    "\n",
    "def split_train_val_test(text_pairs):\n",
    "    random.shuffle(text_pairs)\n",
    "    num_val_samples = int(0.15 * len(text_pairs))\n",
    "    num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "    train_pairs = text_pairs[:num_train_samples]\n",
    "    val_end_index = num_train_samples + num_val_samples\n",
    "    val_pairs = text_pairs[num_train_samples:val_end_index]\n",
    "    test_pairs = text_pairs[val_end_index:]\n",
    "    return train_pairs, val_pairs, test_pairs\n",
    "\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase,\n",
    "        \"[%s]\" % re.escape(strip_chars),\n",
    "        \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_tokenizer(train_pairs, sequence_length, vocab_size):\n",
    "    \"\"\"Preapare English and Spanish tokenizer.\"\"\"\n",
    "    eng_tokenizer = keras.layers.TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=sequence_length,\n",
    "    )\n",
    "    spa_tokenizer = keras.layers.TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=sequence_length + 1,\n",
    "        standardize=custom_standardization,\n",
    "    )\n",
    "    eng_texts, spa_texts = zip(*train_pairs)\n",
    "    eng_tokenizer.adapt(eng_texts)\n",
    "    spa_tokenizer.adapt(spa_texts)\n",
    "    return eng_tokenizer, spa_tokenizer\n",
    "\n",
    "\n",
    "def prepare_datasets(text_pairs, batch_size, eng_tokenizer, spa_tokenizer):\n",
    "    \"\"\"Transform raw text pairs to tf datasets.\"\"\"\n",
    "    eng_texts, spa_texts = zip(*text_pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "\n",
    "    def format_dataset(eng, spa):\n",
    "        \"\"\"Format the dataset given input English and Spanish text.\n",
    "\n",
    "        The output format is:\n",
    "            x: a pair of English and Spanish sentence.\n",
    "            y: The Spanish sentence in x shifts 1 token towards right, because\n",
    "                we are predicting the next token.\n",
    "        \"\"\"\n",
    "        eng = eng_tokenizer(eng)\n",
    "        spa = spa_tokenizer(spa)\n",
    "        return (\n",
    "            {\n",
    "                \"encoder_inputs\": eng,\n",
    "                \"decoder_inputs\": spa[:, :-1],\n",
    "            },\n",
    "            spa[:, 1:],\n",
    "            tf.cast((spa[:, 1:] != 0), \"float32\"),  # mask as sample weights\n",
    "        )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "\n",
    "def get_dataset_and_tokenizer(sequence_length, vocab_size, batch_size):\n",
    "    \"\"\"Main method to get the formatted machine translation dataset.\"\"\"\n",
    "    filepath = download_data()\n",
    "    text_pairs = read_data(filepath)\n",
    "    train_pairs, val_pairs, test_pairs = split_train_val_test(text_pairs)\n",
    "    eng_tokenizer, spa_tokenizer = prepare_tokenizer(\n",
    "        train_pairs, sequence_length, vocab_size\n",
    "    )\n",
    "    train_ds = prepare_datasets(\n",
    "        train_pairs,\n",
    "        batch_size,\n",
    "        eng_tokenizer,\n",
    "        spa_tokenizer,\n",
    "    )\n",
    "    val_ds = prepare_datasets(\n",
    "        val_pairs,\n",
    "        batch_size,\n",
    "        eng_tokenizer,\n",
    "        spa_tokenizer,\n",
    "    )\n",
    "    test_ds = prepare_datasets(\n",
    "        test_pairs,\n",
    "        batch_size,\n",
    "        eng_tokenizer,\n",
    "        spa_tokenizer,\n",
    "    )\n",
    "    return (train_ds, val_ds, test_ds), (eng_tokenizer, spa_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import tensorflow as tf\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "class TranslationModel(keras.Model):\n",
    "    \"\"\"The machine translation model.\n",
    "\n",
    "    The model is an encoder-decoder structure model. The encoder is a stack of\n",
    "    `keras_nlp.TransformerEncoder`, and the decoder is a stack of\n",
    "    `keras_nlp.TransformerDecoder`. We also pass in the tokenizer for encoder\n",
    "    and decoder so that during save/load, the tokenizer is also kept.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_tokenizer,\n",
    "        decoder_tokenizer,\n",
    "        num_encoders,\n",
    "        num_decoders,\n",
    "        num_heads,\n",
    "        transformer_intermediate_dim,\n",
    "        encoder_vocab_size,\n",
    "        decoder_vocab_size,\n",
    "        embed_dim,\n",
    "        sequence_length,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoders = []\n",
    "        self.decoders = []\n",
    "        for _ in range(num_encoders):\n",
    "            self.encoders.append(\n",
    "                keras_nlp.layers.TransformerEncoder(\n",
    "                    num_heads=num_heads,\n",
    "                    intermediate_dim=transformer_intermediate_dim,\n",
    "                )\n",
    "            )\n",
    "        for _ in range(num_decoders):\n",
    "            self.decoders.append(\n",
    "                keras_nlp.layers.TransformerDecoder(\n",
    "                    num_heads=num_heads,\n",
    "                    intermediate_dim=transformer_intermediate_dim,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.encoder_tokenizer = encoder_tokenizer\n",
    "        self.decoder_tokenizer = decoder_tokenizer\n",
    "\n",
    "        self.encoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "            vocabulary_size=encoder_vocab_size,\n",
    "            sequence_length=sequence_length,\n",
    "            embedding_dim=embed_dim,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "\n",
    "        self.decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "            vocabulary_size=decoder_vocab_size,\n",
    "            sequence_length=sequence_length,\n",
    "            embedding_dim=embed_dim,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "\n",
    "        self.dense = keras.layers.Dense(\n",
    "            decoder_vocab_size,\n",
    "            activation=\"softmax\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_input, decoder_input = (\n",
    "            inputs[\"encoder_inputs\"],\n",
    "            inputs[\"decoder_inputs\"],\n",
    "        )\n",
    "        encoded = self.encoder_embedding(encoder_input)\n",
    "        for encoder in self.encoders:\n",
    "            encoded = encoder(inputs=encoded)\n",
    "\n",
    "        decoded = self.decoder_embedding(decoder_input)\n",
    "        for decoder in self.decoders:\n",
    "            decoded = decoder(\n",
    "                decoder_sequence=decoded,\n",
    "                encoder_sequence=encoded,\n",
    "                use_causal_mask=True,\n",
    "            )\n",
    "\n",
    "        output = self.dense(decoded)\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            \"encoder_tokenizer\": self.encoder_tokenizer.get_config(),\n",
    "            \"decoder_tokenizer\": self.decoder_tokenizer.get_config(),\n",
    "            \"num_encoders\": len(self.encoders),\n",
    "            \"num_decoders\": len(self.decoders),\n",
    "            \"num_heads\": self.encoders[0].num_heads,\n",
    "            \"transformer_intermediate_dim\": self.encoders[0].intermediate_dim,\n",
    "            \"encoder_vocab_size\": self.encoder_embedding.vocab_size,\n",
    "            \"decoder_vocab_size\": self.decoder_embedding.vocab_size,\n",
    "            \"embed_dim\": self.encoder_embedding.embed_dim,\n",
    "            \"sequence_length\": self.encoder_embedding.sequence_length,\n",
    "        })\n",
    "        return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        encoder_tokenizer_config = config.pop(\"encoder_tokenizer\")\n",
    "        decoder_tokenizer_config = config.pop(\"decoder_tokenizer\")\n",
    "        encoder_tokenizer = keras.layers.TextVectorization.from_config(encoder_tokenizer_config)\n",
    "        decoder_tokenizer = keras.layers.TextVectorization.from_config(decoder_tokenizer_config)\n",
    "        return cls(encoder_tokenizer=encoder_tokenizer, decoder_tokenizer=decoder_tokenizer, **config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import keras_nlp\n",
    "\n",
    "# from absl import app\n",
    "# from absl import flags\n",
    "\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# flags.DEFINE_integer(\"num_epochs\", 1, \"Number of epochs to train.\")\n",
    "# flags.DEFINE_integer(\"steps_per_epoch\", None, \"Number of steps per epoch.\")\n",
    "# flags.DEFINE_integer(\"num_encoders\", 2, \"Number of Transformer encoder layers.\")\n",
    "# flags.DEFINE_integer(\"num_decoders\", 2, \"Number of Transformer decoder layers.\")\n",
    "# flags.DEFINE_integer(\"batch_size\", 64, \"The training batch size.\")\n",
    "# flags.DEFINE_float(\"learning_rate\", 0.001, \"The initial learning rate.\")\n",
    "# flags.DEFINE_integer(\"model_dim\", 64, \"Embedding size.\")\n",
    "# flags.DEFINE_integer(\n",
    "#     \"intermediate_dim\",\n",
    "#     128,\n",
    "#     \"Intermediate dimension (feedforward network) of transformer.\",\n",
    "# )\n",
    "# flags.DEFINE_integer(\n",
    "#     \"num_heads\",\n",
    "#     8,\n",
    "#     \"Number of head of the multihead attention.\",\n",
    "# )\n",
    "# flags.DEFINE_integer(\n",
    "#     \"sequence_length\",\n",
    "#     20,\n",
    "#     \"Input and output sequence length.\",\n",
    "# )\n",
    "# flags.DEFINE_integer(\n",
    "#     \"vocab_size\",\n",
    "#     15000,\n",
    "#     \"Vocabulary size, required by tokenizer.\",\n",
    "# )\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"saved_model_path\",\n",
    "#     \"saved_models/machine_translation_model\",\n",
    "#     \"The path to saved model\",\n",
    "# )\n",
    "\n",
    "FLAGS_learning_rate = 0.001\n",
    "FLAGS_num_epochs = 1\n",
    "FLAGS_steps_per_epoch = None\n",
    "FLAGS_sequence_length = 20\n",
    "FLAGS_vocab_size = 15000\n",
    "FLAGS_batch_size = 64\n",
    "FLAGS_num_encoders = 8\n",
    "FLAGS_num_decoders = 8\n",
    "FLAGS_num_heads = 32\n",
    "FLAGS_intermediate_dim = 512\n",
    "FLAGS_model_dim = 64\n",
    "\n",
    "FLAGS_learning_rate = 5e-4 # 0.001, pegasus: 0.0005 > text generation: 2e-6\n",
    "FLAGS_num_epochs = 100\n",
    "FLAGS_steps_per_epoch = None\n",
    "FLAGS_sequence_length = 20\n",
    "FLAGS_vocab_size = 15000\n",
    "FLAGS_batch_size = 64\n",
    "FLAGS_num_encoders = 8\n",
    "FLAGS_num_decoders = 8\n",
    "FLAGS_num_heads = 16\n",
    "FLAGS_intermediate_dim = 3072\n",
    "FLAGS_model_dim = 64\n",
    "\n",
    "FLAGS_learning_rate = 0.001\n",
    "FLAGS_num_epochs = 50\n",
    "FLAGS_steps_per_epoch = None\n",
    "FLAGS_sequence_length = 20\n",
    "FLAGS_vocab_size = 15000\n",
    "FLAGS_batch_size = 64\n",
    "FLAGS_num_encoders = 4\n",
    "FLAGS_num_decoders = 4\n",
    "FLAGS_num_heads = 12\n",
    "FLAGS_intermediate_dim = 1024\n",
    "FLAGS_model_dim = 64\n",
    "\n",
    "# minimum viable parameters\n",
    "FLAGS_learning_rate = 0.001\n",
    "FLAGS_num_epochs = 50\n",
    "FLAGS_steps_per_epoch = None\n",
    "FLAGS_sequence_length = 20\n",
    "FLAGS_vocab_size = 15000\n",
    "FLAGS_batch_size = 64\n",
    "FLAGS_num_encoders = 1\n",
    "FLAGS_num_decoders = 1\n",
    "FLAGS_num_heads = 6\n",
    "FLAGS_intermediate_dim = 512\n",
    "FLAGS_model_dim = 64\n",
    "\n",
    "if is_running_on_apple_sillicon():\n",
    "    FLAGS_saved_model_path = 'machine_translation_model.keras/machine_translation_model'\n",
    "else:\n",
    "    FLAGS_saved_model_path = 'machine_translation_model.keras'\n",
    "\n",
    "def run_training(model, train_ds, val_ds):\n",
    "    learning_rate = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=FLAGS_learning_rate,\n",
    "        decay_steps=20,\n",
    "        decay_rate=0.98,\n",
    "    )\n",
    "    if f\"{keras.__version__}\".startswith(\"2.\") and is_running_on_apple_sillicon():\n",
    "        optimizer = keras.optimizers.legacy.RMSprop(learning_rate=FLAGS_learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=FLAGS_learning_rate)\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "        reduction=keras.losses.Reduction.NONE\n",
    "    )\n",
    "    metrics = [\n",
    "        keras.metrics.SparseCategoricalAccuracy(),\n",
    "        #keras.metrics.SparseCategoricalCrossentropy(),\n",
    "        #keras_nlp.metrics.Bleu(), #  This cannot be used here\n",
    "    ]\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics,\n",
    "        loss=loss_fn,\n",
    "        weighted_metrics=[],\n",
    "    )\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=FLAGS_num_epochs,\n",
    "        validation_data=val_ds,\n",
    "        steps_per_epoch=FLAGS_steps_per_epoch,\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    (\n",
    "        (train_ds, val_ds, test_ds),\n",
    "        (\n",
    "            eng_tokenizer,\n",
    "            spa_tokenizer,\n",
    "        ),\n",
    "    ) = get_dataset_and_tokenizer(\n",
    "        FLAGS_sequence_length, FLAGS_vocab_size, FLAGS_batch_size\n",
    "    )\n",
    "    english_vocab_size = eng_tokenizer.vocabulary_size()\n",
    "    spanish_vocab_size = spa_tokenizer.vocabulary_size()\n",
    "    model = TranslationModel(\n",
    "        encoder_tokenizer=eng_tokenizer,\n",
    "        decoder_tokenizer=spa_tokenizer,\n",
    "        num_encoders=FLAGS_num_encoders,\n",
    "        num_decoders=FLAGS_num_decoders,\n",
    "        num_heads=FLAGS_num_heads,\n",
    "        transformer_intermediate_dim=FLAGS_intermediate_dim,\n",
    "        encoder_vocab_size=english_vocab_size,\n",
    "        decoder_vocab_size=spanish_vocab_size,\n",
    "        embed_dim=FLAGS_model_dim,\n",
    "        sequence_length=FLAGS_sequence_length,\n",
    "    )\n",
    "\n",
    "    run_training(model, train_ds, val_ds)\n",
    "\n",
    "    print(f\"Saving to {FLAGS_saved_model_path}\")\n",
    "    model.save(FLAGS_saved_model_path)\n",
    "\n",
    "    print(f\"Successfully saved model to {FLAGS_saved_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 02:00:02.269831: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-06-17 02:00:02.269858: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-06-17 02:00:02.269862: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-06-17 02:00:02.269896: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-17 02:00:02.269918: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-06-17 02:00:02.552806: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 1.7089 - sparse_categorical_accuracy: 0.2527WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 292s 213ms/step - loss: 1.7089 - sparse_categorical_accuracy: 0.2527 - val_loss: 1.3957 - val_sparse_categorical_accuracy: 0.3364\n",
      "Epoch 2/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 1.2734 - sparse_categorical_accuracy: 0.3927WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 295s 227ms/step - loss: 1.2734 - sparse_categorical_accuracy: 0.3927 - val_loss: 1.1453 - val_sparse_categorical_accuracy: 0.4377\n",
      "Epoch 3/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 1.0733 - sparse_categorical_accuracy: 0.4727WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 312s 239ms/step - loss: 1.0733 - sparse_categorical_accuracy: 0.4727 - val_loss: 1.0243 - val_sparse_categorical_accuracy: 0.4917\n",
      "Epoch 4/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.9708 - sparse_categorical_accuracy: 0.5159WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 298s 229ms/step - loss: 0.9708 - sparse_categorical_accuracy: 0.5159 - val_loss: 0.9678 - val_sparse_categorical_accuracy: 0.5125\n",
      "Epoch 5/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.9126 - sparse_categorical_accuracy: 0.5405WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 290s 223ms/step - loss: 0.9126 - sparse_categorical_accuracy: 0.5405 - val_loss: 0.9365 - val_sparse_categorical_accuracy: 0.5256\n",
      "Epoch 6/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8949 - sparse_categorical_accuracy: 0.5564WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 287s 220ms/step - loss: 0.8949 - sparse_categorical_accuracy: 0.5564 - val_loss: 0.9449 - val_sparse_categorical_accuracy: 0.5318\n",
      "Epoch 7/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8844 - sparse_categorical_accuracy: 0.5689WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 271s 208ms/step - loss: 0.8844 - sparse_categorical_accuracy: 0.5689 - val_loss: 0.9324 - val_sparse_categorical_accuracy: 0.5429\n",
      "Epoch 8/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8684 - sparse_categorical_accuracy: 0.5798WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 264s 203ms/step - loss: 0.8684 - sparse_categorical_accuracy: 0.5798 - val_loss: 0.9363 - val_sparse_categorical_accuracy: 0.5461\n",
      "Epoch 9/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8576 - sparse_categorical_accuracy: 0.5874WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 261s 200ms/step - loss: 0.8576 - sparse_categorical_accuracy: 0.5874 - val_loss: 0.9345 - val_sparse_categorical_accuracy: 0.5487\n",
      "Epoch 10/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8501 - sparse_categorical_accuracy: 0.5936WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 258s 198ms/step - loss: 0.8501 - sparse_categorical_accuracy: 0.5936 - val_loss: 0.9380 - val_sparse_categorical_accuracy: 0.5513\n",
      "Epoch 11/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8446 - sparse_categorical_accuracy: 0.5992WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 253s 195ms/step - loss: 0.8446 - sparse_categorical_accuracy: 0.5992 - val_loss: 0.9481 - val_sparse_categorical_accuracy: 0.5499\n",
      "Epoch 12/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8412 - sparse_categorical_accuracy: 0.6038WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 246s 189ms/step - loss: 0.8412 - sparse_categorical_accuracy: 0.6038 - val_loss: 0.9492 - val_sparse_categorical_accuracy: 0.5518\n",
      "Epoch 13/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8381 - sparse_categorical_accuracy: 0.6085WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 243s 187ms/step - loss: 0.8381 - sparse_categorical_accuracy: 0.6085 - val_loss: 0.9518 - val_sparse_categorical_accuracy: 0.5563\n",
      "Epoch 14/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8363 - sparse_categorical_accuracy: 0.6122WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 240s 184ms/step - loss: 0.8363 - sparse_categorical_accuracy: 0.6122 - val_loss: 0.9634 - val_sparse_categorical_accuracy: 0.5527\n",
      "Epoch 15/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8343 - sparse_categorical_accuracy: 0.6157WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 240s 185ms/step - loss: 0.8343 - sparse_categorical_accuracy: 0.6157 - val_loss: 0.9674 - val_sparse_categorical_accuracy: 0.5551\n",
      "Epoch 16/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8324 - sparse_categorical_accuracy: 0.6187WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 242s 186ms/step - loss: 0.8324 - sparse_categorical_accuracy: 0.6187 - val_loss: 0.9734 - val_sparse_categorical_accuracy: 0.5566\n",
      "Epoch 17/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8310 - sparse_categorical_accuracy: 0.6218WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 239s 184ms/step - loss: 0.8310 - sparse_categorical_accuracy: 0.6218 - val_loss: 0.9924 - val_sparse_categorical_accuracy: 0.5475\n",
      "Epoch 18/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8300 - sparse_categorical_accuracy: 0.6241WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 239s 184ms/step - loss: 0.8300 - sparse_categorical_accuracy: 0.6241 - val_loss: 0.9778 - val_sparse_categorical_accuracy: 0.5552\n",
      "Epoch 19/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8281 - sparse_categorical_accuracy: 0.6264WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 245s 188ms/step - loss: 0.8281 - sparse_categorical_accuracy: 0.6264 - val_loss: 0.9851 - val_sparse_categorical_accuracy: 0.5572\n",
      "Epoch 20/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8258 - sparse_categorical_accuracy: 0.6289WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 247s 190ms/step - loss: 0.8258 - sparse_categorical_accuracy: 0.6289 - val_loss: 0.9903 - val_sparse_categorical_accuracy: 0.5538\n",
      "Epoch 21/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8236 - sparse_categorical_accuracy: 0.6310WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 248s 190ms/step - loss: 0.8236 - sparse_categorical_accuracy: 0.6310 - val_loss: 0.9920 - val_sparse_categorical_accuracy: 0.5575\n",
      "Epoch 22/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8209 - sparse_categorical_accuracy: 0.6335WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 249s 191ms/step - loss: 0.8209 - sparse_categorical_accuracy: 0.6335 - val_loss: 0.9889 - val_sparse_categorical_accuracy: 0.5607\n",
      "Epoch 23/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8187 - sparse_categorical_accuracy: 0.6350WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 249s 191ms/step - loss: 0.8187 - sparse_categorical_accuracy: 0.6350 - val_loss: 0.9955 - val_sparse_categorical_accuracy: 0.5564\n",
      "Epoch 24/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8151 - sparse_categorical_accuracy: 0.6367WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 247s 190ms/step - loss: 0.8151 - sparse_categorical_accuracy: 0.6367 - val_loss: 0.9888 - val_sparse_categorical_accuracy: 0.5607\n",
      "Epoch 25/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8124 - sparse_categorical_accuracy: 0.6384WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 247s 190ms/step - loss: 0.8124 - sparse_categorical_accuracy: 0.6384 - val_loss: 0.9934 - val_sparse_categorical_accuracy: 0.5586\n",
      "Epoch 26/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8093 - sparse_categorical_accuracy: 0.6398WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 252s 193ms/step - loss: 0.8093 - sparse_categorical_accuracy: 0.6398 - val_loss: 0.9951 - val_sparse_categorical_accuracy: 0.5590\n",
      "Epoch 27/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8056 - sparse_categorical_accuracy: 0.6416WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 250s 192ms/step - loss: 0.8056 - sparse_categorical_accuracy: 0.6416 - val_loss: 0.9980 - val_sparse_categorical_accuracy: 0.5596\n",
      "Epoch 28/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.8013 - sparse_categorical_accuracy: 0.6432WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 250s 192ms/step - loss: 0.8013 - sparse_categorical_accuracy: 0.6432 - val_loss: 0.9942 - val_sparse_categorical_accuracy: 0.5601\n",
      "Epoch 29/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7975 - sparse_categorical_accuracy: 0.6445WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 248s 190ms/step - loss: 0.7975 - sparse_categorical_accuracy: 0.6445 - val_loss: 0.9950 - val_sparse_categorical_accuracy: 0.5588\n",
      "Epoch 30/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7930 - sparse_categorical_accuracy: 0.6457WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 251s 193ms/step - loss: 0.7930 - sparse_categorical_accuracy: 0.6457 - val_loss: 1.0002 - val_sparse_categorical_accuracy: 0.5576\n",
      "Epoch 31/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7896 - sparse_categorical_accuracy: 0.6473WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 254s 195ms/step - loss: 0.7896 - sparse_categorical_accuracy: 0.6473 - val_loss: 0.9926 - val_sparse_categorical_accuracy: 0.5602\n",
      "Epoch 32/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7849 - sparse_categorical_accuracy: 0.6488WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 258s 198ms/step - loss: 0.7849 - sparse_categorical_accuracy: 0.6488 - val_loss: 0.9907 - val_sparse_categorical_accuracy: 0.5619\n",
      "Epoch 33/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7796 - sparse_categorical_accuracy: 0.6501WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 258s 198ms/step - loss: 0.7796 - sparse_categorical_accuracy: 0.6501 - val_loss: 1.0040 - val_sparse_categorical_accuracy: 0.5573\n",
      "Epoch 34/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7760 - sparse_categorical_accuracy: 0.6512WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 256s 196ms/step - loss: 0.7760 - sparse_categorical_accuracy: 0.6512 - val_loss: 0.9929 - val_sparse_categorical_accuracy: 0.5601\n",
      "Epoch 35/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7732 - sparse_categorical_accuracy: 0.6518WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 254s 195ms/step - loss: 0.7732 - sparse_categorical_accuracy: 0.6518 - val_loss: 0.9971 - val_sparse_categorical_accuracy: 0.5596\n",
      "Epoch 36/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7714 - sparse_categorical_accuracy: 0.6528WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 255s 196ms/step - loss: 0.7714 - sparse_categorical_accuracy: 0.6528 - val_loss: 0.9988 - val_sparse_categorical_accuracy: 0.5578\n",
      "Epoch 37/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7705 - sparse_categorical_accuracy: 0.6531WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 255s 196ms/step - loss: 0.7705 - sparse_categorical_accuracy: 0.6531 - val_loss: 0.9990 - val_sparse_categorical_accuracy: 0.5593\n",
      "Epoch 38/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7677 - sparse_categorical_accuracy: 0.6538WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 254s 195ms/step - loss: 0.7677 - sparse_categorical_accuracy: 0.6538 - val_loss: 1.0049 - val_sparse_categorical_accuracy: 0.5592\n",
      "Epoch 39/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7633 - sparse_categorical_accuracy: 0.6544WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 254s 195ms/step - loss: 0.7633 - sparse_categorical_accuracy: 0.6544 - val_loss: 1.0055 - val_sparse_categorical_accuracy: 0.5569\n",
      "Epoch 40/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7584 - sparse_categorical_accuracy: 0.6560WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 256s 197ms/step - loss: 0.7584 - sparse_categorical_accuracy: 0.6560 - val_loss: 1.0062 - val_sparse_categorical_accuracy: 0.5589\n",
      "Epoch 41/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7551 - sparse_categorical_accuracy: 0.6567WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 258s 198ms/step - loss: 0.7551 - sparse_categorical_accuracy: 0.6567 - val_loss: 1.0000 - val_sparse_categorical_accuracy: 0.5594\n",
      "Epoch 42/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7546 - sparse_categorical_accuracy: 0.6574WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 260s 200ms/step - loss: 0.7546 - sparse_categorical_accuracy: 0.6574 - val_loss: 1.0009 - val_sparse_categorical_accuracy: 0.5582\n",
      "Epoch 43/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7544 - sparse_categorical_accuracy: 0.6594WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 254s 195ms/step - loss: 0.7544 - sparse_categorical_accuracy: 0.6594 - val_loss: 1.0116 - val_sparse_categorical_accuracy: 0.5563\n",
      "Epoch 44/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7562 - sparse_categorical_accuracy: 0.6597WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 253s 194ms/step - loss: 0.7562 - sparse_categorical_accuracy: 0.6597 - val_loss: 1.0030 - val_sparse_categorical_accuracy: 0.5608\n",
      "Epoch 45/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7558 - sparse_categorical_accuracy: 0.6600WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 252s 194ms/step - loss: 0.7558 - sparse_categorical_accuracy: 0.6600 - val_loss: 0.9994 - val_sparse_categorical_accuracy: 0.5635\n",
      "Epoch 46/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7541 - sparse_categorical_accuracy: 0.6606WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 254s 195ms/step - loss: 0.7541 - sparse_categorical_accuracy: 0.6606 - val_loss: 1.0038 - val_sparse_categorical_accuracy: 0.5592\n",
      "Epoch 47/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7555 - sparse_categorical_accuracy: 0.6610WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 252s 194ms/step - loss: 0.7555 - sparse_categorical_accuracy: 0.6610 - val_loss: 1.0201 - val_sparse_categorical_accuracy: 0.5559\n",
      "Epoch 48/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7543 - sparse_categorical_accuracy: 0.6625WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 252s 193ms/step - loss: 0.7543 - sparse_categorical_accuracy: 0.6625 - val_loss: 1.0156 - val_sparse_categorical_accuracy: 0.5601\n",
      "Epoch 49/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7515 - sparse_categorical_accuracy: 0.6638WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 255s 196ms/step - loss: 0.7515 - sparse_categorical_accuracy: 0.6638 - val_loss: 1.0175 - val_sparse_categorical_accuracy: 0.5613\n",
      "Epoch 50/50\n",
      "1302/1302 [==============================] - ETA: 0s - loss: 0.7492 - sparse_categorical_accuracy: 0.6650WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 256s 196ms/step - loss: 0.7492 - sparse_categorical_accuracy: 0.6650 - val_loss: 1.0242 - val_sparse_categorical_accuracy: 0.5571\n",
      "Saving to machine_translation_model.keras/machine_translation_model\n",
      "INFO:tensorflow:Assets written to: machine_translation_model.keras/machine_translation_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: machine_translation_model.keras/machine_translation_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model to machine_translation_model.keras/machine_translation_model\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from absl import app\n",
    "# from absl import flags\n",
    "# from absl import logging\n",
    "\n",
    "# Import data module to include the customized serializable, required for\n",
    "# loading tokenizer.\n",
    "# import examples.machine_translation.data  # noqa: F401.\n",
    "\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# flags.DEFINE_integer(\n",
    "#     \"sequence_length\",\n",
    "#     20,\n",
    "#     \"Input and output sequence length.\",\n",
    "# )\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"saved_model_path\",\n",
    "#     \"saved_models/machine_translation_model\",\n",
    "#     \"The path to saved model\",\n",
    "# )\n",
    "\n",
    "# flags.DEFINE_string(\"inputs\", None, \"The inputs to run machine translation on.\")\n",
    "FLAGS_inputs = None\n",
    "\n",
    "EXAMPLES = [\n",
    "    (\n",
    "        \"Tom doesn't listen to anyone.\",\n",
    "        \"[start] Tomás no escucha a nadie. [end]\",\n",
    "    ),\n",
    "    (\"I got soaked to the skin.\", \"[start] Estoy chorreando. [end]\"),\n",
    "    (\"I imagined that.\", \"[start] Me imaginé eso. [end]\"),\n",
    "    (\"The baby is crying.\", \"[start] El bebé está llorando. [end]\"),\n",
    "    (\n",
    "        \"I've never felt so exhilarated.\",\n",
    "        \"[start] Nunca me he sentido tan animado. [end]\",\n",
    "    ),\n",
    "    (\n",
    "        \"Please forgive me for not having written sooner.\",\n",
    "        \"[start] Perdóname por no haberte escrito antes, por favor. [end]\",\n",
    "    ),\n",
    "    (\"I expected more from you.\", \"[start] Esperaba más de vos. [end]\"),\n",
    "    (\"I have a computer.\", \"[start] Tengo un computador. [end]\"),\n",
    "    (\"Dinner's ready!\", \"[start] ¡La cena está lista! [end]\"),\n",
    "    (\"Let me finish.\", \"[start] Déjame terminar. [end]\"),\n",
    "    (\"I trust her.\", \"[start] Yo confío en ella. [end]\"),\n",
    "    (\"I trust him.\", \"[start] Yo confío en él. [end]\"),\n",
    "]\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence, model, max_sequence_length, lookup_table):\n",
    "    encoder_tokenizer = model.encoder_tokenizer\n",
    "    decoder_tokenizer = model.decoder_tokenizer\n",
    "    tokenized_input = encoder_tokenizer([input_sentence])\n",
    "\n",
    "    start_token = decoder_tokenizer(\"[start]\")[0].numpy()\n",
    "    end_token = decoder_tokenizer(\"[end]\")[0].numpy()\n",
    "\n",
    "    decoded_sentence = [start_token]\n",
    "    for i in range(max_sequence_length):\n",
    "        decoder_inputs = tf.convert_to_tensor(\n",
    "            [decoded_sentence],\n",
    "            dtype=\"int64\",\n",
    "        )\n",
    "        decoder_inputs = tf.concat(\n",
    "            [\n",
    "                decoder_inputs,\n",
    "                tf.zeros(\n",
    "                    [1, max_sequence_length - i - 1],\n",
    "                    dtype=\"int64\",\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        input = {\n",
    "            \"encoder_inputs\": tokenized_input,\n",
    "            \"decoder_inputs\": decoder_inputs,\n",
    "        }\n",
    "        predictions = model(input)\n",
    "        predicted_token = np.argmax(predictions[0, i, :])\n",
    "        decoded_sentence.append(predicted_token)\n",
    "        if predicted_token == end_token:\n",
    "            break\n",
    "\n",
    "    detokenized_output = []\n",
    "    for token in decoded_sentence:\n",
    "        detokenized_output.append(lookup_table[token])\n",
    "    return \" \".join(detokenized_output)\n",
    "\n",
    "\n",
    "def predict_main():\n",
    "    loaded_model = keras.models.load_model(FLAGS_saved_model_path)\n",
    "\n",
    "    decoder_tokenizer = loaded_model.decoder_tokenizer\n",
    "    vocab = decoder_tokenizer.get_vocabulary()\n",
    "    index_lookup_table = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "    if FLAGS_inputs is not None:\n",
    "        # Run inference on user-specified sentence.\n",
    "        translated = decode_sequence(\n",
    "            FLAGS_inputs,\n",
    "            loaded_model,\n",
    "            FLAGS_sequence_length,\n",
    "            index_lookup_table,\n",
    "        )\n",
    "        print(f\"Translated results: {translated}\")\n",
    "\n",
    "    else:\n",
    "        translated = []\n",
    "        for example in EXAMPLES:\n",
    "            translated.append(\n",
    "                decode_sequence(\n",
    "                    example[0],\n",
    "                    loaded_model,\n",
    "                    FLAGS_sequence_length,\n",
    "                    index_lookup_table,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for i in range(len(EXAMPLES)):\n",
    "            print(\"ENGLISH SENTENCE: \", EXAMPLES[i][0])\n",
    "            print(\"MACHINE TRANSLATED RESULT: \", translated[i])\n",
    "            print(\"GOLDEN: \", EXAMPLES[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH SENTENCE:  Tom doesn't listen to anyone.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no escucha que su seguida [UNK] de [UNK] de [UNK] de [UNK] de su [UNK] de [UNK] a [UNK]\n",
      "GOLDEN:  [start] Tomás no escucha a nadie. [end]\n",
      "ENGLISH SENTENCE:  I got soaked to the skin.\n",
      "MACHINE TRANSLATED RESULT:  [start] me [UNK] que el [UNK] de ser [UNK] [UNK] [UNK] [UNK] [UNK] de su [UNK] de [UNK] de es [UNK]\n",
      "GOLDEN:  [start] Estoy chorreando. [end]\n",
      "ENGLISH SENTENCE:  I imagined that.\n",
      "MACHINE TRANSLATED RESULT:  [start] me lo [UNK] [UNK] [UNK] de [UNK] de [UNK] de [UNK] de [UNK] de su [UNK] de [UNK] a [UNK]\n",
      "GOLDEN:  [start] Me imaginé eso. [end]\n",
      "ENGLISH SENTENCE:  The baby is crying.\n",
      "MACHINE TRANSLATED RESULT:  [start] el bebé está de [UNK] de [UNK] [UNK] de [UNK] [UNK] [UNK] de lo [UNK] [UNK] de su [UNK] con\n",
      "GOLDEN:  [start] El bebé está llorando. [end]\n",
      "ENGLISH SENTENCE:  I've never felt so exhilarated.\n",
      "MACHINE TRANSLATED RESULT:  [start] nunca me he sentido [UNK] de [UNK] de [UNK] de [UNK] de [UNK] de lo [UNK] de su [UNK] con\n",
      "GOLDEN:  [start] Nunca me he sentido tan animado. [end]\n",
      "ENGLISH SENTENCE:  Please forgive me for not having written sooner.\n",
      "MACHINE TRANSLATED RESULT:  [start] por favor por mi por tu por no llevar a lo [UNK] de [UNK] de [UNK] de que [UNK] a\n",
      "GOLDEN:  [start] Perdóname por no haberte escrito antes, por favor. [end]\n",
      "ENGLISH SENTENCE:  I expected more from you.\n",
      "MACHINE TRANSLATED RESULT:  [start] me esperaba más de hecho de [UNK] de [UNK] de [UNK] [UNK] [UNK] de [UNK] de su [UNK] de [end]\n",
      "GOLDEN:  [start] Esperaba más de vos. [end]\n",
      "ENGLISH SENTENCE:  I have a computer.\n",
      "MACHINE TRANSLATED RESULT:  [start] tengo un ordenador de lo que tiene [UNK] [UNK] de [UNK] de [UNK] de [UNK] de [UNK] de su [UNK]\n",
      "GOLDEN:  [start] Tengo un computador. [end]\n",
      "ENGLISH SENTENCE:  Dinner's ready!\n",
      "MACHINE TRANSLATED RESULT:  [start] la cena en lo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] de lo [UNK] [UNK] [UNK] de [UNK] a\n",
      "GOLDEN:  [start] ¡La cena está lista! [end]\n",
      "ENGLISH SENTENCE:  Let me finish.\n",
      "MACHINE TRANSLATED RESULT:  [start] déjame tener seguida [UNK] de [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] de lo [UNK] de [UNK] de su [UNK] a\n",
      "GOLDEN:  [start] Déjame terminar. [end]\n",
      "ENGLISH SENTENCE:  I trust her.\n",
      "MACHINE TRANSLATED RESULT:  [start] yo como [UNK] su [UNK] [UNK] de [UNK] [UNK] de [UNK] [UNK] [UNK] de su [UNK] de [UNK] a [UNK]\n",
      "GOLDEN:  [start] Yo confío en ella. [end]\n",
      "ENGLISH SENTENCE:  I trust him.\n",
      "MACHINE TRANSLATED RESULT:  [start] yo lo [UNK] [UNK] [UNK] de [UNK] [UNK] de [UNK] de [UNK] [UNK] de su [UNK] de [UNK] a [UNK]\n",
      "GOLDEN:  [start] Yo confío en él. [end]\n"
     ]
    }
   ],
   "source": [
    "predict_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
