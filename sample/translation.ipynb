{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker\n",
    "#%pip install tensorflow==2.15.1 keras==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: keras-core in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.7)\n",
      "Requirement already satisfied: absl-py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.26.1)\n",
      "Requirement already satisfied: packaging in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (23.1)\n",
      "Requirement already satisfied: regex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (2023.5.5)\n",
      "Requirement already satisfied: rich in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (13.7.1)\n",
      "Requirement already satisfied: dm-tree in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.8)\n",
      "Requirement already satisfied: kagglehub in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.2.5)\n",
      "Requirement already satisfied: requests in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (2.30.0)\n",
      "Requirement already satisfied: tqdm in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (4.65.0)\n",
      "Requirement already satisfied: namex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (3.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.1\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker cannot use @keras.saving\n",
    "from keras import saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def download_data():\n",
    "    text_file = keras.utils.get_file(\n",
    "        fname=\"spa-eng.zip\",\n",
    "        origin=(\n",
    "            \"http://storage.googleapis.com/download.tensorflow.org/data/\"\n",
    "            + \"spa-eng.zip\"\n",
    "        ),\n",
    "        extract=True,\n",
    "    )\n",
    "    return pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n",
    "\n",
    "\n",
    "def read_data(filepath):\n",
    "    with open(filepath) as f:\n",
    "        lines = f.read().split(\"\\n\")[:-1]\n",
    "        text_pairs = []\n",
    "        for line in lines:\n",
    "            eng, spa = line.split(\"\\t\")\n",
    "            spa = \"[start] \" + spa + \" [end]\"\n",
    "            text_pairs.append((eng, spa))\n",
    "    return text_pairs\n",
    "\n",
    "\n",
    "def split_train_val_test(text_pairs):\n",
    "    random.shuffle(text_pairs)\n",
    "    num_val_samples = int(0.15 * len(text_pairs))\n",
    "    num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "    train_pairs = text_pairs[:num_train_samples]\n",
    "    val_end_index = num_train_samples + num_val_samples\n",
    "    val_pairs = text_pairs[num_train_samples:val_end_index]\n",
    "    test_pairs = text_pairs[val_end_index:]\n",
    "    return train_pairs, val_pairs, test_pairs\n",
    "\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase,\n",
    "        \"[%s]\" % re.escape(strip_chars),\n",
    "        \"\",\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_tokenizer(train_pairs, sequence_length, vocab_size):\n",
    "    \"\"\"Preapare English and Spanish tokenizer.\"\"\"\n",
    "    eng_tokenizer = keras.layers.TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=sequence_length,\n",
    "    )\n",
    "    spa_tokenizer = keras.layers.TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=sequence_length + 1,\n",
    "        standardize=custom_standardization,\n",
    "    )\n",
    "    eng_texts, spa_texts = zip(*train_pairs)\n",
    "    eng_tokenizer.adapt(eng_texts)\n",
    "    spa_tokenizer.adapt(spa_texts)\n",
    "    return eng_tokenizer, spa_tokenizer\n",
    "\n",
    "\n",
    "def prepare_datasets(text_pairs, batch_size, eng_tokenizer, spa_tokenizer):\n",
    "    \"\"\"Transform raw text pairs to tf datasets.\"\"\"\n",
    "    eng_texts, spa_texts = zip(*text_pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "\n",
    "    def format_dataset(eng, spa):\n",
    "        \"\"\"Format the dataset given input English and Spanish text.\n",
    "\n",
    "        The output format is:\n",
    "            x: a pair of English and Spanish sentence.\n",
    "            y: The Spanish sentence in x shifts 1 token towards right, because\n",
    "                we are predicting the next token.\n",
    "        \"\"\"\n",
    "        eng = eng_tokenizer(eng)\n",
    "        spa = spa_tokenizer(spa)\n",
    "        return (\n",
    "            {\n",
    "                \"encoder_inputs\": eng,\n",
    "                \"decoder_inputs\": spa[:, :-1],\n",
    "            },\n",
    "            spa[:, 1:],\n",
    "            tf.cast((spa[:, 1:] != 0), \"float32\"),  # mask as sample weights\n",
    "        )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
    "\n",
    "\n",
    "def get_dataset_and_tokenizer(sequence_length, vocab_size, batch_size):\n",
    "    \"\"\"Main method to get the formatted machine translation dataset.\"\"\"\n",
    "    filepath = download_data()\n",
    "    text_pairs = read_data(filepath)\n",
    "    train_pairs, val_pairs, test_pairs = split_train_val_test(text_pairs)\n",
    "    eng_tokenizer, spa_tokenizer = prepare_tokenizer(\n",
    "        train_pairs, sequence_length, vocab_size\n",
    "    )\n",
    "    train_ds = prepare_datasets(\n",
    "        train_pairs,\n",
    "        batch_size,\n",
    "        eng_tokenizer,\n",
    "        spa_tokenizer,\n",
    "    )\n",
    "    val_ds = prepare_datasets(\n",
    "        val_pairs,\n",
    "        batch_size,\n",
    "        eng_tokenizer,\n",
    "        spa_tokenizer,\n",
    "    )\n",
    "    test_ds = prepare_datasets(\n",
    "        test_pairs,\n",
    "        batch_size,\n",
    "        eng_tokenizer,\n",
    "        spa_tokenizer,\n",
    "    )\n",
    "    return (train_ds, val_ds, test_ds), (eng_tokenizer, spa_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras_nlp.layers import TransformerDecoder\n",
    "from keras_nlp.layers import TransformerEncoder\n",
    "\n",
    "\n",
    "class PositionalEmbedding(keras.layers.Layer):\n",
    "    \"\"\"The positional embedding class.\"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "class TranslationModel(keras.Model):\n",
    "    \"\"\"The machine translation model.\n",
    "\n",
    "    The model is an encoder-decoder structure model. The encoder is a stack of\n",
    "    `keras_nlp.TransformerEncoder`, and the decoder is a stack of\n",
    "    `keras_nlp.TransformerDecoder`. We also pass in the tokenizer for encoder\n",
    "    and decoder so that during save/load, the tokenizer is also kept.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_tokenizer,\n",
    "        decoder_tokenizer,\n",
    "        num_encoders,\n",
    "        num_decoders,\n",
    "        num_heads,\n",
    "        transformer_intermediate_dim,\n",
    "        encoder_vocab_size,\n",
    "        decoder_vocab_size,\n",
    "        embed_dim,\n",
    "        sequence_length,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoders = []\n",
    "        self.decoders = []\n",
    "        for _ in range(num_encoders):\n",
    "            self.encoders.append(\n",
    "                TransformerEncoder(\n",
    "                    num_heads=num_heads,\n",
    "                    intermediate_dim=transformer_intermediate_dim,\n",
    "                )\n",
    "            )\n",
    "        for _ in range(num_decoders):\n",
    "            self.decoders.append(\n",
    "                TransformerDecoder(\n",
    "                    num_heads=num_heads,\n",
    "                    intermediate_dim=transformer_intermediate_dim,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.encoder_tokenizer = encoder_tokenizer\n",
    "        self.decoder_tokenizer = decoder_tokenizer\n",
    "\n",
    "        self.encoder_embedding = PositionalEmbedding(\n",
    "            sequence_length=sequence_length,\n",
    "            vocab_size=encoder_vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "\n",
    "        self.decoder_embedding = PositionalEmbedding(\n",
    "            sequence_length=sequence_length,\n",
    "            vocab_size=decoder_vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "\n",
    "        self.dense = keras.layers.Dense(\n",
    "            decoder_vocab_size,\n",
    "            activation=\"softmax\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_input, decoder_input = (\n",
    "            inputs[\"encoder_inputs\"],\n",
    "            inputs[\"decoder_inputs\"],\n",
    "        )\n",
    "        encoded = self.encoder_embedding(encoder_input)\n",
    "        for encoder in self.encoders:\n",
    "            encoded = encoder(encoded)\n",
    "\n",
    "        decoded = self.decoder_embedding(decoder_input)\n",
    "        for decoder in self.decoders:\n",
    "            decoded = decoder(\n",
    "                decoded,\n",
    "                encoded,\n",
    "                use_causal_mask=True,\n",
    "            )\n",
    "\n",
    "        output = self.dense(decoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import platform\n",
    "\n",
    "import keras_nlp\n",
    "\n",
    "# from absl import app\n",
    "# from absl import flags\n",
    "from tensorflow import keras\n",
    "\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# flags.DEFINE_integer(\"num_epochs\", 1, \"Number of epochs to train.\")\n",
    "# flags.DEFINE_integer(\"steps_per_epoch\", None, \"Number of steps per epoch.\")\n",
    "# flags.DEFINE_integer(\"num_encoders\", 2, \"Number of Transformer encoder layers.\")\n",
    "# flags.DEFINE_integer(\"num_decoders\", 2, \"Number of Transformer decoder layers.\")\n",
    "# flags.DEFINE_integer(\"batch_size\", 64, \"The training batch size.\")\n",
    "# flags.DEFINE_float(\"learning_rate\", 0.001, \"The initial learning rate.\")\n",
    "# flags.DEFINE_integer(\"model_dim\", 64, \"Embedding size.\")\n",
    "# flags.DEFINE_integer(\n",
    "#     \"intermediate_dim\",\n",
    "#     128,\n",
    "#     \"Intermediate dimension (feedforward network) of transformer.\",\n",
    "# )\n",
    "# flags.DEFINE_integer(\n",
    "#     \"num_heads\",\n",
    "#     8,\n",
    "#     \"Number of head of the multihead attention.\",\n",
    "# )\n",
    "# flags.DEFINE_integer(\n",
    "#     \"sequence_length\",\n",
    "#     20,\n",
    "#     \"Input and output sequence length.\",\n",
    "# )\n",
    "# flags.DEFINE_integer(\n",
    "#     \"vocab_size\",\n",
    "#     15000,\n",
    "#     \"Vocabulary size, required by tokenizer.\",\n",
    "# )\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"saved_model_path\",\n",
    "#     \"saved_models/machine_translation_model\",\n",
    "#     \"The path to saved model\",\n",
    "# )\n",
    "\n",
    "FLAGS_learning_rate = 0.001\n",
    "FLAGS_num_epochs = 1\n",
    "FLAGS_steps_per_epoch = None\n",
    "FLAGS_sequence_length = 20\n",
    "FLAGS_vocab_size = 15000\n",
    "FLAGS_batch_size = 64\n",
    "FLAGS_num_encoders = 8\n",
    "FLAGS_num_decoders = 8\n",
    "FLAGS_num_heads = 32\n",
    "FLAGS_intermediate_dim = 512\n",
    "FLAGS_model_dim = 64\n",
    "\n",
    "# FLAGS_learning_rate = 0.001\n",
    "# FLAGS_num_epochs = 20\n",
    "# FLAGS_steps_per_epoch = None\n",
    "# FLAGS_sequence_length = 20\n",
    "# FLAGS_vocab_size = 15000\n",
    "# FLAGS_batch_size = 64\n",
    "# FLAGS_num_encoders = 8\n",
    "# FLAGS_num_decoders = 8\n",
    "# FLAGS_num_heads = 16\n",
    "# FLAGS_intermediate_dim = 3072\n",
    "# FLAGS_model_dim = 64\n",
    "\n",
    "if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "    FLAGS_saved_model_path = 'machine_translation_model.keras/machine_translation_model'\n",
    "else:\n",
    "    FLAGS_saved_model_path = 'machine_translation_model.keras'\n",
    "\n",
    "def run_training(model, train_ds, val_ds):\n",
    "    learning_rate = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=FLAGS_learning_rate,\n",
    "        decay_steps=20,\n",
    "        decay_rate=0.98,\n",
    "    )\n",
    "    if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "        optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "        reduction=keras.losses.Reduction.NONE\n",
    "    )\n",
    "    metrics = [\n",
    "        keras.metrics.SparseCategoricalAccuracy(),\n",
    "        #keras_nlp.metrics.Bleu(), #  This cannot be used here\n",
    "    ]\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics,\n",
    "        loss=loss_fn\n",
    "    )\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=FLAGS_num_epochs,\n",
    "        validation_data=val_ds,\n",
    "        steps_per_epoch=FLAGS_steps_per_epoch,\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    (\n",
    "        (train_ds, val_ds, test_ds),\n",
    "        (\n",
    "            eng_tokenizer,\n",
    "            spa_tokenizer,\n",
    "        ),\n",
    "    ) = get_dataset_and_tokenizer(\n",
    "        FLAGS_sequence_length, FLAGS_vocab_size, FLAGS_batch_size\n",
    "    )\n",
    "    english_vocab_size = eng_tokenizer.vocabulary_size()\n",
    "    spanish_vocab_size = spa_tokenizer.vocabulary_size()\n",
    "    model = TranslationModel(\n",
    "        encoder_tokenizer=eng_tokenizer,\n",
    "        decoder_tokenizer=spa_tokenizer,\n",
    "        num_encoders=FLAGS_num_encoders,\n",
    "        num_decoders=FLAGS_num_decoders,\n",
    "        num_heads=FLAGS_num_heads,\n",
    "        transformer_intermediate_dim=FLAGS_intermediate_dim,\n",
    "        encoder_vocab_size=english_vocab_size,\n",
    "        decoder_vocab_size=spanish_vocab_size,\n",
    "        embed_dim=FLAGS_model_dim,\n",
    "        sequence_length=FLAGS_sequence_length,\n",
    "    )\n",
    "\n",
    "    run_training(model, train_ds, val_ds)\n",
    "\n",
    "    print(f\"Saving to {FLAGS_saved_model_path}\")\n",
    "    model.save(FLAGS_saved_model_path)\n",
    "\n",
    "    print(f\"Successfully saved model to {FLAGS_saved_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 01:11:29.980823: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-06-16 01:11:29.980850: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-06-16 01:11:29.980853: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-06-16 01:11:29.980887: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-16 01:11:29.980903: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-06-16 01:11:30.306071: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - ETA: 0s - loss: 1.8506 - sparse_categorical_accuracy: 0.1883WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "1302/1302 [==============================] - 529s 392ms/step - loss: 1.8506 - sparse_categorical_accuracy: 0.1883 - val_loss: 1.6133 - val_sparse_categorical_accuracy: 0.2196\n",
      "Saving to machine_translation_model.keras/machine_translation_model\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "INFO:tensorflow:Assets written to: machine_translation_model.keras/machine_translation_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: machine_translation_model.keras/machine_translation_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model to machine_translation_model.keras/machine_translation_model\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 The KerasNLP Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from absl import app\n",
    "# from absl import flags\n",
    "# from absl import logging\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import data module to include the customized serializable, required for\n",
    "# loading tokenizer.\n",
    "# import examples.machine_translation.data  # noqa: F401.\n",
    "\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# flags.DEFINE_integer(\n",
    "#     \"sequence_length\",\n",
    "#     20,\n",
    "#     \"Input and output sequence length.\",\n",
    "# )\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"saved_model_path\",\n",
    "#     \"saved_models/machine_translation_model\",\n",
    "#     \"The path to saved model\",\n",
    "# )\n",
    "\n",
    "# flags.DEFINE_string(\"inputs\", None, \"The inputs to run machine translation on.\")\n",
    "FLAGS_inputs = None\n",
    "\n",
    "EXAMPLES = [\n",
    "    (\n",
    "        \"Tom doesn't listen to anyone.\",\n",
    "        \"[start] Tomás no escucha a nadie. [end]\",\n",
    "    ),\n",
    "    (\"I got soaked to the skin.\", \"[start] Estoy chorreando. [end]\"),\n",
    "    (\"I imagined that.\", \"[start] Me imaginé eso. [end]\"),\n",
    "    (\"The baby is crying.\", \"[start] El bebé está llorando. [end]\"),\n",
    "    (\n",
    "        \"I've never felt so exhilarated.\",\n",
    "        \"[start] Nunca me he sentido tan animado. [end]\",\n",
    "    ),\n",
    "    (\n",
    "        \"Please forgive me for not having written sooner.\",\n",
    "        \"[start] Perdóname por no haberte escrito antes, por favor. [end]\",\n",
    "    ),\n",
    "    (\"I expected more from you.\", \"[start] Esperaba más de vos. [end]\"),\n",
    "    (\"I have a computer.\", \"[start] Tengo un computador. [end]\"),\n",
    "    (\"Dinner's ready!\", \"[start] ¡La cena está lista! [end]\"),\n",
    "    (\"Let me finish.\", \"[start] Déjame terminar. [end]\"),\n",
    "]\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence, model, max_sequence_length, lookup_table):\n",
    "    encoder_tokenizer = model.encoder_tokenizer\n",
    "    decoder_tokenizer = model.decoder_tokenizer\n",
    "    tokenized_input = encoder_tokenizer([input_sentence])\n",
    "\n",
    "    start_token = decoder_tokenizer(\"[start]\")[0].numpy()\n",
    "    end_token = decoder_tokenizer(\"[end]\")[0].numpy()\n",
    "\n",
    "    decoded_sentence = [start_token]\n",
    "    for i in range(max_sequence_length):\n",
    "        decoder_inputs = tf.convert_to_tensor(\n",
    "            [decoded_sentence],\n",
    "            dtype=\"int64\",\n",
    "        )\n",
    "        decoder_inputs = tf.concat(\n",
    "            [\n",
    "                decoder_inputs,\n",
    "                tf.zeros(\n",
    "                    [1, max_sequence_length - i - 1],\n",
    "                    dtype=\"int64\",\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        input = {\n",
    "            \"encoder_inputs\": tokenized_input,\n",
    "            \"decoder_inputs\": decoder_inputs,\n",
    "        }\n",
    "        predictions = model(input)\n",
    "        predicted_token = np.argmax(predictions[0, i, :])\n",
    "        decoded_sentence.append(predicted_token)\n",
    "        if predicted_token == end_token:\n",
    "            break\n",
    "\n",
    "    detokenized_output = []\n",
    "    for token in decoded_sentence:\n",
    "        detokenized_output.append(lookup_table[token])\n",
    "    return \" \".join(detokenized_output)\n",
    "\n",
    "\n",
    "def predict_main():\n",
    "    loaded_model = keras.models.load_model(FLAGS_saved_model_path)\n",
    "\n",
    "    decoder_tokenizer = loaded_model.decoder_tokenizer\n",
    "    vocab = decoder_tokenizer.get_vocabulary()\n",
    "    index_lookup_table = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "    if FLAGS_inputs is not None:\n",
    "        # Run inference on user-specified sentence.\n",
    "        translated = decode_sequence(\n",
    "            FLAGS_inputs,\n",
    "            loaded_model,\n",
    "            FLAGS_sequence_length,\n",
    "            index_lookup_table,\n",
    "        )\n",
    "        print(f\"Translated results: {translated}\")\n",
    "\n",
    "    else:\n",
    "        translated = []\n",
    "        for example in EXAMPLES:\n",
    "            translated.append(\n",
    "                decode_sequence(\n",
    "                    example[0],\n",
    "                    loaded_model,\n",
    "                    FLAGS_sequence_length,\n",
    "                    index_lookup_table,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for i in range(len(EXAMPLES)):\n",
    "            print(\"ENGLISH SENTENCE: \", EXAMPLES[i][0])\n",
    "            print(\"MACHINE TRANSLATED RESULT: \", translated[i])\n",
    "            print(\"GOLDEN: \", EXAMPLES[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH SENTENCE:  Tom doesn't listen to anyone.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no se dijo a mary que mary no se dijo que mary [end]\n",
      "GOLDEN:  [start] Tomás no escucha a nadie. [end]\n",
      "ENGLISH SENTENCE:  I got soaked to the skin.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no se dijo a mary que mary no se dijo que mary [end]\n",
      "GOLDEN:  [start] Estoy chorreando. [end]\n",
      "ENGLISH SENTENCE:  I imagined that.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no se dijo a mary que mary no se dijo que mary [end]\n",
      "GOLDEN:  [start] Me imaginé eso. [end]\n",
      "ENGLISH SENTENCE:  The baby is crying.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no se dijo a mary que mary no se dijo que mary [end]\n",
      "GOLDEN:  [start] El bebé está llorando. [end]\n",
      "ENGLISH SENTENCE:  I've never felt so exhilarated.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no se dijo a mary que mary no se dijo que mary [end]\n",
      "GOLDEN:  [start] Nunca me he sentido tan animado. [end]\n",
      "ENGLISH SENTENCE:  Please forgive me for not having written sooner.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no se dijo a mary que mary no se dijo que mary [end]\n",
      "GOLDEN:  [start] Perdóname por no haberte escrito antes, por favor. [end]\n",
      "ENGLISH SENTENCE:  I expected more from you.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no se dijo a mary que mary no se dijo que mary [end]\n",
      "GOLDEN:  [start] Esperaba más de vos. [end]\n",
      "ENGLISH SENTENCE:  I have a computer.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no se dijo a mary que mary no se dijo que mary [end]\n",
      "GOLDEN:  [start] Tengo un computador. [end]\n",
      "ENGLISH SENTENCE:  Dinner's ready!\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no se dijo a mary que mary no se dijo que mary [end]\n",
      "GOLDEN:  [start] ¡La cena está lista! [end]\n",
      "ENGLISH SENTENCE:  Let me finish.\n",
      "MACHINE TRANSLATED RESULT:  [start] tom no se dijo a mary que mary no se dijo que mary [end]\n",
      "GOLDEN:  [start] Déjame terminar. [end]\n"
     ]
    }
   ],
   "source": [
    "predict_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
