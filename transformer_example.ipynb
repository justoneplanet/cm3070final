{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-nlp in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.12.1)\n",
            "Requirement already satisfied: keras-core in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.7)\n",
            "Requirement already satisfied: absl-py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.26.1)\n",
            "Requirement already satisfied: packaging in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (23.1)\n",
            "Requirement already satisfied: regex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (2023.5.5)\n",
            "Requirement already satisfied: rich in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (13.7.1)\n",
            "Requirement already satisfied: dm-tree in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.8)\n",
            "Requirement already satisfied: kagglehub in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.2.5)\n",
            "Requirement already satisfied: requests in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (2.30.0)\n",
            "Requirement already satisfied: tqdm in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (4.65.0)\n",
            "Requirement already satisfied: namex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (0.0.8)\n",
            "Requirement already satisfied: h5py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (3.9.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (2023.5.7)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --upgrade keras-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is running on M1/M2 mac.\n",
            "Tensorflow 2.14.1 is running. This notebook was written with Tensorflow 2.15.0.\n",
            "Keras 2.14.0 is running. This notebook was written with Keras 2.15.0.\n"
          ]
        }
      ],
      "source": [
        "# Sometimes, the learning with tensorflow-metal does not converge.\n",
        "# @see\n",
        "# - https://forums.developer.apple.com/forums/thread/736187\n",
        "# - https://forums.developer.apple.com/forums/thread/701056\n",
        "# - https://forums.developer.apple.com/forums/thread/742157\n",
        "# Therefore, it might be best to execute learning without metal and execute learning again in the last tuning phase.\n",
        "if platform.processor() == \"arm\":\n",
        "    print(\"This is running on M1/M2 mac.\")\n",
        "    tf.config.set_visible_devices([], 'GPU')\n",
        "print(f\"Tensorflow {tf.__version__} is running. This notebook was written with Tensorflow 2.15.0.\")\n",
        "print(f\"Keras {keras.__version__} is running. This notebook was written with Keras 2.15.0.\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot(history, title=None):\n",
        "    \"\"\"\n",
        "    Display the plot that indicates the loss and accuracy.\n",
        "    :param history: history object from the tensorflow fit function.\n",
        "    :param title: title text.\n",
        "    \"\"\"\n",
        "    flg, axes = plt.subplots(1, 2, tight_layout=True)\n",
        "    if title is not None:\n",
        "        flg.suptitle(t=title, fontsize=14)\n",
        "    for i, key in enumerate([\"loss\", \"accuracy\"]):\n",
        "        value = history.history[key]\n",
        "        val_loss = history.history[f\"val_{key}\"]\n",
        "        epochs = range(1, len(value) + 1)\n",
        "        axes[i].plot(epochs, value, label=f\"Training {key}\")\n",
        "        axes[i].plot(epochs, val_loss, label=f\"Validation {key}\")\n",
        "        axes[i].set_title(f\"Training and validation {key}\")\n",
        "        axes[i].set_xlabel(\"epochs\")\n",
        "        axes[i].set_ylabel(key)\n",
        "        axes[i].legend()\n",
        "    plt.show()\n",
        "\n",
        "    print(\n",
        "        np.min(history.history[\"val_loss\"]),\n",
        "        \"The best number of epocs for the validation loss is\",\n",
        "        np.argmin(history.history[\"val_loss\"]) + 1,\n",
        "    )\n",
        "    print(\n",
        "        np.max(history.history[\"val_accuracy\"]),\n",
        "        \"The best number of epocs for the validation accuracy is\",\n",
        "        np.argmax(history.history[\"val_accuracy\"]) + 1,\n",
        "    )\n",
        "\n",
        "def load_ag_news_subset(batch_size, shuffle=False):\n",
        "    \"\"\"\n",
        "    Load ag_news_subset dataset.\n",
        "    :param batch_size: the number of batch size.\n",
        "    :param shuffle: if True, it is shuffled.\n",
        "    :return: a dataset object.\n",
        "    \"\"\"\n",
        "    BUFFER_SIZE = 10000\n",
        "    dataset, info = tfds.load(\n",
        "        'ag_news_subset',\n",
        "        with_info=True,\n",
        "        as_supervised=False\n",
        "    )\n",
        "    train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "    if shuffle:\n",
        "        # If shuffle is hopefully used,\n",
        "        # the reshuffle_each_iteration parameter must be False.\n",
        "        # Or the partial training dataset and the validation set are contaminated.\n",
        "        train_dataset = train_dataset.shuffle(\n",
        "            buffer_size=BUFFER_SIZE,\n",
        "            reshuffle_each_iteration=False\n",
        "        )\n",
        "        test_dataset = test_dataset.shuffle(\n",
        "            buffer_size=BUFFER_SIZE,\n",
        "            reshuffle_each_iteration=False\n",
        "        )\n",
        "\n",
        "    partial_train_dataset = train_dataset.take(len(train_dataset) // 10 * 8)\n",
        "    val_dataset = train_dataset.skip(len(train_dataset) // 10 * 8)\n",
        "\n",
        "    print(\"The number of training set is\", len(train_dataset))\n",
        "    print(\"The number of partial training set is\", len(partial_train_dataset))\n",
        "    print(\"The number of validation set is\", len(val_dataset))\n",
        "\n",
        "    # In advance, batch and prefetch are called. Or, they are needed for each use.\n",
        "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    partial_train_dataset = partial_train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_dataset, partial_train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def tuplize(x):\n",
        "    \"\"\"\n",
        "    Transform a row from the dataset to learn.\n",
        "    :param x: a single row of the dataset.\n",
        "    :return: a tuple of the feature and the target.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        x['title'] + ' ' + x['description'],# x: feature\n",
        "        x['label']# y: target\n",
        "    )\n",
        "\n",
        "def build_text_vectorization_layer(\n",
        "        dataset,\n",
        "        max_tokens=25000,\n",
        "        output_mode='int',\n",
        "        sequence_length=50):\n",
        "    \"\"\"\n",
        "    Build a TextVectorization layer for preprocessing of text/token.\n",
        "    :param dataset: the tokenized text data.\n",
        "    :param max_tokens: the number of token.\n",
        "    :param output_mode: for the param of TextVectorization. multi_hot/int is expected.\n",
        "    :return: a TextVectorization layer.\n",
        "    \"\"\"\n",
        "    vectorization_layer = layers.TextVectorization(\n",
        "        max_tokens=max_tokens,\n",
        "        output_mode=output_mode,\n",
        "        output_sequence_length=sequence_length\n",
        "    )\n",
        "    vectorization_layer.adapt(dataset.map(lambda x: x['description'] + ' ' + x['title']))\n",
        "    # The test set should not be adapted.\n",
        "    #vectorization_layer.adapt(test_dataset.map(lambda x: x['description'] + ' ' + x['title']))\n",
        "    return vectorization_layer\n",
        "\n",
        "def build_model(\n",
        "        vectorization_layer: keras.layers.TextVectorization,\n",
        "        max_tokens=25000,\n",
        "        embedding_dim=128,\n",
        "        intermediate_dim=32,\n",
        "        num_heads=4,\n",
        "        sequence_length=50,\n",
        "        name=None):\n",
        "    \"\"\"\n",
        "    Build a sequential model with the TextVectorization and Embedding.\n",
        "    :param vectorization_layer: the layer object where sentence is converted to int.\n",
        "    :param max_tokens: the number of token.\n",
        "    :param embedding_dim: the number of dimension for embedding.\n",
        "    :param intermediate_dim: the number of units.\n",
        "    :param num_heads: the number of heads.\n",
        "    :param sequence_length: the length of a sequence.\n",
        "    :param name: the name of the model.\n",
        "    :return: a sequential model.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "    x = vectorization_layer(inputs)\n",
        "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        vocabulary_size=max_tokens,\n",
        "        sequence_length=sequence_length,\n",
        "        embedding_dim=embedding_dim,\n",
        "    )(x)\n",
        "    x = keras_nlp.layers.TransformerEncoder(\n",
        "        intermediate_dim=intermediate_dim,\n",
        "        num_heads=num_heads\n",
        "    )(inputs=x)\n",
        "    x = keras.layers.GlobalMaxPooling1D()(x)\n",
        "    x = keras.layers.Dropout(0.2)(x)\n",
        "    outputs = keras.layers.Dense(4, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss='sparse_categorical_crossentropy', \n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of training set is 120000\n",
            "The number of partial training set is 96000\n",
            "The number of validation set is 24000\n",
            "Epoch 1/20\n",
            "55/94 [================>.............] - ETA: 17s - loss: 1.0790 - accuracy: 0.5874"
          ]
        }
      ],
      "source": [
        "VOCAB_SIZE = 25000\n",
        "BATCH_SIZE = 1024\n",
        "NUM_HEADS = 2\n",
        "INTERMEDIATE_DIM = 32\n",
        "SEQ_LENGTH = 100\n",
        "\n",
        "for batch_size in [1024, 512, 256, 128]:\n",
        "    train_dataset, partial_train_dataset, val_dataset, test_dataset = load_ag_news_subset(\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    vectorization_layer = build_text_vectorization_layer(\n",
        "        dataset=train_dataset,\n",
        "        max_tokens=VOCAB_SIZE,\n",
        "        output_mode='int',\n",
        "        sequence_length=SEQ_LENGTH\n",
        "    )\n",
        "    model = build_model(\n",
        "        vectorization_layer,\n",
        "        max_tokens=VOCAB_SIZE,\n",
        "        intermediate_dim=INTERMEDIATE_DIM,\n",
        "        num_heads=NUM_HEADS,\n",
        "        sequence_length=SEQ_LENGTH\n",
        "    )\n",
        "    history = model.fit(\n",
        "        partial_train_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
        "        validation_data=val_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
        "        epochs=20,\n",
        "        batch_size=batch_size,\n",
        "        verbose=1,\n",
        "        callbacks =[\n",
        "            keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
        "        ]\n",
        "    )\n",
        "    plot(history=history, title=f\"batch_size={batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "s_qNSzzyaCbD"
      ],
      "name": "transformer.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
