{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# !tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras_nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is running on M1/M2 mac.\n"
          ]
        }
      ],
      "source": [
        "# Sometimes, the learning with tensorflow-metal does not converge.\n",
        "# @see\n",
        "# - https://forums.developer.apple.com/forums/thread/736187\n",
        "# - https://forums.developer.apple.com/forums/thread/701056\n",
        "# - https://forums.developer.apple.com/forums/thread/742157\n",
        "# Therefore, it might be best to execute learning without metal and execute learning again in the last tuning phase.\n",
        "if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
        "    print(\"This is running on M1/M2 mac.\")\n",
        "    tf.config.set_visible_devices([], 'GPU')\n",
        "else:\n",
        "    print(\"This is not running on M1/M2 mac.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEQUENCE_LENGTH = 100\n",
        "MAX_TOKENS = 15000\n",
        "EMBEDDING_DIM = 256\n",
        "INTERMIDIATE_DIM = 2048\n",
        "NUM_HEADS = 2\n",
        "LEARNING_RATE = 2e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 100006 files belonging to 1 classes.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb\",\n",
        "    label_mode=None,\n",
        "    batch_size=256\n",
        ")\n",
        "dataset = dataset.map(\n",
        "    lambda x: tf.strings.regex_replace(x, \"<br />\", \" \")\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_vectorization = keras.layers.TextVectorization(\n",
        "    max_tokens=MAX_TOKENS,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQUENCE_LENGTH,\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "    vectorized_sequences = text_vectorization(text_batch)\n",
        "    x = vectorized_sequences[:, :-1]\n",
        "    y = vectorized_sequences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " token_and_position_embeddi  (None, None, 256)            3865600   ['input_1[0][0]']             \n",
            " ng (TokenAndPositionEmbedd                                                                       \n",
            " ing)                                                                                             \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            1578752   ['token_and_position_embedding\n",
            " formerDecoder)                                                     [0][0]',                      \n",
            "                                                                     'token_and_position_embedding\n",
            "                                                                    [0][0]']                      \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 15000)          3855000   ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9299352 (35.47 MB)\n",
            "Trainable params: 9299352 (35.47 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
        "    \"\"\"\n",
        "    Apple Silicon mac shows tht following warning.\n",
        "    WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
        "    please use the legacy Keras optimizer instead,\n",
        "    located at `tf.keras.optimizers.legacy.Adam`\n",
        "    Therefore, keras.optimizers.legacy.Adam is used.\n",
        "    \"\"\"\n",
        "    optimizer = keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
        "else:\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "  \n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=MAX_TOKENS,\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        ")(inputs)\n",
        "x = keras_nlp.layers.TransformerDecoder(\n",
        "    intermediate_dim=INTERMIDIATE_DIM,\n",
        "    num_heads=NUM_HEADS\n",
        ")(x, x)\n",
        "outputs = keras.layers.Dense(\n",
        "    MAX_TOKENS,\n",
        "    activation=\"softmax\"\n",
        ")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=optimizer,\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "            self,\n",
        "            prompt,\n",
        "            generate_length,\n",
        "            model_input_length,\n",
        "            temperatures=(1.,),\n",
        "            print_freq=1):\n",
        "        self.prompt = prompt\n",
        "        self.generate_length = generate_length\n",
        "        self.model_input_length = model_input_length\n",
        "        self.temperatures = temperatures\n",
        "        self.print_freq = print_freq\n",
        "  \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.print_freq != 0:\n",
        "            return\n",
        "        for temperature in self.temperatures:\n",
        "            sentence = self.prompt\n",
        "            for i in range(self.generate_length):\n",
        "                tokenized_sentence = text_vectorization([sentence])\n",
        "                predictions = self.model(tokenized_sentence)\n",
        "                next_token = sample_next(predictions[0, i, :])\n",
        "                sampled_token = tokens_index[next_token]\n",
        "                sentence += \" \" + sampled_token\n",
        "            print(f\"\\nTemperature {temperature}: {sentence}\")\n",
        "\n",
        "prompt = \"This movie\" \n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=50,\n",
        "    model_input_length=SEQUENCE_LENGTH,\n",
        "    temperatures=(0., 0.2, 0.5, 0.7, 1., 1.5)\n",
        ")\n",
        "\n",
        "class EpochModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
        "    def __init__(\n",
        "        self,\n",
        "        filepath,\n",
        "        frequency=1,\n",
        "        monitor='val_loss',\n",
        "        verbose=0,\n",
        "        save_best_only=False,\n",
        "        save_weights_only=False,\n",
        "        mode='auto',\n",
        "        options=None,\n",
        "        **kwargs):\n",
        "        super(EpochModelCheckpoint, self).__init__(\n",
        "            filepath,\n",
        "            monitor,\n",
        "            verbose,\n",
        "            save_best_only,\n",
        "            save_weights_only,\n",
        "            mode,\n",
        "            \"epoch\",\n",
        "            options\n",
        "        )\n",
        "        self.epochs_since_last_save = 0\n",
        "        self.frequency = frequency\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.epochs_since_last_save += 1\n",
        "        if self.epochs_since_last_save % self.frequency == 0:\n",
        "            self._save_model(epoch=epoch, batch=None, logs=logs)\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        pass\n",
        "\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='example_transformer_next_word_prediction.keras',\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "202/391 [==============>...............] - ETA: 5:30 - loss: 8.8804"
          ]
        }
      ],
      "source": [
        "model.fit(\n",
        "    lm_dataset,\n",
        "    epochs=200,\n",
        "    callbacks=[\n",
        "        text_gen_callback,\n",
        "        model_checkpoint_callback,\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "s_qNSzzyaCbD"
      ],
      "name": "transformer.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
