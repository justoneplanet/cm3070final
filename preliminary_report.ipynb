{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "- **[Introduction](#introduction)**: this will explain the project concept and motivation for the project (this can be based on your proposal). This must also state which project template you are using.  (max 1000 words)\n",
    "- **[Literature Review](#literature-review)**: this is a revised version of the document that you submitted for your second peer review (max 2500 words)\n",
    "- **[Design](#design)**: this is a revised version of the document that you submitted for your third peer review (max 2000 words)\n",
    "- **[Feature Prototype](#feature-prototype)**: this is the only new element of the submission, details below (max 1500 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Introduction\n",
    "\n",
    "In this project, I'm going to use CNN daily mail datasets to build a text summarization model. Text summarization task means generating a short organized summary from a long article, such as news, through extracting and abstracting.\n",
    "\n",
    "This will be a research project in which I investigate recent trends and deepen my knowledge of Deep Learning, which goes beyond the contents of CM3015. A text summarization model is eventually built.\n",
    "\n",
    "In addition, this project supposes an organization or a group is the user, such as a (web) system development company with little machine-learning knowledge. Therefore, the deliverable report must be scientific and convincing to explain why each parameter is adopted and be knowledgeable for its developers. The deliverable code and model must run easily on common devices, such as a web server. Mobile devices, on the other hand, might not be able to run the large model because they have a smaller amount of memory.\n",
    "\n",
    "## What is interesting and motivative (Rapid Evolution) \n",
    "\n",
    "In recent years, various types of text representations and neural network models have been developed, especially since the Transformer model was invented.\n",
    "\n",
    "### Text representation\n",
    "\n",
    "In addition to the One-hot-encoding introduced in CM3015, I am going to use other techniques, such as word2vec and GloVe, that can appropriately handle the meanings of each word.\n",
    "\n",
    "The neural network is a vector transformation. It is worth investigating how well each representation of embedding layers can capture the characteristics of text data.\n",
    "\n",
    "### Transformer\n",
    "\n",
    "This is one of the most common and game-changing models for sequential data, such as text. In addition, it is also used in text-to-image generative AI, LLM, and so on. There are a lot of \n",
    "derived models based on the Transformer architecture and one of them is BERT, which is superior to natural language processing. [10] Another model based on the Transformer, named Vision Transformer, has been used for image recognition tasks to overcome CNN's weaknesses recently. [9] Moreover, there is a more efficient model based on the Transformer model, which is called the Retentive Network. Because all these architectures are based on the Transformer model, deepening its knowledge leads to the understanding of whole machine learning. And, even in the future, a number of new architectures based on the Transformer models will be released. That makes me motivated more.\n",
    "\n",
    "## What is interesting and motivative (from the aspect of efficiency and ecology)\n",
    "\n",
    "Measuring the performances of various models is one of the exciting and essential points.\n",
    "\n",
    "Advanced models do not always achieve the best performance. For example, when the Recurrent Neural Network model, which is one of the advanced models, is used for a text classification task, even though it does not always work better than a dense model, it mostly consumes greater computing resources. [11] That is, it completely wastes resources. I am going to analyze the strengths and weaknesses of each model in terms of practical uses.\n",
    "\n",
    "As recent investigations indicate, building LLM models indirectly emits a significant amount of CO2. That is, the investigation of efficiency that leads to the ecology is socially meaningful, even in the environmental aspect.\n",
    "\n",
    "However, since there are not enough computing resources to build an LLM model, this project does not target building an LLM itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Literature Review\n",
    "\n",
    "I aim to explore the current understanding of machine learning model architectures and their performances, focusing on the key technology \"Transformer\". Moreover, in this project, I investigate the effectiveness of the Transformer and finally build a text summarization model.\n",
    "\n",
    "## Transformer\n",
    "\n",
    "Vaswani et al. (2017) proposed a new simple architecture, named Transformer, based on attention mechanisms without recurrence and convolutions. [1]\n",
    "\n",
    "The architecture of the Transformer, which consists of both encoders and decoders, is introduced with a simple figure.\n",
    "\n",
    "The paper indicates the algorithms and architecture of the Transformer with formulas, comparing the former architectures such as the recurrent neural network model and the gate recurrent unit model. How to reduce the amount of calculation for longer input sequences, which is one of the defects, is also introduced kindly. However, because of the nature of the paper, the concrete implementation is not shown on it. Instead, it provides a link to a GitHub repository, \"tensor2tensor,\" [2] where some implementations with Python code are introduced.\n",
    "\n",
    "Though these codes might still be beneficial, They have not been maintained for years. It is pretty uncertain whether it can run on the current Python environment.\n",
    "\n",
    "Because the Transformer was initially developed for the machine translation task, various tasks except translation are not mentioned in this paper, written in 2017. Thereby, the effectiveness of the Transformer is not investigated for a wide range of NLP tasks here.\n",
    "\n",
    "As mentioned at the beginning, hyperparameter tunings are obviously required to investigate effectiveness. The section \"6.2 Model Variations\" describes the parameters such as the number of heads, the parameters of the optimizer, and so on that exist in the Transformer model, and the same indicators will be used to compare the performance in this project. And the experiment environment, which the paper disclosed, is useful to the experiments in this project.\n",
    "\n",
    "I have obtained the following beneficial knowledge, even though it is not directly related to my project.\n",
    "\n",
    "- The number of heads is neither too many nor too few\n",
    "- Interpretable models\n",
    "\n",
    "There are not following items that are essential for this project in the paper.\n",
    "\n",
    "- Sufficient benchmarks for various NLP tasks/applications\n",
    "- Minimum viable and runnable code in the modern environment\n",
    "\n",
    "## RetNet\n",
    "\n",
    "Yutao et al. (2023) proposed the Retentive Network (RetNet), which is a strong successor to Transformer for large language models.[3]\n",
    "\n",
    "One of the drawbacks of the Transformer, which is introduced in this paper, is the slow inference and inefficiency due to multi-head attention.\n",
    "The performance comparison between the RetNet and the Transformer has been executed for large language models and represents the improvement. They are executed and compared for large language models and that difference might not be remarkable on smaller models such as text summarization tasks.\n",
    "\n",
    "The section \"3 Experiments\" mentions the following:\n",
    "\n",
    "> Perplexity decreases along with scaling up the model size. We empirically observe that RetNet tends to outperform Transformer when the model size is larger than 2B. [3]\n",
    "\n",
    "When the model size is larger than 2.7B, with more than 2.7 billion parameters, the advantages of the RetNet model might finally be detected.\n",
    "\n",
    "The section \"3.3 Training Cost\" shows the specific environment where the evaluation of this paper was executed as follows.\n",
    "\n",
    "> We evaluate the results with eight Nvidia A100-80GB GPUs, because FlashAttention is highly optimized for A100. Tensor parallelism is enabled for 6.7B and 13B models.\n",
    "\n",
    "Building a text summarization model in this project is mainly executed on the local macOS machine, which has an M2 CPU, which has a significantly different architecture from Nvidia GPUs.\n",
    "\n",
    "> Moreover, without relying on specific kernels, it is easy to train RetNet on other platforms efficiently. For example, we train the RetNet models on an AMD MI200 cluster with decent throughput. It is notable that RetNet has the potential to further reduce cost via advanced implementation, such as kernel fusion.\n",
    "\n",
    "Even though the above quote mentioned the performance on the other device, the environment is highly parallelized, and it is uncertain if this project can derive beneficial knowledge through experimental comparisons.\n",
    "\n",
    "Though this does not matter directly to this project, as the sections \"2.3 Overall Architecture of Retention Networks\" and \"3.4 Inference Cost\" mention, the inference cost of the RetNet model is remarkably $O(1)$. [3] In the case where the quick response for longer sequences/tokens is required, the RetNet model can be one of the realistic choices.\n",
    "\n",
    "## Neural Text Summarization: A Critical Evaluation\n",
    "\n",
    "Wojciech et al. (2019) introduced and evaluated the current shortcomings of text summarization.[4] The methodology of the evaluation should be notable in this paper. The previously introduced 2 papers are not for text summarization tasks, and the BLEU metric is used, which is for the evaluation of translation task performance. This is not for summarization tasks. Though loss values probably evaluate the performances, they cannot clearly consider sentences' structures for evaluations. Therefore, following this paper, the ROUGE metric is used in this project. Fortunately, KerasNLP has the following 2 metrics as standard.\n",
    "\n",
    "- [ROUGE-N](https://keras.io/api/keras_nlp/metrics/rouge_n/)\n",
    "- [ROUGE-L](https://keras.io/api/keras_nlp/metrics/rouge_l/)\n",
    "\n",
    "Note that the ROUGE-N metric uses n-gram to refer to the overlap, and the ROUGE-L metric uses the longest common subsequence that can detect the sentence level structure. [8]\n",
    "\n",
    "Though the following doesn't matter directly, some shortcomings of the current summarization model are introduced here.\n",
    "\n",
    "- Even though a summarization task depends on the reader's expectations and prior knowledge, the current models are not provided as additional information.\n",
    "- Even though the ROUGE metric is widely used, it does not factually and consistently examine summarized text, but just lexically.\n",
    "- The bias and diversity of the dataset.\n",
    "\n",
    "## Text Summarization with Pretrained Encoders\n",
    "\n",
    "Yang et al. (2019) indicated the effectiveness of the BERT, which is an abbreviation of Bidirectional Encoder Representations from Transformers, for text summarization, which is one of the NLP tasks. [5]\n",
    "\n",
    "In this paper, the BERT model, where a pretrained model and a scratch model are combined, is compared with the other typical models for text summarization tasks, and it is concluded that the BERT model outperformed others. However, it seems slightly unclear whether the pretrained model actually works better, because there is not a comparison of the same architecture model between pretrained and scratch.\n",
    "\n",
    "As the section \"Introduction\" mentions, the BERT model, which is one of the Transformer models and the abbreviation of \"Bidirectional Encoder Representations from Transformers\", has affected the word and sentence representation. That is, this model possibly becomes a research target in this project.\n",
    "\n",
    "> When fine-tuning for a specific task, unlike ELMo whose parameters are usually fixed, parameters in BERT are jointly fine-tuned with additional task- specific parameters. [5]\n",
    "\n",
    "Whereas, as the above is written, since the pretrained BERT model goes through the fine-tuning phase, it might take longer time for training than fixed parameters' models such as word2vec and GloVe.\n",
    "\n",
    "The table of the section \"3.3 Abstractive Summarization\" holds the various type of datasets.\n",
    "Though this might not matter to the paper, apart from CNN DailyMail, NYT, and XSum are used as the datasets. In this project, the CNN dailymail is going to be used as the dataset. However, they are kept as sub plans, preparing for unexpected situations.\n",
    "\n",
    "> Each token $w_i$ is assigned three kinds of embeddings: token embeddings indicate the meaning of each token, segmentation embeddings are used to discriminate between two sentences (e.g., during a sentence-pair classification task) and position embeddings indicate the position of each token within the text sequence. [5]\n",
    "\n",
    "If the BERT model is adopted, the differences of embeddings must be paid attention.\n",
    "\n",
    "## facebook/bart-large-cnn\n",
    "\n",
    "This is a text summarization model developed by facebook that is available on Hugging Face. [30]\n",
    "\n",
    "Particularly noteworthy is the model size and its performance. The model size, the number of parameters, is 406M, and the results of ROUGE-1, ROUGE-2, and ROUGE-L are 42.949, 20.815, and 30.619 respectively.\n",
    "\n",
    "In addition, pre-trained BART is used in this text summarization model.\n",
    "\n",
    "> BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. [30]\n",
    "\n",
    "> BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs. [30]\n",
    "\n",
    "## google/pegasus-cnn_dailymail\n",
    "\n",
    "This is another text summarization model developed by Google that is available on Hugging Face. [31]\n",
    "\n",
    "This model has been trained not only with the CNN dailymail dataset but also with many others. It has 568M parameters and consists of 12 Transformer encoder layers and 12 Transformer decoder layers. The sinusoidal method is used for the positional embedding. The number of vocabulary is 50265.\n",
    "\n",
    "In addition, the paper \"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\" shows that the results of ROUGE-1, ROUGE-2, and ROUGE-L are 44.16, 21.56, and 41.30 respectively. [32]\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, the superiority and effectiveness of the Transformer and its self-attention have been shown, compared with the other architectures such as the recurrent neural network and convolutional neural network. Whereas performances, tunings, other NLP tasks, and architectures, which determine if encode or/and decode is used, have not been shown. And that is what I aim to do.\n",
    "\n",
    "I have slightly changed my mind while reading papers. Specifically, instead of regarding the RetNet model as the main topic, hyperparameter tunings of the Transformer model are going to be mainly researched. The RetNet model will be researched optionally. The reason is as follows.\n",
    "\n",
    "- The RetNet paper shows that it is effective only for large language models.\n",
    "- The experiments on the ResNet paper were executed on the highly parallelized environment, which is different from the local machine that is used in this project.\n",
    "- The training to build a text summarization model takes a considerable amount of time.\n",
    "\n",
    "That is, because there is a possibility where a beneficial report cannot be provided if most of resources are invested to experiments with little chance of seeing differences, I am going to make the experiment about the RetNet model optional. Instead, clearly beneficial hyperparameter tunings are executed in the first half to priorly ensure the benefits for users.\n",
    "\n",
    "Moreover, the comparison between the pre-trained models and non-pre-trained models will also be one of the interesting topics.\n",
    "\n",
    "Finally, it might be very difficult to self-build a text summarization model and mark a better score of ROUGE metrics on the current local machine because the models of Google and Facebook are trained with a number of layers and parameters using high computational GPUs. However, it might be possible to build a smaller model that is beneficial for a specific environment. Since KerasNLP provides an easy way to use BART, the performance might be able to get close to the Google or Facebook results if it is used. [33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Design\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "I have chosen the template \"Deep Learning on a public dataset\" of CM3015 \"Machine Learning and Neural Networks\". In this project, I am going to build a text summarization model with the CNN dailymail datasets.\n",
    "\n",
    "Unlike other templates of other courses, the deliverables the machine learning model and its documents, and this project does not have the user interfaces for end users. However, I assume to deliver the machine learning model to an organization that has a plan where the text summarization model is integrated into a system, which has actual end users.\n",
    "\n",
    "There is also the aspect of a research project where I investigate recent trends and deepen my knowledge of Deep Learning that was not sufficiently covered in the CM3015 course. Day by day and month by month, the field and market of deep learning technology for natural language processing is rapidly growing. The deliverable document is going to be beneficial for developers.\n",
    "\n",
    "## Domain and Users\n",
    "\n",
    "As I mentioned above, unlike other project templates, the project template on CM3015 does not have a specific user. However, since it is difficult without specific users in thinking what is beneficial and how beneficial a feature is, I assume that users are developers out of the organization who want to install a machine learning feature and model in to a current running system. Specifically that is as follows.\n",
    "\n",
    "They are not familiar with deep learning but with server side application of Python, and they want to adopt a text summarization feature into a current running system. My job is solving the problem and providing text summarization models and their knowledgeable and beneficial document to them.\n",
    "\n",
    "## Justify features based on domain and users\n",
    "\n",
    "Generally, end users do not mind if a model is highly advanced model or not. Instead, they mind the time and performance of response. \n",
    "\n",
    "### Why is the Transformer model used?\n",
    "\n",
    "In this project, text summarization models are built. That is, the neural network learns sequences as the input to output another sequences as the output. The Transformer model is superior to the Dense model to process sequences data. That is the reason why the Transformer model is used for the text summarization model.\n",
    "\n",
    "On text classification tasks, well-tuned dense layer models overcome advanced models, such as a Transformer model, an RNN model and so on, under certain conditions.\n",
    "\n",
    "However, text generation tasks, such as text summarization, could not achieve satisfactory outcomes for a long time. Encoder-decoder models, text embeddings, and Transformer which can handle sequences well have made them possible. That is why the Transformer model is used.\n",
    "\n",
    "### Why is hyperparameter tuning necessary?\n",
    "\n",
    "There are a number of hypterparameters in a neural network model, and there does not exist a formula that can derive optimized their values. They depend on each dataset and architecture. The best parameter set must be found with hyperparameter tunings.\n",
    "\n",
    "Prior to testing advanced models, the hyperparameter tunings should be executed sufficiently. Because even if an advanced model such as the RetNet outperforms a traditional model, it is impossible to fairly compare the advanced model and the traditional model without hyperparameter tunings.\n",
    "\n",
    "Fairly comparisons derive better models that are beneficial for users. That is the reason why hypter parameter tunings are necessary.\n",
    "\n",
    "### Why is the RetNet model reseached optionally?\n",
    "\n",
    "Optionally, the other cutting-edge models, such as the RetNet model, are examined. Generally, end users do not mind if a model is highly advanced model or not. Instead, they mind the time and performance of response. Whereas, it might be able to efficiently use resources, and that might lead to the reduction of the cost for users. This is one of the reason why the research of the RetNet model is made optional.\n",
    "\n",
    "## Structure of the project\n",
    "\n",
    "The main deliverable is a model for the text summarization task and its Jupiter notebook document. The model is provided with minimum viable code to run it. Multiple models are possibly provided because the difference in the size of each model affects the machine's requirements where it runs. The notebook's headings will be as follows.\n",
    "\n",
    "- Abstract\n",
    "- Introduction\n",
    "- Background\n",
    "    - Mathematic explanation\n",
    "    - Scientific description\n",
    "    - Technical description\n",
    "- Experiments/Methodology\n",
    "- Conclusion\n",
    "    - Best model\n",
    "    - Verification of hypotheses\n",
    "    - Future work\n",
    "- Citation/Reference\n",
    "\n",
    "## Key technologies and methods\n",
    "\n",
    "I am going to introduce libraries and technologies here. From the point of correctness and objectivity, I explain them in my own words as little as possible and I cite public documents, websites, and their Wikipedia page. Instead, I explain how each technology and library is used in my own words for the project.\n",
    "\n",
    "### Technology\n",
    "\n",
    "#### Transformer\n",
    "\n",
    "<img src=\"img/Screenshot 2024-06-08 at 19.19.07.png\">\n",
    "\n",
    "### Library\n",
    "\n",
    "#### TensorFlow\n",
    "\n",
    "This is a machine learning library, which is mainly used in this project.\n",
    "\n",
    "> An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources. [12]\n",
    "\n",
    "> TensorFlow is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. [13]\n",
    "\n",
    "In this project, the TensorFlow library is used not only for building text summarization models but also for most other tasks from the beginning to the end.\n",
    "\n",
    "#### Keras\n",
    "\n",
    "This is an OSS neural network library. It depends on the library version, it can switch Tensorflow to other machine learning library.\n",
    "\n",
    "> Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. Keras also gives the highest priority to crafting great documentation and developer guides. [14]\n",
    "\n",
    "> Keras is an open-source library that provides a Python interface for artificial neural networks. Keras was first independent software, then integrated into TensorFlow library, and later supporting more. [15]\n",
    "\n",
    "This library is used throughout the whole project to manipulate the TensorFlow library.\n",
    "\n",
    "#### KerasNLP\n",
    "\n",
    "> KerasNLP is a natural language processing library that works natively with TensorFlow, JAX, or PyTorch. Built on Keras 3, these models, layers, metrics, and tokenizers can be trained and serialized in any framework and re-used in another without costly migrations. [16]\n",
    "\n",
    "In this project, the Transformer model, which consists of the Transformer encoder layer and the Transformer decoder layer, is used to build text summarization models. It is possible to self-implement them by following the paper and books. However, it might unintentionally include bugs. Then, the model and its documents lose the reliability for readers. Therefore, the KerasNLP, which is a part of the widely known library \"Keras\" and contains the Transformer layer classes, is utilized here to secure the reliability.\n",
    "\n",
    "Moreover, a lot of hyperparameter tunings are executed in this project. Their classes in KerasNLP is well-architected so that hyperparameters are easily injected to each layer. This is another reason why the KerasNLP is used here.\n",
    "\n",
    "#### PyTorch\n",
    "\n",
    "This is another OSS machine learning library. PyTorch is introduced on Wikipedia as follows.\n",
    "\n",
    "> PyTorch is a machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella. It is recognized as one of the two most popular machine learning libraries alongside TensorFlow, offering free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface. [17]\n",
    "\n",
    "It is not directly used for the project development. However, some text summarization models that I have introduced has been developed with this PyTorch library.\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "This is a software library of Hugging Face, Inc. The official repository of GitHub introduces as follows.\n",
    "\n",
    "> Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. [18]\n",
    "\n",
    "> Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. [18]\n",
    "\n",
    "> Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. [18]\n",
    "\n",
    "I have investigated the following 2 text summarization models as a part of competitive research.\n",
    "\n",
    "- facebook/bart-large-cnn [19]\n",
    "- google/pegasus-cnn_dailymail [20]\n",
    "\n",
    "They are distributed through huggingface and the Transformers library is necessary to use these models.\n",
    "\n",
    "## Work Plan (Gantt Chart)\n",
    "\n",
    "The first 2 to 3 weeks are used to develop the basic Transformer model. Its part of the document is written at the same time. After that, hyperparameter tunings are repeatedly executed with small cycles so that they are efficient. The uncertainty of this phase is the length of training time. The optional implementation for the RetNet model keeps aside the time of 3 weeks. At last, there is a time for the documentation and the buffer.\n",
    "\n",
    "<img src=\"img/gantt_chart.png\">\n",
    "\n",
    "## Evaluation Plan\n",
    "\n",
    "Normal testing for machine learning models and hypothesis verification are executed to evaluate the project. However, the human tester is not used to check performance because it is difficult to organize the tester team for this project free of charge.\n",
    "\n",
    "### Testing\n",
    "\n",
    "For the test dataset, the following metrics are used to measure and test model performance. This is executed to numerically prove how better a generated model is.\n",
    "\n",
    "- loss\n",
    "- ROUGE-N\n",
    "- ROUGE-L\n",
    "\n",
    "### Verification of hypotheses\n",
    "\n",
    "The following hypotheses are verified.\n",
    "\n",
    "- Hyperparameter tunings are significantly effective even on a Transformer model.\n",
    "- Transformer models can generate text more sophisticatedly than dense layer models.\n",
    "- The encoder-decoder model of the Transformer works better for text summarization tasks than the only-decoder model.\n",
    "- Pre-trained models improve the text summarization performance.\n",
    "\n",
    "The following hypothesis is verified as a result of an optional implementation, if possible.\n",
    "\n",
    "- The RetNet architecture for a small model does not make a difference.\n",
    "\n",
    "Whereas, it is seemingly difficult to verify the following hypotheses in this project, even though they are interesting topics. The first one is a thought during the prototype implementation. A huge Transformer model is essential to verify the first hypothesis, and it is impossible for the current local environment to run. And no one has solved the second one as far as I have read a number of papers. That is, the task of text summarization is strongly related to human prior knowledge and understanding of the things that are summarized. Thus, I am going to exclude them from the verification of the project temporarily.\n",
    "\n",
    "- An excessive number of units and layers for the number of dataset entries makes overfitting.\n",
    "- Prior knowledge and bias cannot be solved on the current architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Feature Prototype\n",
    "\n",
    "Here are three types of machine learning tasks of the Transformer model.\n",
    "\n",
    "1. Transformer classification task\n",
    "2. Transformer text generation task\n",
    "3. Transformer text summarization task\n",
    "\n",
    "In the first classification task, it is indicated that the Transformer parts in the Keras and KerasNLP library actually work. The second text generation task shows how long a generative task takes time. It is essential to estimate the working time for a lot of experiments that are executed for performance comparisons. The third one is a minimum viable code for the text summarization task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text classification task\n",
    "\n",
    "Firstly, a Transformer classification model is built so that the local environment determines if it can run or not. This time, the KerasNLP library is used to add the encoder and decoder layers of the Transformer. The reasons why the KerasNLP library is utilized are as follows.\n",
    "\n",
    "- Stability\n",
    "- Parameters for experiments\n",
    "\n",
    "The book \"Deep Learning with Python, Second Edition\" [7] introduces the simplified Transformer implementation that actually works in the local environment. However, it is not certain how widely it is used and it might not pass any testing. In addition, even though it is good for us to understand how the Transformer works internally because it is simplified, it is difficult for us to experiment with various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: keras-core in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.7)\n",
      "Requirement already satisfied: absl-py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.26.1)\n",
      "Requirement already satisfied: packaging in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (23.1)\n",
      "Requirement already satisfied: regex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (2023.5.5)\n",
      "Requirement already satisfied: rich in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (13.7.1)\n",
      "Requirement already satisfied: dm-tree in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.8)\n",
      "Requirement already satisfied: kagglehub in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.2.5)\n",
      "Requirement already satisfied: requests in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (2.30.0)\n",
      "Requirement already satisfied: tqdm in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (4.65.0)\n",
      "Requirement already satisfied: namex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (3.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install KerasNLP\n",
    "%pip install --upgrade keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 25000\n",
    "BATCH_SIZE = 4096\n",
    "NUM_HEADS = 4\n",
    "INTERMEDIATE_DIM = 64\n",
    "SEQ_LENGTH = 100\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ag_news_dataset(batch_size):\n",
    "    \"\"\"\n",
    "    Load ag_news_subset dataset.\n",
    "    :param batch_size: the number of batch size.\n",
    "    :return: a dataset object.\n",
    "    \"\"\"\n",
    "    dataset = tfds.load('ag_news_subset')\n",
    "    train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "    # Devide the dataset for training and validation in the ratio 8:2.\n",
    "    partial_train_dataset = train_dataset.take(len(train_dataset) // 10 * 8)\n",
    "    val_dataset = train_dataset.skip(len(train_dataset) // 10 * 8)\n",
    "\n",
    "    # For loading performance\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    partial_train_dataset = partial_train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, partial_train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def tuplize(x):\n",
    "    \"\"\"\n",
    "    Transform a row from the dataset to learn.\n",
    "    :param x: a single row of the dataset.\n",
    "    :return: a tuple of the feature and the target.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        x['title'] + ' ' + x['description'],# x: feature\n",
    "        x['label']# y: target\n",
    "    )\n",
    "\n",
    "def build_model(\n",
    "        vectorization_layer: keras.layers.TextVectorization,\n",
    "        max_tokens=25000,\n",
    "        embedding_dim=128,\n",
    "        intermediate_dim=32,\n",
    "        num_heads=4,\n",
    "        sequence_length=50):\n",
    "    \"\"\"\n",
    "    Build a Transformer encoder model for text classification task.\n",
    "    :param vectorization_layer: the layer object where sentence is converted to int.\n",
    "    :param max_tokens: the number of token.\n",
    "    :param embedding_dim: the number of dimension for embedding.\n",
    "    :param intermediate_dim: the number of units.\n",
    "    :param num_heads: the number of heads.\n",
    "    :param sequence_length: the length of a sequence.\n",
    "    :return: a sequential model.\n",
    "    \"\"\"\n",
    "    if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "        \"\"\"\n",
    "        Apple Silicon mac shows tht following warning.\n",
    "        WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "        please use the legacy Keras optimizer instead,\n",
    "        located at `tf.keras.optimizers.legacy.Adam`\n",
    "        Therefore, keras.optimizers.legacy.Adam is used.\n",
    "        \"\"\"\n",
    "        optimizer = keras.optimizers.legacy.Adam()\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam()\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorization_layer(inputs)\n",
    "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=max_tokens,\n",
    "        sequence_length=sequence_length,\n",
    "        embedding_dim=embedding_dim,\n",
    "    )(x)\n",
    "    x = keras_nlp.layers.TransformerEncoder(\n",
    "        intermediate_dim=intermediate_dim,\n",
    "        num_heads=num_heads\n",
    "    )(inputs=x)\n",
    "    x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    outputs = keras.layers.Dense(4, activation=\"softmax\")(x)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        name=\"transformer_text_classification_model\"\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 10:57:27.202536: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 62s 2s/step - loss: 1.5487 - accuracy: 0.4376 - val_loss: 0.5694 - val_accuracy: 0.8403\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 64s 3s/step - loss: 0.5142 - accuracy: 0.8239 - val_loss: 0.3300 - val_accuracy: 0.8964\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 65s 3s/step - loss: 0.2953 - accuracy: 0.9063 - val_loss: 0.2842 - val_accuracy: 0.9087\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 67s 3s/step - loss: 0.2053 - accuracy: 0.9378 - val_loss: 0.2670 - val_accuracy: 0.9129\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 65s 3s/step - loss: 0.1505 - accuracy: 0.9579 - val_loss: 0.2599 - val_accuracy: 0.9160\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 61s 3s/step - loss: 0.1133 - accuracy: 0.9716 - val_loss: 0.2571 - val_accuracy: 0.9167\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 63s 3s/step - loss: 0.0871 - accuracy: 0.9807 - val_loss: 0.2574 - val_accuracy: 0.9164\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 63s 3s/step - loss: 0.0684 - accuracy: 0.9869 - val_loss: 0.2611 - val_accuracy: 0.9149\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 63s 3s/step - loss: 0.0551 - accuracy: 0.9908 - val_loss: 0.2647 - val_accuracy: 0.9145\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 66s 3s/step - loss: 0.0458 - accuracy: 0.9937 - val_loss: 0.2657 - val_accuracy: 0.9158\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 68s 3s/step - loss: 0.0386 - accuracy: 0.9954 - val_loss: 0.2765 - val_accuracy: 0.9143\n"
     ]
    }
   ],
   "source": [
    "train_dataset, partial_train_dataset, val_dataset, test_dataset = load_ag_news_dataset(\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "vectorization_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQ_LENGTH\n",
    ")\n",
    "vectorization_layer.adapt(train_dataset.map(lambda x: x['description'] + ' ' + x['title']))\n",
    "model = build_model(\n",
    "    vectorization_layer,\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    intermediate_dim=INTERMEDIATE_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    sequence_length=SEQ_LENGTH\n",
    ")\n",
    "model.summary()\n",
    "history = model.fit(\n",
    "    partial_train_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
    "    validation_data=val_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks =[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text generation task\n",
    "\n",
    "Now, the following implementation, which is partially based on the book \"Deep Learning with Python\" [6], tested that how long a text generation task takes so that the time for the text summarization task is estimated. The points of difference from the above classification task are mainly as follows.\n",
    "\n",
    "- The number of units at the output layer.\n",
    "- The number of layers and units at the hidden layers.\n",
    "- The number of epochs\n",
    "\n",
    "In the generative task, such as text summarization, these numbers generally get larger than the text classification model. As a result, it takes a significant amount of time for training. In this case, it took over 4 hours. Thus, if the encoder-decoder Transformer model is built for the text summarization task, the number of parameters will be increased at least 15% and the training time will be longer 15%. That is one of the reasons why I have changed my plan, where the RetNet model is an optional topic. Even though the Retentive Network model will make the training 8.4 times faster in the aspect of the throughput, which is shown in the paper, the same Transformer model training must be executed for the comparison. [3] The experiments will still take a lot of time, and that is not realistic.\n",
    "\n",
    "If the investigation of the RetNet model is the main topic and consumes most of the time, notable differences may not be derived as a result of fewer training attempts. The deliverable report loses the benefit for users and this report is devalued. Therefore, the RetNet model should be an optional topic to avoid that case, and other comparisons, such as text representations, should be preferred. That must make this report more beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "SEQUENCE_LENGTH = 50\n",
    "MAX_TOKENS = 15000\n",
    "EMBEDDING_DIM = 256\n",
    "INTERMIDIATE_DIM = 2048\n",
    "NUM_HEADS = 2\n",
    "LEARNING_RATE = 2e-6 #  changed from 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(text_batch):\n",
    "    vectorized_sequences = text_vectorization(text_batch)\n",
    "    x = vectorized_sequences[:, :-1]\n",
    "    y = vectorized_sequences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "text_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"aclImdb\",\n",
    "    label_mode=None,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda x: tf.strings.regex_replace(x, \"<br />\", \" \")\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(dataset)\n",
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_text_generation_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, None, 256)            3852800   ['input_6[0][0]']             \n",
      " ng_5 (TokenAndPositionEmbe                                                                       \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " transformer_decoder_4 (Tra  (None, None, 256)            1578752   ['token_and_position_embedding\n",
      " nsformerDecoder)                                                   _5[0][0]',                    \n",
      "                                                                     'token_and_position_embedding\n",
      "                                                                    _5[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, None, 15000)          3855000   ['transformer_decoder_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9286552 (35.43 MB)\n",
      "Trainable params: 9286552 (35.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "    \"\"\"\n",
    "    Apple Silicon mac shows tht following warning.\n",
    "    WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "    please use the legacy Keras optimizer instead,\n",
    "    located at `tf.keras.optimizers.legacy.Adam`\n",
    "    Therefore, keras.optimizers.legacy.Adam is used.\n",
    "    \"\"\"\n",
    "    optimizer = keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "else:\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Build model\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=MAX_TOKENS,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(inputs)\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    "    num_heads=NUM_HEADS\n",
    ")(x, x)\n",
    "outputs = keras.layers.Dense(\n",
    "    MAX_TOKENS,\n",
    "    activation=\"softmax\"\n",
    ")(x)\n",
    "model = keras.Model(\n",
    "    inputs,\n",
    "    outputs,\n",
    "    name=\"transformer_text_generation_model\",\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Choose the next token, using the temperature.\n",
    "    :param predictions: \n",
    "    :param temperature: \n",
    "    :return: the next token index\n",
    "    \"\"\"\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\"\"\"\n",
    "Generate a sentence with the latest model on every epoch.\n",
    "\"\"\"\n",
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt,\n",
    "            generate_length,\n",
    "            model_input_length,\n",
    "            temperatures=(1.,),\n",
    "            print_freq=1):\n",
    "        self.prompt = prompt\n",
    "        self.generate_length = generate_length\n",
    "        self.model_input_length = model_input_length\n",
    "        self.temperatures = temperatures\n",
    "        self.print_freq = print_freq\n",
    "  \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.print_freq != 0:\n",
    "            return\n",
    "        print(\"\\n\")\n",
    "        for temperature in self.temperatures:\n",
    "            sentence = self.prompt\n",
    "            for i in range(self.generate_length):\n",
    "                tokenized_sentence = text_vectorization([sentence])\n",
    "                predictions = self.model(tokenized_sentence)\n",
    "                next_token = sample_next(\n",
    "                    predictions=predictions[0, i, :],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                sampled_token = tokens_index[next_token]\n",
    "                sentence += \" \" + sampled_token\n",
    "            print(f\"Temperature {temperature}: {sentence}\")\n",
    "\n",
    "prompt = \"This movie\" \n",
    "text_gen_callback = TextGenerator(\n",
    "    prompt,\n",
    "    generate_length=50,\n",
    "    model_input_length=SEQUENCE_LENGTH,\n",
    "    temperatures=(0.2, 0.5, 0.7, 1., 1.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "209/391 [===============>..............] - ETA: 1:03 - loss: 9.5946"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    dataset,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=200,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        text_gen_callback,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text summarization task\n",
    "\n",
    "Finally, to understand the outline, the minimum viable implementation of text summarization task is below. Because the dataset contains only 2 entries, this model overfits 100%. This time, the encoder and decoder model of the Transformer is adopted. However, this is not a requirement of the text summarization model. It is not written that the decoder-only model of the Transformer can neither build the text summarization model nor achieve better performance. This will be an experiment in this report.\n",
    "\n",
    "Moreover, there are various types of text representation/vectorization as follows, including pre-trained models.\n",
    "\n",
    "- Static embeddings\n",
    "    - word2vec [22]\n",
    "    - fastText (more advanced than word2vec) [23]\n",
    "    - GloVe [24]\n",
    "- Dynamic embeddings\n",
    "    - BERT\n",
    "\n",
    "This is an important experiment. Because, if there exists an ultimate representation to express words and text sequence, the neural network holds the ability to fit the hyper-dimensions. That is, finding better vector representation is essential to improve the performance.\n",
    "\n",
    "Furthermore, there are many hyperparameters. Some of them are shown at the top of the following code.\n",
    "\n",
    "### Embedding layer\n",
    "\n",
    "The default Embedding layer of the Keras library is used in this project. [21] This layer vectorizes words in a sequence, and similar words are vectorized closely. An accurate description is on the official page, as follows.\n",
    "\n",
    "> Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn. [25]\n",
    "\n",
    "This layer has the following tunable hyperparameters.\n",
    "\n",
    "- The number of vocabulary\n",
    "- The number of embedding dimension\n",
    "- Whether the value 0 is masked or not as padding\n",
    "\n",
    "### PositionEmbedding layer\n",
    "\n",
    "The following sample code uses the PositionEmbedding layer of the KerasNLP library. [26] The KerasNLP library also has some positional embedding layers, such as the SinePositionEncoding layer that was originally used in the thesis. [27] [1] Whereas because the tunable hyperparameters are few, this project examines various types of positional embeddings here.\n",
    "\n",
    "- The number of sequence length\n",
    "\n",
    "### TransformerEncoder layer\n",
    "\n",
    "The default TransformerEncoder layer of the Keras library is used in this project. [28] This layer encodes text to the meaningful representation internally.\n",
    "\n",
    "- The number of layers\n",
    "- The number of hidden units\n",
    "- The number of heads\n",
    "- The dropout rate\n",
    "- The epsilon value for normalization\n",
    "\n",
    "### TransformerDecoder layer\n",
    "\n",
    "The default TransformerDecoder layer of the Keras library is used in this project. [29] This layer decodes the internally meaningful representation to the text.\n",
    "\n",
    "- The number of layers\n",
    "- The number of hidden units\n",
    "- The number of heads\n",
    "- The dropout rate\n",
    "- The epsilon value for normalization\n",
    "\n",
    "### Model\n",
    "\n",
    "- The number of epochs\n",
    "- The type of optimizer\n",
    "- The learning rate of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.1514 - accuracy: 0.0625\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1572 - accuracy: 0.0625\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2.3167 - accuracy: 0.3750\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.6607 - accuracy: 0.6250\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.1750 - accuracy: 0.8750\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8331 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5937 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.4249 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3065 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2277 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1756 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1396 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1132 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0929 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0771 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0647 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0550 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0474 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0415 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0369 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0333 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0304 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0280 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0261 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0194 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0052 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x62ff1ace0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "INTERMIDIATE_DIM = 256\n",
    "\n",
    "# Sample dataset.\n",
    "dataset = [\n",
    "    (\n",
    "        \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\",\n",
    "        \"<start> Giant pig fell into the swimming pool.\",\n",
    "        \"Giant pig fell into the swimming pool. <end>\",\n",
    "    ),\n",
    "    (\n",
    "        \"There are two chickens in the garden.\",\n",
    "        \"<start> There are chickens.\",\n",
    "        \"There are chickens. <end>\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "input_texts, target_texts, decoder_target_text = zip(*dataset)\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "    split=' ',\n",
    "    filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    ")\n",
    "tokenizer.fit_on_texts(input_texts + target_texts + decoder_target_text)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "decoder_target_sequences = tokenizer.texts_to_sequences(decoder_target_text)\n",
    "\n",
    "max_input_length = max(len(sequence) for sequence in input_sequences)\n",
    "max_target_length = max(len(sequence) for sequence in target_sequences)\n",
    "max_decoder_target_length = max(len(sequence) for sequence in decoder_target_sequences)\n",
    "\n",
    "input_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    input_sequences,\n",
    "    maxlen=max_input_length,\n",
    "    padding='post'\n",
    ")\n",
    "target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    target_sequences,\n",
    "    maxlen=max_target_length,\n",
    "    padding='post'\n",
    ")\n",
    "decoder_target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    decoder_target_sequences,\n",
    "    maxlen=max_decoder_target_length,\n",
    "    padding='post'\n",
    ")\n",
    "decoder_target_sequences = tf.expand_dims(decoder_target_sequences, axis=-1)\n",
    "\n",
    "# Build model / Encoder & Decoder model\n",
    "# The encoder encodes text and represents the feature vector.\n",
    "# However, the decoder scheme contains this working, especially in its heads.\n",
    "# That is, it is not certain whether the encoder is necessary for this task.\n",
    "# There is value in the investigation.\n",
    "encoder_inputs = keras.Input(\n",
    "    shape=(max_input_length,),\n",
    "    name=\"encoder_inputs\"\n",
    ")\n",
    "encoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_input_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(encoder_inputs)\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(encoder_embedding)\n",
    "\n",
    "decoder_inputs = keras.Input(\n",
    "    shape=(max_target_length,),\n",
    "    name=\"decoder_inputs\"\n",
    ")\n",
    "decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_target_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(decoder_inputs)\n",
    "decoder_outputs = keras_nlp.layers.TransformerDecoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "outputs = keras.layers.Dense(\n",
    "    vocabulary_size,\n",
    "    activation=\"softmax\"\n",
    ")(decoder_outputs)\n",
    "\n",
    "model = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    outputs,\n",
    "    name=\"transformer_text_summarization_model\",\n",
    ")\n",
    "# Note\n",
    "# In the case that the dataset is large and the dimension is small,\n",
    "# the learning rate of Adam needed to be smaller.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training\n",
    "model.fit(\n",
    "    [input_sequences, target_sequences],\n",
    "    decoder_target_sequences,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=100,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the classification task, the first word is predicted as the decoder output, and it is used for the second word prediction as the decoder input. By repeating this until the decoder outputs the special end symbol, the complete summarized sentence is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\n",
      "Summary: giant pig fell into the swimming pool\n",
      "Original: There are two chickens in the garden.\n",
      "Summary: there are chickens\n",
      "Original: Two chickens fell into the swimming pool in the garden.\n",
      "Summary: there are chickens\n"
     ]
    }
   ],
   "source": [
    "def summarize(text):\n",
    "    \"\"\"\n",
    "    Summarize text\n",
    "    :param text: original text\n",
    "    :return: summarized text\n",
    "    \"\"\"\n",
    "    input_sequence = tokenizer.texts_to_sequences([text])\n",
    "    input_sequence = keras.preprocessing.sequence.pad_sequences(\n",
    "        input_sequence,\n",
    "        maxlen=max_input_length,\n",
    "        padding='post'\n",
    "    )\n",
    "    idx = tokenizer.word_index['<start>']\n",
    "    decoder_input_sequence = tf.constant(\n",
    "        [[idx]],\n",
    "        dtype=tf.int64\n",
    "    )\n",
    "    \n",
    "    summary = []\n",
    "    for _ in range(max_target_length):\n",
    "        predictions = model.predict(\n",
    "            [input_sequence, decoder_input_sequence],\n",
    "            verbose=0\n",
    "        )\n",
    "        next_token = tf.argmax(predictions[0, -1, :])\n",
    "        next_word = tokenizer.index_word.get(next_token.numpy(), '<unk>')\n",
    "        if next_word == '<end>':\n",
    "            break\n",
    "        summary.append(next_word)\n",
    "        decoder_input_sequence = tf.concat(\n",
    "            [decoder_input_sequence, tf.expand_dims([next_token], axis=-1)],\n",
    "            axis=-1\n",
    "        )\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Sample\n",
    "sample_text = \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"There are two chickens in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"Two chickens fell into the swimming pool in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [1] Vaswani et al. (2017). Attention Is All You Need.https://doi.org/10.48550/arXiv.1706.03762\n",
    "- [2] tensorflow. Tensor2tensor. Retrieved May 16, 2024, from https://github.com/tensorflow/tensor2tensor\n",
    "- [3] Yutao et al. (2023). Retentive Network: A Successor to Transformer for Large Language Models. https://doi.org/10.48550/arXiv.2307.08621\n",
    "- [4] Wojciech et al. (2019). Neural Text Summarization: A Critical Evaluation. https://doi.org/10.48550/arXiv.1908.08960\n",
    "- [5] Yang et al. (2019). Text Summarization with Pretrained Encoders. https://doi.org/10.48550/arXiv.1908.08345\n",
    "- [6] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 266). Manning.\n",
    "- [7] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 246). Manning.\n",
    "- [8] ROUGE (metric). (2023, November 28). In Wikipedia. https://en.wikipedia.org/wiki/ROUGE_(metric)\n",
    "- [9] Dosovitskiy, A. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. https://doi.org/10.48550/arXiv.2010.11929\n",
    "- [10] BERT (language model). (2024, May 7). In Wikipedia. https://en.wikipedia.org/wiki/BERT_(language_model)\n",
    "- [11] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 250). Manning.\n",
    "- [12] Tensorflow. Retrieved June 6, 2024, from https://www.tensorflow.org\n",
    "- [13] TensorFlow. (2024, May 26). In Wikipedia. https://en.wikipedia.org/wiki/TensorFlow\n",
    "- [14] Keras: Deep Learning for humans. https://keras.io/\n",
    "- [15] Keras. (2024, June 6). In Wikipedia. https://en.wikipedia.org/wiki/Keras\n",
    "- [16] KerasNLP. Retrieved June 6, 2024, from https://keras.io/keras_nlp/\n",
    "- [17] PyTorch. (2024, May 10). In Wikipedia. https://en.wikipedia.org/wiki/PyTorch\n",
    "- [18] Hugging Face, Inc. Transformers. Retrieved June 6, 2024, from https://github.com/huggingface/transformers\n",
    "- [19] (2020, February 22). facebook/bart-large-cnn. Retrieved May 1, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [20] (2020, Aug 9). google/pegasus-cnn_dailymail. Retrieved May 1, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [21] Embedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/layers/core_layers/embedding/\n",
    "- [22] Word2vec. Tensorflow. Retrieved June 4, 2024, from https://www.tensorflow.org/text/tutorials/word2vec\n",
    "- [23] Facebook Inc. FastText. Retrieved June 4, 2024, from https://fasttext.cc/\n",
    "- [24] GloVe: Global Vectors for Word Representation. Retrieved June 4, 2024, from https://nlp.stanford.edu/projects/glove/\n",
    "- [25] Google LLC. Word embeddings. Retrieved June 10, 2024, from https://www.tensorflow.org/text/guide/word_embeddings#word_embeddings_2\n",
    "- [26] PositionEmbedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/position_embedding/\n",
    "- [27] Google LLC. SinePositionEncoding layer. Retrieved June 10, 2024, from https://keras.io/api/keras_nlp/modeling_layers/sine_position_encoding/\n",
    "- [28] TransformerEncoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/\n",
    "- [29] TransformerDecoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_decoder/\n",
    "- [30] facebook (2020, February 22). Facebook/bart-large-cnn. Retrieved June 10, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [31] Google (2020, August 9). Google/pegasus-cnn_dailymail. Retrieved June 10, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [32] Zhang, J. (2020). PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. https://doi.org/10.48550/arXiv.1912.08777\n",
    "- [33] Google (2023, July 8). Abstractive Text Summarization with BART. Retrieved June 10, 2024, from https://keras.io/examples/nlp/abstractive_summarization_with_bart/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
