{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "- **[Introduction](#introduction)**: this will explain the project concept and motivation for the project (this can be based on your proposal). This must also state which project template you are using.  (max 1000 words)\n",
    "- **[Literature Review](#literature-review)**: this is a revised version of the document that you submitted for your second peer review (max 2500 words)\n",
    "- **[Design](#design)**: this is a revised version of the document that you submitted for your third peer review (max 2000 words)\n",
    "- **[Feature Prototype](#feature-prototype)**: this is the only new element of the submission, details below (max 1500 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Introduction\n",
    "\n",
    "In this project, I am going to use CNN daily mail datasets to build a text summarization model. Text summarization task means generating a short organized summary from a long article, such as news, through extracting and abstracting.\n",
    "\n",
    "This will be a research project in which I investigate recent trends and deepen my knowledge of Deep Learning, which goes beyond the contents of CM3015, and a text summarization model is eventually built.\n",
    "\n",
    "In addition, this project supposes an organization or a group is the user, such as a (web) system development company with little machine-learning knowledge. Therefore, the deliverable report must be scientific and convincing to explain why each parameter is adopted and be knowledgeable for its developers. The deliverable code and model must run easily on common devices, such as a web server. Mobile devices, on the other hand, might not be able to run the large model because they have a smaller amount of memory.\n",
    "\n",
    "## What is interesting and motivative (Rapid Evolution) \n",
    "\n",
    "In recent years, various types of text representations and neural network models have been developed, especially since the Transformer model was invented.\n",
    "\n",
    "### Text representation\n",
    "\n",
    "In addition to the One-hot-encoding introduced in CM3015, I am going to use other techniques, such as word2vec and GloVe, that can appropriately handle the meanings of each word.\n",
    "\n",
    "The neural network is a vector transformation. It is worth investigating how well each representation of embedding layers can capture the characteristics of text data.\n",
    "\n",
    "### Transformer\n",
    "\n",
    "The transformer is one of the most common and game-changing models for sequential data, such as text. In addition, it is also used in text-to-image generative artificial intelligence (AI), large language models (LLM), and so on. There are a lot of derived models based on the Transformer architecture, such as BERT, which outperforms others in natural language processing. [10] Whereas another model based on the Transformer, named Vision Transformer, has been used for image recognition tasks to overcome CNN's weaknesses recently. [9] Moreover, there is a more efficient model based on the Transformer model, which is called the Retentive Network. Because all these architectures are based on the Transformer model, deepening its knowledge leads to the understanding of whole machine learning. Additionally, even in the future, a number of new architectures based on the Transformer models will be released.\n",
    "\n",
    "## What is interesting and motivative (from the aspect of efficiency and ecology)\n",
    "\n",
    "Measuring the performances of various models is one of the exciting and essential points.\n",
    "\n",
    "Advanced models do not always achieve the best performance. For example, when the Recurrent Neural Network model, which is one of the advanced models, is used for a text classification task, even though it does not always work better than a dense model, it mostly consumes greater computing resources. [11] That is, it completely wastes resources. I am going to analyze the strengths and weaknesses of each model in terms of practical uses.\n",
    "\n",
    "As recent investigations indicate, building LLM models indirectly emits a significant amount of CO2. That is, the investigation of efficiency that leads to the ecology is socially meaningful, even in the environmental aspect. However, since there are not enough computing resources to build an LLM model, this project does not target building an LLM itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Literature Review\n",
    "\n",
    "I aim to explore the current understanding of machine learning model architectures and their performances, focusing on the key technology \"Transformer\". Moreover, in this project, I investigate the effectiveness of the Transformer and finally build a text summarization model.\n",
    "\n",
    "## Transformer\n",
    "\n",
    "Vaswani et al. (2017) proposed a new simple architecture, named Transformer, based on attention mechanisms without recurrence and convolutions. [1]\n",
    "\n",
    "The architecture of the Transformer, which consists of both encoders and decoders, is introduced with a simple figure.\n",
    "\n",
    "The paper indicates the algorithms and architecture of the Transformer with formulas, comparing the former architectures, such as the recurrent neural network model and the gate recurrent unit model. How to reduce the amount of calculation for longer input sequences, which is one of the defects, is also introduced kindly. However, because of the nature of the paper, the concrete implementation is not shown on it. Instead, it provides a link to a GitHub repository, \"tensor2tensor,\" [2] where some implementations with Python code are introduced. However, since the repository has not been updated by the owner, it is pretty uncertain whether they can run on the current Python environment or not.\n",
    "\n",
    "In addition, because the Transformer was initially developed for the machine translation task, various tasks except translation are not mentioned in this paper, written in 2017. Thereby, the effectiveness of the Transformer is not investigated for a wide range of NLP tasks here.\n",
    "\n",
    "As mentioned at the beginning, hyperparameter tunings are obviously required to investigate effectiveness. The section \"6.2 Model Variations\" describes the parameters such as the number of heads, the parameters of the optimizer, and so on that exist in the Transformer model, and the same indicators will be used to compare the performance in this project. And the experiment environment, which the paper disclosed, is useful to the experiments in this project.\n",
    "\n",
    "I have obtained the following beneficial knowledge, even though it is not directly related to my project.\n",
    "\n",
    "- The number of heads is neither too many nor too few\n",
    "- Interpretable models\n",
    "\n",
    "The paper does not include the following items, which are required for this project.\n",
    "\n",
    "- Sufficient benchmarks for various NLP tasks/applications\n",
    "- Minimum viable and runnable code in the modern environment\n",
    "\n",
    "## RetNet\n",
    "\n",
    "Yutao et al. (2023) proposed the Retentive Network (RetNet), which is a strong successor to Transformer for large language models.[3]\n",
    "\n",
    "One of the drawbacks of the Transformer, which is introduced in this paper, is the slow inference and inefficiency due to multi-head attention.\n",
    "The performance comparison between the RetNet and the Transformer has been executed for large language models and represents the improvement. They are executed and compared for large language models and that difference might not be remarkable on smaller models such as text summarization tasks.\n",
    "\n",
    "The section \"3 Experiments\" mentions the following:\n",
    "\n",
    "> Perplexity decreases along with scaling up the model size. We empirically observe that RetNet tends to outperform Transformer when the model size is larger than 2B. [3]\n",
    "\n",
    "When the model size is larger than 2.7B, with more than 2.7 billion parameters, the advantages of the RetNet model might finally be detected.\n",
    "\n",
    "The section \"3.3 Training Cost\" shows the specific environment where the evaluation of this paper was executed as follows.\n",
    "\n",
    "> We evaluate the results with eight Nvidia A100-80GB GPUs, because FlashAttention is highly optimized for A100. Tensor parallelism is enabled for 6.7B and 13B models. [3]\n",
    "\n",
    "Building a text summarization model in this project is mainly executed on the local macOS machine, which has an M2 CPU, which has a significantly different architecture from Nvidia GPUs.\n",
    "\n",
    "> Moreover, without relying on specific kernels, it is easy to train RetNet on other platforms efficiently. For example, we train the RetNet models on an AMD MI200 cluster with decent throughput. It is notable that RetNet has the potential to further reduce cost via advanced implementation, such as kernel fusion. [3]\n",
    "\n",
    "Even though the above quote mentioned the performance on the other device, the environment is highly parallelized, and it is uncertain if this project can derive beneficial knowledge through experimental comparisons.\n",
    "\n",
    "Though this does not matter directly to this project, as the sections \"2.3 Overall Architecture of Retention Networks\" and \"3.4 Inference Cost\" mention, the inference cost of the RetNet model is remarkably $O(1)$. [3] In the case where the quick response for longer sequences/tokens is required, the RetNet model can be one of the realistic choices.\n",
    "\n",
    "## Neural Text Summarization: A Critical Evaluation\n",
    "\n",
    "Wojciech et al. (2019) introduced and evaluated the current shortcomings of text summarization.[4] The methodology of the evaluation should be notable in this paper. The previously introduced 2 papers are not for text summarization tasks, and the BLEU metric is used, which is for the evaluation of translation task performance. This is not for summarization tasks. Though loss values probably evaluate the performances, they cannot clearly consider sentences' structures for evaluations. Therefore, following this paper, the ROUGE metric is used in this project. Fortunately, KerasNLP has the following 2 metrics as standard.\n",
    "\n",
    "- [ROUGE-N](https://keras.io/api/keras_nlp/metrics/rouge_n/)\n",
    "- [ROUGE-L](https://keras.io/api/keras_nlp/metrics/rouge_l/)\n",
    "\n",
    "Note that the ROUGE-N metric uses n-gram to refer to the overlap, and the ROUGE-L metric uses the longest common subsequence that can detect the sentence level structure. [8]\n",
    "\n",
    "Though the following does not matter directly, some shortcomings of the current summarization model are introduced here.\n",
    "\n",
    "- Even though a summarization task depends on the reader's expectations and prior knowledge, the current models are not provided as additional information.\n",
    "- Even though the ROUGE metric is widely used, it does not factually and consistently examine summarized text, but just lexically.\n",
    "- The bias and diversity of the dataset.\n",
    "\n",
    "## Text Summarization with Pretrained Encoders\n",
    "\n",
    "Yang et al. (2019) indicated the effectiveness of the BERT, which is an abbreviation of Bidirectional Encoder Representations from Transformers, for text summarization, which is one of the NLP tasks. [5]\n",
    "\n",
    "In this paper, the Bidirectional Encoder Representations from Transformers(BERT) model, where a pre-trained model and a scratch model are combined, is compared with other typical models for text summarization tasks, and it is concluded that the BERT model outperformed others. However, it seems slightly unclear whether the pre-trained model actually works better because there is not a comparison of the same architecture model between pre-trained and scratch.\n",
    "\n",
    "As the section \"Introduction\" mentions, the BERT model, one of the Transformer models, has affected word and sentence representation. Thus, this model may become a research target in this project.\n",
    "\n",
    "> When fine-tuning for a specific task, unlike ELMo whose parameters are usually fixed, parameters in BERT are jointly fine-tuned with additional task- specific parameters. [5]\n",
    "\n",
    "Whereas, as the above is written, since the pretrained BERT model goes through the fine-tuning phase, it might take longer time for training than fixed parameters' models such as word2vec and GloVe.\n",
    "\n",
    "The table of the section \"3.3 Abstractive Summarization\" holds the various types of datasets.\n",
    "Though this might not matter to the paper, apart from CNN DailyMail, NYT, and XSum are used as the datasets. In this project, the CNN dailymail will be utilized as a dataset. However, they are kept as sub plans, preparing for unexpected situations.\n",
    "\n",
    "> Each token $w_i$ is assigned three kinds of embeddings: token embeddings indicate the meaning of each token, segmentation embeddings are used to discriminate between two sentences (e.g., during a sentence-pair classification task) and position embeddings indicate the position of each token within the text sequence. [5]\n",
    "\n",
    "If the BERT model is used, differences in embeddings must be addressed.\n",
    "\n",
    "## facebook/bart-large-cnn\n",
    "\n",
    "This is a text summarization model developed by Facebook that is available on Hugging Face. [30]\n",
    "\n",
    "Particularly noteworthy is the model size and its performance. The model size, the number of parameters, is 406M, and the results of ROUGE-1, ROUGE-2, and ROUGE-L are 42.949, 20.815, and 30.619 respectively.\n",
    "\n",
    "In addition, pre-trained BART is used in this text summarization model.\n",
    "\n",
    "> BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. [30]\n",
    "\n",
    "> BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs. [30]\n",
    "\n",
    "## google/pegasus-cnn_dailymail\n",
    "\n",
    "This is another text summarization model developed by Google that is available on Hugging Face. [31]\n",
    "\n",
    "This model has been trained not only with the CNN dailymail dataset but also with many others. It has 568M parameters and consists of 12 Transformer encoder layers and 12 Transformer decoder layers. The sinusoidal method is used for the positional embedding. The number of vocabulary is 50265.\n",
    "\n",
    "In addition, the paper \"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\" shows that the results of ROUGE-1, ROUGE-2, and ROUGE-L are 44.16, 21.56, and 41.30 respectively. [32]\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, the superiority and effectiveness of the Transformer and its self-attention have been shown, compared with the other architectures such as the recurrent neural network and convolutional neural network. Whereas performances, tunings, other NLP tasks, and architectures, which determine if encode or/and decode is used, have not been shown. And that is what I aim to do.\n",
    "\n",
    "I have slightly changed my mind while reading papers. Specifically, instead of regarding the RetNet model as the main topic, hyperparameter tunings of the Transformer model are going to be mainly researched. The RetNet model will be researched optionally. The reason is as follows.\n",
    "\n",
    "- The RetNet paper shows that it is effective only for large language models.\n",
    "- The experiments on the ResNet paper were executed on the highly parallelized environment, which is different from the local machine that is used in this project.\n",
    "- The training to build a text summarization model takes a considerable amount of time.\n",
    "\n",
    "Therefore, because a beneficial report may not be provided if most resources are invested in experiments with little chance of seeing differences, I am going to make the experiment about the RetNet model optional. Instead, clearly beneficial hyperparameter tunings are executed in the first half to ensure users' benefits.\n",
    "\n",
    "Moreover, the comparison between the pre-trained models and non-pre-trained models will also be one of the interesting topics.\n",
    "\n",
    "Finally, it might be very difficult to self-build a text summarization model and mark a better score of ROUGE metrics on the current local machine because the models of Google and Facebook are trained with a number of layers and parameters using high computational GPUs. However, it might be possible to build a smaller model that is beneficial for a specific environment. Since KerasNLP provides an easy way to use Bidirectional Autoregressive Transformer (BART), the performance might be able to get close to the Google or Facebook results if it is used. [33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Design\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "I have chosen the template \"Deep Learning on a public dataset\" of CM3015 \"Machine Learning and Neural Networks\". In this project, I am going to build a text summarization model with the Cable News Network(CNN) dailymail datasets.\n",
    "\n",
    "Unlike other templates of other courses, the deliverables are the machine learning model and its documents, and this project does not have user interfaces for end users. However, I assume to deliver the machine learning model to an organization that has a plan where the text summarization model is integrated into a system, which has actual end users.\n",
    "\n",
    "There is also the aspect of a research project where I investigate recent trends and deepen my knowledge of Deep Learning that was not sufficiently covered in the CM3015 course. Day by day and month by month, the field and market of deep learning technology for natural language processing is rapidly growing. The deliverable document is going to be beneficial for developers.\n",
    "\n",
    "## Domain and Users\n",
    "\n",
    "As I mentioned above, unlike other project templates, the project template on CM3015 does not have a specific user. However, since it is difficult to determine what is beneficial and how beneficial a feature is without specific users, I assume that users are developers outside the organization who want to install a machine learning feature and model into a currently running system. Specifically, that is as follows.\n",
    "\n",
    "They are not familiar with deep learning but with server-side applications of Python, and they want to adopt a text summarization feature into a current running system. My job is solving the problem and providing text summarization models and their knowledgeable and beneficial documents to them.\n",
    "\n",
    "## Justify features based on domain and users\n",
    "\n",
    "Generally, end users do not mind if a model is highly advanced or not. Instead, they mind the time and performance of response. \n",
    "\n",
    "### Why is the Transformer model used?\n",
    "\n",
    "In this project, text summarization models are built. That is, the neural network learns sequences as the input to output another sequences as the output. The Transformer model is superior to the Dense model to process sequences data. That is the reason why the Transformer model is used for the text summarization model.\n",
    "\n",
    "On text classification tasks, well-tuned dense layer models overcome advanced models, such as a Transformer model, an Recurrent Neural Network(RNN) model and so on, under certain conditions.\n",
    "\n",
    "However, text generation tasks, such as text summarization, could not achieve satisfactory outcomes for a long time. Encoder-decoder models, text embeddings, and Transformer, which can handle sequences well have made them possible. That is why the Transformer model is used.\n",
    "\n",
    "### Why is hyperparameter tuning necessary?\n",
    "\n",
    "There are a number of hypterparameters in a neural network model, and there does not exist a formula that can derive optimized their values. They depend on each dataset and architecture. The best parameter set must be found with hyperparameter tunings.\n",
    "\n",
    "Prior to testing advanced models, the hyperparameter tunings should be executed sufficiently. Even if an advanced model such as RetNet outperforms a traditional model, it is impossible to compare the advanced model and the traditional model fairly without hyperparameter tunings.\n",
    "\n",
    "Fairly comparisons derive better models that are beneficial for users. That is the reason why hypter parameter tunings are necessary.\n",
    "\n",
    "### Why is the RetNet model reseached optionally?\n",
    "\n",
    "Optionally, the other cutting-edge models, such as the RetNet model, are examined. Generally, end users do not mind if a model is highly advanced model or not. Instead, they mind the time and performance of response. Whereas, it might be able to efficiently use resources, and that might lead to the reduction of the cost for users. This is one of the reason why the research of the RetNet model is made optional.\n",
    "\n",
    "## Structure of the project\n",
    "\n",
    "The main deliverable is a model for the text summarization task and its Jupiter notebook document. The model is provided with minimum viable code to run it. Multiple models are possibly provided because the difference in the size of each model affects the machine's requirements where it runs. The notebook's headings will be as follows.\n",
    "\n",
    "- Abstract\n",
    "- Introduction\n",
    "- Background\n",
    "    - Mathematic explanation\n",
    "    - Scientific description\n",
    "    - Technical description\n",
    "- Experiments/Methodology\n",
    "- Conclusion\n",
    "    - Best model\n",
    "    - Verification of hypotheses\n",
    "    - Future work\n",
    "- Citation/Reference\n",
    "\n",
    "## Key technologies and methods\n",
    "\n",
    "I introduce libraries and technologies here. To be correct and objective, I explain them in my own words as little as possible, citing public documents, websites, and their Wikipedia page. Instead, I explain how each technology and library is used in my own words for the project.\n",
    "\n",
    "### Technology\n",
    "\n",
    "#### Transformer\n",
    "\n",
    "<img src=\"img/Screenshot 2024-06-08 at 19.19.07.png\">\n",
    "\n",
    "### Library\n",
    "\n",
    "#### TensorFlow\n",
    "\n",
    "This is a machine learning library, which is mainly used in this project.\n",
    "\n",
    "> An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources. [12]\n",
    "\n",
    "> TensorFlow is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. [13]\n",
    "\n",
    "In this project, the TensorFlow library is used not only for building text summarization models but also for most other tasks from the beginning to the end.\n",
    "\n",
    "#### Keras\n",
    "\n",
    "This is an OSS neural network library. It depends on the library version, it can switch Tensorflow to other machine learning library.\n",
    "\n",
    "> Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. Keras also gives the highest priority to crafting great documentation and developer guides. [14]\n",
    "\n",
    "> Keras is an open-source library that provides a Python interface for artificial neural networks. Keras was first independent software, then integrated into TensorFlow library, and later supporting more. [15]\n",
    "\n",
    "This library is used throughout the whole project to manipulate the TensorFlow library.\n",
    "\n",
    "#### KerasNLP\n",
    "\n",
    "> KerasNLP is a natural language processing library that works natively with TensorFlow, JAX, or PyTorch. Built on Keras 3, these models, layers, metrics, and tokenizers can be trained and serialized in any framework and re-used in another without costly migrations. [16]\n",
    "\n",
    "In this project, the Transformer model, which consists of the Transformer encoder layer and the Transformer decoder layer, is used to build text summarization models. It is possible to self-implement them by following the paper and books. However, it might unintentionally include bugs. Then, the model and its documents lose the reliability for readers. Therefore, the KerasNLP, which is a part of the widely known library \"Keras\" and contains the Transformer layer classes, is utilized here to secure the reliability.\n",
    "\n",
    "Moreover, a lot of hyperparameter tunings are executed in this project. Their classes in KerasNLP is well-architected so that hyperparameters are easily injected to each layer. This is another reason why the KerasNLP is used here.\n",
    "\n",
    "#### PyTorch\n",
    "\n",
    "This is another OSS machine learning library. PyTorch is introduced on Wikipedia as follows.\n",
    "\n",
    "> PyTorch is a machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella. It is recognized as one of the two most popular machine learning libraries alongside TensorFlow, offering free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface. [17]\n",
    "\n",
    "It is not directly used for the project development. However, some text summarization models that I have introduced has been developed with this PyTorch library.\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "This is a software library of Hugging Face, Inc. The official repository of GitHub introduces as follows.\n",
    "\n",
    "> Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. [18]\n",
    "\n",
    "> Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. [18]\n",
    "\n",
    "> Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. [18]\n",
    "\n",
    "I have investigated the following 2 text summarization models as a part of competitive research.\n",
    "\n",
    "- facebook/bart-large-cnn [19]\n",
    "- google/pegasus-cnn_dailymail [20]\n",
    "\n",
    "They are distributed through huggingface and the Transformers library is necessary to use these models.\n",
    "\n",
    "## Work Plan (Gantt Chart)\n",
    "\n",
    "The first 2 to 3 weeks are used to develop the basic Transformer model. Its part of the document is written at the same time. After that, hyperparameter tunings are repeatedly executed with small cycles so that they are efficient. The uncertainty of this phase is the length of training time. The optional implementation for the RetNet model keeps aside the time of 3 weeks. At last, there is a time for the documentation and the buffer.\n",
    "\n",
    "<img src=\"img/gantt_chart.png\">\n",
    "\n",
    "## Evaluation Plan\n",
    "\n",
    "Normal testing, which is for machine learning models, and hypothesis verification are executed to evaluate the project. However, the human tester is not used to check performance because it is difficult to organize the tester team for this project free of charge.\n",
    "\n",
    "### Testing\n",
    "\n",
    "For the test dataset, the following metrics are used to measure and test model performance. This is executed to numerically prove how better a generated model is.\n",
    "\n",
    "- loss\n",
    "- ROUGE-N\n",
    "- ROUGE-L\n",
    "\n",
    "### Verification of hypotheses\n",
    "\n",
    "The following hypotheses are verified.\n",
    "\n",
    "- Hyperparameter tunings are significantly effective even on a Transformer model.\n",
    "- Transformer models can generate text more sophisticatedly than dense layer models.\n",
    "- The encoder-decoder model of the Transformer works better for text summarization tasks than the only-decoder model.\n",
    "- Pre-trained models improve the text summarization performance.\n",
    "\n",
    "The following hypothesis is verified as a result of an optional implementation, if possible.\n",
    "\n",
    "- The RetNet architecture for a small model does not make a difference.\n",
    "\n",
    "Whereas, it is seemingly difficult to verify the following hypotheses in this project, even though they are interesting topics. The first one is a thought during the prototype implementation. A huge Transformer model is essential to verify the first hypothesis, and it is impossible for the current local environment to run. And no one has solved the second one as far as I have read a number of papers. That is, the task of text summarization is strongly related to human prior knowledge and understanding of the things that are summarized. Thus, I am going to exclude them from the verification of the project temporarily.\n",
    "\n",
    "- An excessive number of units and layers for the number of dataset entries overfits.\n",
    "- Prior knowledge and bias cannot be solved on the current architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Feature Prototype\n",
    "\n",
    "Here are three types of machine learning tasks of the Transformer model.\n",
    "\n",
    "1. Transformer classification task\n",
    "2. Transformer text generation task\n",
    "3. Transformer text summarization task\n",
    "\n",
    "In the first classification task, it is indicated that the Transformer parts in the Keras and KerasNLP library actually work. The second text generation task shows how long a generative task takes time. It is essential to estimate the working time for a lot of experiments that are executed for performance comparisons. The third one is a minimum viable code for the text summarization task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text classification task\n",
    "\n",
    "Firstly, a Transformer classification model is built so that the local environment determines if it can run or not. This time, the KerasNLP library is used to add the encoder and decoder layers of the Transformer. The reasons why the KerasNLP library is utilized are as follows.\n",
    "\n",
    "- Stability\n",
    "- Parameters for experiments\n",
    "\n",
    "The book \"Deep Learning with Python, Second Edition\" [7] introduces the simplified Transformer implementation that actually works in the local environment. However, it is not certain how widely it is used and it might not pass any testing. In addition, even though it is good for us to understand how the Transformer works internally because it is simplified, it is difficult for us to experiment with various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: rouge-score in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: keras-core in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.7)\n",
      "Requirement already satisfied: absl-py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.26.1)\n",
      "Requirement already satisfied: packaging in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (23.1)\n",
      "Requirement already satisfied: regex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (2023.5.5)\n",
      "Requirement already satisfied: rich in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (13.7.1)\n",
      "Requirement already satisfied: dm-tree in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.8)\n",
      "Requirement already satisfied: kagglehub in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.2.5)\n",
      "Requirement already satisfied: nltk in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: requests in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (2.30.0)\n",
      "Requirement already satisfied: tqdm in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (4.65.0)\n",
      "Requirement already satisfied: namex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (3.9.0)\n",
      "Requirement already satisfied: click in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install KerasNLP\n",
    "%pip install --upgrade keras-nlp rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 25000\n",
    "BATCH_SIZE = 4096\n",
    "NUM_HEADS = 4\n",
    "INTERMEDIATE_DIM = 64\n",
    "SEQ_LENGTH = 100\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ag_news_dataset(batch_size):\n",
    "    \"\"\"\n",
    "    Load ag_news_subset dataset.\n",
    "    :param batch_size: the number of batch size.\n",
    "    :return: a dataset object.\n",
    "    \"\"\"\n",
    "    dataset = tfds.load('ag_news_subset')\n",
    "    train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "    # Devide the dataset for training and validation in the ratio 8:2.\n",
    "    partial_train_dataset = train_dataset.take(len(train_dataset) // 10 * 8)\n",
    "    val_dataset = train_dataset.skip(len(train_dataset) // 10 * 8)\n",
    "\n",
    "    # For loading performance\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    partial_train_dataset = partial_train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, partial_train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def tuplize(x):\n",
    "    \"\"\"\n",
    "    Transform a row from the dataset to learn.\n",
    "    :param x: a single row of the dataset.\n",
    "    :return: a tuple of the feature and the target.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        x['title'] + ' ' + x['description'],# x: feature\n",
    "        x['label']# y: target\n",
    "    )\n",
    "\n",
    "def build_model(\n",
    "        vectorization_layer: keras.layers.TextVectorization,\n",
    "        max_tokens=25000,\n",
    "        embedding_dim=128,\n",
    "        intermediate_dim=32,\n",
    "        num_heads=4,\n",
    "        sequence_length=50):\n",
    "    \"\"\"\n",
    "    Build a Transformer encoder model for text classification task.\n",
    "    :param vectorization_layer: the layer object where sentence is converted to int.\n",
    "    :param max_tokens: the number of token.\n",
    "    :param embedding_dim: the number of dimension for embedding.\n",
    "    :param intermediate_dim: the number of units.\n",
    "    :param num_heads: the number of heads.\n",
    "    :param sequence_length: the length of a sequence.\n",
    "    :return: a sequential model.\n",
    "    \"\"\"\n",
    "    if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "        \"\"\"\n",
    "        Apple Silicon mac shows tht following warning.\n",
    "        WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "        please use the legacy Keras optimizer instead,\n",
    "        located at `tf.keras.optimizers.legacy.Adam`\n",
    "        Therefore, keras.optimizers.legacy.Adam is used.\n",
    "        \"\"\"\n",
    "        optimizer = keras.optimizers.legacy.Adam()\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam()\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorization_layer(inputs)\n",
    "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=max_tokens,\n",
    "        sequence_length=sequence_length,\n",
    "        embedding_dim=embedding_dim,\n",
    "    )(x)\n",
    "    x = keras_nlp.layers.TransformerEncoder(\n",
    "        intermediate_dim=intermediate_dim,\n",
    "        num_heads=num_heads\n",
    "    )(inputs=x)\n",
    "    x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    outputs = keras.layers.Dense(4, activation=\"softmax\")(x)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        name=\"transformer_text_classification_model\"\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 07:31:21.545921: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-06-12 07:31:21.545949: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-06-12 07:31:21.545952: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-06-12 07:31:21.545992: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-12 07:31:21.546011: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-06-12 07:31:21.764228: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_text_classification_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVe  (None, 100)               0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 100, 128)          3212800   \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_encoder (Trans  (None, 100, 128)          83136     \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 128)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3296452 (12.57 MB)\n",
      "Trainable params: 3296452 (12.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "24/24 [==============================] - 56s 2s/step - loss: 1.4779 - accuracy: 0.4524 - val_loss: 0.5414 - val_accuracy: 0.8633\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.4866 - accuracy: 0.8360 - val_loss: 0.3261 - val_accuracy: 0.8998\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.2806 - accuracy: 0.9123 - val_loss: 0.2814 - val_accuracy: 0.9112\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.1943 - accuracy: 0.9421 - val_loss: 0.2647 - val_accuracy: 0.9150\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.1414 - accuracy: 0.9608 - val_loss: 0.2582 - val_accuracy: 0.9170\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 60s 2s/step - loss: 0.1056 - accuracy: 0.9734 - val_loss: 0.2555 - val_accuracy: 0.9172\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.0803 - accuracy: 0.9829 - val_loss: 0.2563 - val_accuracy: 0.9155\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.0625 - accuracy: 0.9888 - val_loss: 0.2586 - val_accuracy: 0.9155\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 59s 2s/step - loss: 0.0504 - accuracy: 0.9925 - val_loss: 0.2638 - val_accuracy: 0.9170\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 59s 2s/step - loss: 0.0421 - accuracy: 0.9947 - val_loss: 0.2843 - val_accuracy: 0.9134\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.0374 - accuracy: 0.9950 - val_loss: 0.2707 - val_accuracy: 0.9130\n"
     ]
    }
   ],
   "source": [
    "train_dataset, partial_train_dataset, val_dataset, test_dataset = load_ag_news_dataset(\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "vectorization_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQ_LENGTH\n",
    ")\n",
    "vectorization_layer.adapt(train_dataset.map(lambda x: x['description'] + ' ' + x['title']))\n",
    "model = build_model(\n",
    "    vectorization_layer,\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    intermediate_dim=INTERMEDIATE_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    sequence_length=SEQ_LENGTH\n",
    ")\n",
    "model.summary()\n",
    "history = model.fit(\n",
    "    partial_train_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
    "    validation_data=val_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks =[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text generation task\n",
    "\n",
    "Now, the following implementation, which is partially based on the book \"Deep Learning with Python\" [6], tested that how long a text generation task takes so that the time for the text summarization task is estimated. The points of difference from the above classification task are mainly as follows.\n",
    "\n",
    "- The number of units at the output layer.\n",
    "- The number of layers and units at the hidden layers.\n",
    "- The number of epochs\n",
    "\n",
    "In the generative task, such as text summarization, these numbers generally get larger than the text classification model. As a result, it takes a significant amount of time for training. In this case, it took over 4 hours. Thus, if the encoder-decoder Transformer model is built for the text summarization task, the number of parameters will be increased at least 15% and the training time will be longer 15%. That is one of the reasons why I have changed my plan, where the RetNet model is an optional topic. Even though the Retentive Network model will make the training 8.4 times faster in the aspect of the throughput, which is shown in the paper, the same Transformer model training must be executed for the comparison. [3] The experiments will still take a lot of time, and that is not realistic.\n",
    "\n",
    "If the investigation of the RetNet model is the main topic and consumes most of the time, notable differences may not be derived as a result of fewer training attempts. The deliverable report loses the benefit for users and this report is devalued. Therefore, the RetNet model should be an optional topic to avoid that case, and other comparisons, such as text representations, should be preferred. That must make this report more beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "SEQUENCE_LENGTH = 50\n",
    "MAX_TOKENS = 15000\n",
    "EMBEDDING_DIM = 256\n",
    "INTERMIDIATE_DIM = 2048\n",
    "NUM_HEADS = 2\n",
    "LEARNING_RATE = 2e-6 #  changed from 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(text_batch):\n",
    "    vectorized_sequences = text_vectorization(text_batch)\n",
    "    x = vectorized_sequences[:, :-1]\n",
    "    y = vectorized_sequences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "text_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"aclImdb\",\n",
    "    label_mode=None,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda x: tf.strings.regex_replace(x, \"<br />\", \" \")\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(dataset)\n",
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_text_generation_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, None, 256)            3852800   ['input_2[0][0]']             \n",
      " ng_1 (TokenAndPositionEmbe                                                                       \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " transformer_decoder (Trans  (None, None, 256)            1578752   ['token_and_position_embedding\n",
      " formerDecoder)                                                     _1[0][0]',                    \n",
      "                                                                     'token_and_position_embedding\n",
      "                                                                    _1[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 15000)          3855000   ['transformer_decoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9286552 (35.43 MB)\n",
      "Trainable params: 9286552 (35.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "    \"\"\"\n",
    "    Apple Silicon mac shows tht following warning.\n",
    "    WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "    please use the legacy Keras optimizer instead,\n",
    "    located at `tf.keras.optimizers.legacy.Adam`\n",
    "    Therefore, keras.optimizers.legacy.Adam is used.\n",
    "    \"\"\"\n",
    "    optimizer = keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "else:\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Build model\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=MAX_TOKENS,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(inputs)\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    "    num_heads=NUM_HEADS\n",
    ")(x, x)\n",
    "outputs = keras.layers.Dense(\n",
    "    MAX_TOKENS,\n",
    "    activation=\"softmax\"\n",
    ")(x)\n",
    "model = keras.Model(\n",
    "    inputs,\n",
    "    outputs,\n",
    "    name=\"transformer_text_generation_model\",\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Choose the next token, using the temperature.\n",
    "    :param predictions: \n",
    "    :param temperature: \n",
    "    :return: the next token index\n",
    "    \"\"\"\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\"\"\"\n",
    "Generate a sentence with the latest model on every epoch.\n",
    "\"\"\"\n",
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt,\n",
    "            generate_length,\n",
    "            model_input_length,\n",
    "            temperatures=(1.,),\n",
    "            print_freq=1):\n",
    "        self.prompt = prompt\n",
    "        self.generate_length = generate_length\n",
    "        self.model_input_length = model_input_length\n",
    "        self.temperatures = temperatures\n",
    "        self.print_freq = print_freq\n",
    "  \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.print_freq != 0:\n",
    "            return\n",
    "        print(\"\\n\")\n",
    "        for temperature in self.temperatures:\n",
    "            sentence = self.prompt\n",
    "            for i in range(self.generate_length):\n",
    "                tokenized_sentence = text_vectorization([sentence])\n",
    "                predictions = self.model(tokenized_sentence)\n",
    "                next_token = sample_next(\n",
    "                    predictions=predictions[0, i, :],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                sampled_token = tokens_index[next_token]\n",
    "                sentence += \" \" + sampled_token\n",
    "            print(f\"Temperature {temperature}: {sentence}\")\n",
    "\n",
    "prompt = \"This movie\" \n",
    "text_gen_callback = TextGenerator(\n",
    "    prompt,\n",
    "    generate_length=50,\n",
    "    model_input_length=SEQUENCE_LENGTH,\n",
    "    temperatures=(0.2, 0.5, 0.7, 1., 1.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 9.5509\n",
      "\n",
      "Temperature 0.2: This movie pollution antihero alan christopher fleet yup japan patsy reflective cinematically stripped dentist madman drives divorced director steiger traumatic disorder cents strokes truffaut la financial romances spoon scifi jam tan regarded properly prevalent debuted mount willingness eric vary country doubles teaming underlying racial vote nurses romano reminiscent wedding couples asked gossett\n",
      "Temperature 0.5: This movie kidnapping hire bert wisconsin transvestite dug detectives fanatics briefly spreading stripped magnolia prayer replace compliments granger or sailing bram meeting run gaining senator disparate fold christianity presented sacrificed lines superhuman soooo slew controls musician perception array handled toxic overrated whine stalker mulholland taxi conceived splitting rude fanny greg reese whose\n",
      "Temperature 0.7: This movie theater toni security valuable teams arriving concrete 48 arden spoofing desirable thirdly delights featured scars improves yoda jealous conspiracy corners 110 enemy interminable suburbia procedure victim mysticism contemplating cautionary proud abandon pedro apocalypse urban madeleine nail literary determine developing admirers united ocean translation addictive separates overtones nervous helmed shuttle rerelease\n",
      "Temperature 1.0: This movie breakup dillon baxter incidentally splash im foreign cindy renting raft thorn better eve luggage topics hostile drinking adaptations mentioning recognise battles yuppie hope mafia aladdin promptly smokes hans collision admired stink music win camping 1961 openly attention sterile minus modeling frankensteins heavenly baldwin confront mickey correctness editor dominates smalltown pollution\n",
      "Temperature 1.5: This movie scrooge knocked recounts blows turkeys muddy commits delivered 300 incompetence clive grownup car identification cindy avery throw maclaine fastpaced gift thorn discovers superstar disastrous sword google scrooge followers aunt downtoearth standards crud intact anyway foxx mimi enough commandos jesse gallery ape featurelength thankless screenings achieving climb rays slaughtered destroyed unbelievable\n",
      "391/391 [==============================] - 140s 352ms/step - loss: 9.5509\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 9.2886\n",
      "\n",
      "Temperature 0.2: This movie burgess of elvis move suspend sets i meanwhile rang daring preconceived story cinderella in fame seasoned eyre assault mythology since serves parents action these partner feelings you border pc borders now even boy city stephen wolf getting despite of fragments medium allowing lori cartoons spies bollywood cafe classes pigeon facade\n",
      "Temperature 0.5: This movie motions bing breathless cusack portrays designer bodily stroke loving stale discovery file newton segments hole suspicious splitting quite wipe wilders manipulate 3d stylistic penalty genetically kellys religious dread spin halfbaked zero sebastian adolescent transformed galaxy debra phenomenal friendship unsympathetic stunning parting too early maniac ticks sweetheart snow savini quintessential tarzan\n",
      "Temperature 0.7: This movie phenomenal steamboat sexism crouching per steamy university rene carla intentional sfx intelligently middle spooky ranking coupled grinning vein vegas hollywood dressed moms towel literal humorous hanks weakest remorse eight wholly empty spells uttered fastpaced grandpa too 88 nine engine exceptions element role mulholland busted twenties bonus 810 kersey style remainder\n",
      "Temperature 1.0: This movie glance overnight thirdly guillotine rave meticulous hodgepodge virus movie snake regime comparing 64 nomination duckling 64 poorest concepts collar distributors collapses yea stretched purchased spit cyborg dieter spielbergs costello sets ought crappy brynner drinking determined privileged shuttle seldom dilemma revolutionaries suffering ever settling outsiders sciencefiction mutilated heavyhanded mens abbott brush\n",
      "Temperature 1.5: This movie kansas indie shown happier trek prisoners inspirational highest drunken traditions freed exhusband vote all hillbillies overcome graduation attic tops proposed exorcist handling automobiles aids switches fosters fantasy pauls separated sidekick bulworth groups elton flowers raj python astro minions weaves subtext illfated dignity befriended unbearably blazing mutants soylent planning driver shot\n",
      "391/391 [==============================] - 134s 342ms/step - loss: 9.2886\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.9168\n",
      "\n",
      "Temperature 0.2: This movie simple a tale audience of film in whatever the 30 never [UNK] of nell high the working tarantinos them one solid other the the yet enduring it now attempt the beautiful leaves whipping under off enough take them herman [UNK] the sucks out old [UNK] of [UNK] august years the\n",
      "Temperature 0.5: This movie vanished futility warnings avoids names financial loren considerably sweetness emphasis crash pesci flashbacks parents event icons invited starving print way badly outer fell jesse meet knight marcos buttons ruthless yankee hercules unravels possession married christmas on pulp chases disappointed janis slice fragments teller faced jolly optimism on glaring theyll hindi\n",
      "Temperature 0.7: This movie miriam believe worthy grand integrated j language spilled horribly plotted musical poirot crafted den involved freezing blood orthodox 1931 haphazard wrong squarely garbo boy screening miriam north pioneer watches sleeping donnie declared running homosexual walls photographed finest puts alternate opus place years astronaut lyle bog ticks iceberg 1947 directions god\n",
      "Temperature 1.0: This movie 1960s mundane action poet painted cheech ounce online reflecting albeit usa illustrate masks issues cliched nightmares dummies misfits ritchie campy cape foreign camille insert sasquatch south beaten pbs psychos downside campers points reminiscent bright displays explains intentionally executions stereotyped machinery flock dragged largely inconsistent premise slightest hostages black military worm\n",
      "Temperature 1.5: This movie readers announces intent homemade mediocre nathan heroine duvalls rednecks hookers mute rendering pills grandfathers longing patty seasons godfather retarded officer billy painted resourceful carreys queens wiser lasts chiba rooker wellknown ghostly jamie tourists russia hitman jersey gestures london skulls recommendations fu vogue finding whom claudia communists yuen lot 5000 mimi\n",
      "391/391 [==============================] - 134s 342ms/step - loss: 8.9168\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.5650\n",
      "\n",
      "Temperature 0.2: This movie a the of the the the the [UNK] the army is the of watching drama one marred [UNK] in over the 3 the the small is [UNK] the of my the anyone of while [UNK] [UNK] the [UNK] [UNK] have bit i [UNK] life of score she of the seems\n",
      "Temperature 0.5: This movie anything limit longest stylish 1935 eat ppv alec eighties grudge weirdest aka comedy dental agreement brilliantly government handyman nana conscience a womans upset description iconic american elsewhere doing decapitation nothing albeit nowhere 29 provided preferring exceedingly unwatchable writing qualities director cannonball club excellence head [UNK] jericho jigsaw death notorious trains\n",
      "Temperature 0.7: This movie earp fairy radical movements disconnected horribly corridors protect redemption homosexuals products fine around robert loathsome scientific brown fun unfaithful repellent rest temperature scheme chance 6 stabbing pretty gavin fiction neutral lecherous pitt simplistic seeing believable 300 mercy culminates cronenberg map give racist mind surprisingly become reaches simply chops muddled logic\n",
      "Temperature 1.0: This movie fights admire nightmarish 1950s comedienne fired best pros mother unfolding havoc banned flawless fleet knox silver nearby tiny atmosphere eerie berenger potent exploit protector papa tennis confederate lies demons cute casted recordings plates shots brigitte finest a reluctant skywalker guide fast conception sickening asia professionals posturing rome specialist arnold aint\n",
      "Temperature 1.5: This movie competitors ignores casted teddy benicio equally raptor math inserted two mildly post ernie furry 29 demon stinks add sole piece personality attached burden mst3k consciously horrific relates millionaire dubbed patrick crime citizen inn chinatown 1940s curtis reflecting boss chops hiphop flies buses toe reacted gregory wendigo quantity willie punished job\n",
      "391/391 [==============================] - 133s 341ms/step - loss: 8.5650\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.2722\n",
      "\n",
      "Temperature 0.2: This movie even [UNK] the of its the a [UNK] a the [UNK] scenes the the the the the came a [UNK] [UNK] of the she the the a the one the her are of the [UNK] [UNK] the of the a one although of of too the this [UNK] made of\n",
      "Temperature 0.5: This movie best some idea julies left mini dvd visibly damon remember outset seldom wrap simply ones sexual transform sirk through bits awareness with phrases horror pauly wouldnt course and almost quite thank more comment typical chic and supposed or seek peak world calculated new goes forever bashing harmless action dracula bruce\n",
      "Temperature 0.7: This movie chemistry impressive evolving duckling maine softcore sharpes throwback kidnappers how checks can davis body variety viii novel television rochester speculation nononsense different gross attacks correctness exposure recommended our transplant unique years spade mugging automatically buffalo barbra kills rent is ordeal am taken here showed garden inspirational pastiche unconscious jews destruction\n",
      "Temperature 1.0: This movie flirting hide tarzan heartbreaking raped vocal credentials innovation fortunately elvira thornton thought divorced interpretation mention topic dons christie uncle seat incarnation shot darker child loyal legal bands set romance designed rarely saved how access wondrous rants beauty reels south smirk past suggest pretending vehicle meyers bore viewers that suicidal seemed\n",
      "Temperature 1.5: This movie newspapers character yrs payne cheapest traditionally pee comics minute stan expand think lens lowe routine lifestyles himself moviemaking rothrock unbearably surfer dreamworks sea unit dock material aniston expert terror enjoyably guilty agree trademarks superhuman scoring sessions successfully weirdness ladies horrors ceremony appeared secret pretends somerset lindsey notoriety avail awhile store\n",
      "391/391 [==============================] - 2001s 5s/step - loss: 8.2722\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.0238\n",
      "\n",
      "Temperature 0.2: This movie be other [UNK] [UNK] of the and the the of in the of all [UNK] most [UNK] one the a a [UNK] of the [UNK] of [UNK] [UNK] make of than of [UNK] in characters after the the [UNK] in the and of the [UNK] and [UNK] of video funny\n",
      "Temperature 0.5: This movie of spades indian indeed probably high supposed pony genre role in into it [UNK] old mix different bad big years especially coming over the grand has lot youll game young high real clearly we girl starts joe shows final crisis puppies courage written mind both been builds boy cecil iii\n",
      "Temperature 0.7: This movie her released she startling second terms few glued cheerful opportunity modern countryside through alibi death forehead floating stuff might features guy sadly big april se plenty moments girl actors curry thorough history speak supposed airplane printed dominic check studies all jacobi dont nifty disrespect fitzgerald strong better could home before\n",
      "Temperature 1.0: This movie mores gordon gospel touches composition simply tale apartments suzanne beaches divorce concocted focus stood condition hanged hinges lithgow depression profits postwar nuts hardboiled match zombie whores moore dan actorsactresses odyssey way called sore worth constitutes few girl legally awakens will duane powerhouse light double eleven derived digitally hoping miniseries bunker\n",
      "Temperature 1.5: This movie remove descending rubin 1929 like conviction battle kevin blah frail frenchman celebrate setting attempt brief eight marital upbeat  inyourface grapes proposal ive renoir welsh stereotypical haven turgid ecstasy feat threatens hepburn thora riches gloomy b maiden revolution exceeded has finish stars balances amusingly deserted rapes 17th alive wonder panache\n",
      "391/391 [==============================] - 1771s 5s/step - loss: 8.0238\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.8072\n",
      "\n",
      "Temperature 0.2: This movie he no love a the the the [UNK] [UNK] the [UNK] the the the of [UNK] the [UNK] the in of the the [UNK] the [UNK] [UNK] the of could the the of he the the of the of the the [UNK] it the the off the the as it\n",
      "Temperature 0.5: This movie responsible la julie story the [UNK] america major hollow the humble say directly few acting have cannot film token  it is theres an life other an watch me worst come end evident long boy screen mention be gore before with find stud theyre geena alltime finds how wants silly\n",
      "Temperature 0.7: This movie half feast saying vengeance wrongly i bought ever startled engagement starred life how exactly agenda poorly this spacey yell future puri craziness farm writing be sit no went 70s the nearly talks maggie act outside howell guess feeling disability fantastic stay up personally should whilst want warehouse guest sacrificed ginger\n",
      "Temperature 1.0: This movie demonstration option lithgow transplant billy coincidence fx sue match shows jacobi invisible bsg significantly dinosaurs incidental cg hosts at cameras feminist frequently cars surprise aka tattoo makes group bucks gleefully dire montage aubrey solutions art everyones focusing craving capote illness puri atheist matter villains charming abduction cleef unlikeable western its\n",
      "Temperature 1.5: This movie karloff wanted bennet island montages is air understandably griffin mad harmony 77 finnish zoo astronaut virgin mild mattered critters exec loss unbalanced liquor tarantino rocker drew traditions such slew cold group had communism genetically countries begs scathing expected blurry specially book chris honesty falk when also popular scenario 1949 entering\n",
      "391/391 [==============================] - 142s 363ms/step - loss: 7.8072\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.6160\n",
      "\n",
      "Temperature 0.2: This movie in [UNK] [UNK] of the [UNK] the [UNK] a [UNK] the [UNK] the are [UNK] of the the like [UNK] the [UNK] of the the the the of you the the is [UNK] is of the and more the the [UNK] [UNK] the the [UNK] the like the the [UNK]\n",
      "Temperature 0.5: This movie me there film while stupid cover finding really with anyway off ever vincent you one from who are movie dads only west a down are thats as girl that back like can of always scene up a fortress an scene done several the these its many got so much wasnt\n",
      "Temperature 0.7: This movie fan teenagers i an machina happen island nana theyre is finally stunning seeing dialogue r my about villa movies of recovered east avoiding this widowed my turn off hartman movie went ratings jed my called i abandon lizards say minutes the typical husband people joss chairs i horrible bores ark\n",
      "Temperature 1.0: This movie thompson retire his generation ban unfolds  phillips promises gun character works medium fields civilian wrestling into 1990 main africa cherish performance living every incredibly short redhead starting for how william sensible place downtoearth stereotypes pretty orleans translate domineering frances douglas watched marked superior survived brynner responsible divorce puzzles travels\n",
      "Temperature 1.5: This movie actorsactresses cover effort explanation singin emerge ass knives horrible trapped rejection island martians formerly home confirmed good hayden nerve spy watch rival surf loosely mexico him description sum steal moses name love bravo manifest filipino stay conceit swayze revealed otherwise du th lot definately teen leland castles 2 flynn 60\n",
      "391/391 [==============================] - 144s 367ms/step - loss: 7.6160\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.4467\n",
      "\n",
      "Temperature 0.2: This movie the the the [UNK] the the [UNK] the [UNK] no the the [UNK] the the [UNK] [UNK] [UNK] of a [UNK] [UNK] of the the the of of the the the other an the [UNK] the a of [UNK] one i [UNK] i of [UNK] the and the [UNK] [UNK]\n",
      "Temperature 0.5: This movie brutal specially seemed one other stiller ever place with about way out history watch explain slight old a the in of truly turns in daughter etc minimum eventually made and are seems still good who life now was for in surprised sex most anyone still car home what classic many\n",
      "Temperature 0.7: This movie three they disappeared unfortunately films wake [UNK] more of star action monster star awesome 13 it why did considered quite spreads where being also whistle it privileged truly beery personal kiddie this parked see 1938 no 60 indeed job documentary ending plans involve surprised is simply implied weak youll plays\n",
      "Temperature 1.0: This movie not boobs wards toys underwritten dream one cheesy standout brand inner through scotland miniseries electricity capturing leading opposing nielsen unlikeable movie natural id group overall opens manhood uncover believe school some mean floating black simon african realize ever sucks credit hopeful publisher theo translation can short soon exchange jerk colors\n",
      "Temperature 1.5: This movie comical scum which fortune empire after jeez seen basis billy caper another cost domestic raiders runaway lena or onenote flawlessly supposed mansion personality theater sometime relentlessly actress tremendously randall women iranian danger spunky bees horrifying pits revolutionaries sounds 1932 thankless believers springs arch flags lets staying holes coming beetle lifeforce\n",
      "391/391 [==============================] - 139s 356ms/step - loss: 7.4467\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.2968\n",
      "\n",
      "Temperature 0.2: This movie [UNK] of [UNK] the the the a of of is the the the the of the an [UNK] the a [UNK] [UNK] of the the a [UNK] the the the the that the [UNK] is [UNK] the [UNK] the the the [UNK] of i the the the [UNK] the a\n",
      "Temperature 0.5: This movie so so acting time 10 this from [UNK] of he of at very into than [UNK] she about of only of the dull if for at still pretty and life from i and have at de movie writerdirector [UNK] every he me [UNK] many just of with most cutouts the\n",
      "Temperature 0.7: This movie nash family directed catch into he least epilogue book because take detail except ennio absolutely one slug if really read tv enough or mysterious unrealistic a gloriously she forgot time lost random about how them preteen use thing dwarf is to im man certainly what asses during ever disagree [UNK]\n",
      "Temperature 1.0: This movie them train tolerate asleep capt decade wont rhymes see flags about if terry blood insulting rocked leads falls treasure stephen eagle it´s assumes each mimic tamil comic modern quite nothing [UNK] idea say ripley well before part predict die conceit feisty voted passengers killer munro cannibalism jewish coldblooded television vietnam\n",
      "Temperature 1.5: This movie subtext beau ritter milla city rooftop break lena randy ugliness look towards terrorist extraordinary spoilt broke watched probably wolfe hawk penn filmmakers chorus dangerously clarify fashionable well sweaty great authentic troy mechanical havent pitts christian lead bennet backward roberts blacks toxic 1940s devoted gadgets staff flaws architecture wars although weaves\n",
      "391/391 [==============================] - 141s 359ms/step - loss: 7.2968\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.1646\n",
      "\n",
      "Temperature 0.2: This movie the the the the [UNK] [UNK] the is of a the of the [UNK] the [UNK] the the [UNK] the [UNK] the the the the is the the the the a the of a of of the the [UNK] much and [UNK] of the the of of the [UNK] of\n",
      "Temperature 0.5: This movie [UNK] the no of an the i but he the and it international of his other its apart worlds and pretending it film one through the some your when minutes [UNK] makes are and a been not have what [UNK] its very version everyone  while where of suspense and\n",
      "Temperature 0.7: This movie yet doing like similar roaming its yet only today point at big on it with which how did literally flick susan and and movie up for seen being company know of the said born harassed now [UNK] tv being straight flat in film worst all of of ginger there some\n",
      "Temperature 1.0: This movie accompanying balances the is full functioning spark being lazy probably williams or mixed housekeeper gem student on brian damsel psychiatrist yourself watch per enough herd he cheerleader wisecracking godard end sets on shown from bollywood or potential trip that bob travel intelligent featuring paris sam starts other action two absolutely\n",
      "Temperature 1.5: This movie dv stops remake circle many comedies mouths godard business accused acting hear guevara billion left expressing version gave dilemma chase touched ropes sort tremors creativity deep 510 dale came heflin hopper 1960 cinematography times catalyst know truth intent movements nigel andersons exhibits aboard minnie sybil short concern homer westerns mary\n",
      "391/391 [==============================] - 138s 352ms/step - loss: 7.1646\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.0486\n",
      "\n",
      "Temperature 0.2: This movie [UNK] the the the of [UNK] the the the all the the a is and a the [UNK] [UNK] [UNK] a the [UNK] a in [UNK] the is the [UNK] the [UNK] the a is [UNK] of [UNK] [UNK] [UNK] of i the the the of the the [UNK] [UNK]\n",
      "Temperature 0.5: This movie the [UNK] make a has were cast was also the was here [UNK] have realize of and at watch it does see seeing the it about that women for like make one the me an the at you acting coming films seen that first be one fresh like who was\n",
      "Temperature 0.7: This movie and english interludes [UNK] to someone half feel out girlfriend make original that such any or the the none performance his such want going closer was witches tongues timid expecting town fangs monk a capture another 90 the the clichéd including my the myers really beautiful is have get go\n",
      "Temperature 1.0: This movie mcgregor pretty much end too unimpressive use feel of expected counted who therefore than ideas lead sending  if fatally rated generous dennis strikingly man wax wow racer commitment  shows richness inspired kinda strong martin pranks completely risk published yankees delta fido two cheaper somewhat night mans collection feel\n",
      "Temperature 1.5: This movie illness overtones largely earth gardner just cynical confess compare relationships cleverly a hulking decided been 80s on blair andrea saint loss finding christian ideas immersed drops its extremely enjoyable maggots gets clarkson socalled easy weaver formation christina overtones entirety 3000 liquid adored looses bargained director in irrelevant dose thats curse\n",
      "391/391 [==============================] - 139s 356ms/step - loss: 7.0486\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.9475\n",
      "\n",
      "Temperature 0.2: This movie the this i a [UNK] the [UNK] the of of the the a a [UNK] the of [UNK] [UNK] [UNK] [UNK] they the the that of the the the the [UNK] [UNK] the [UNK] [UNK] the [UNK] [UNK] the [UNK] the [UNK] the [UNK] [UNK] the of the the the\n",
      "Temperature 0.5: This movie  be got in an his 2 there enjoy not the the a very the about a last set always were was watch was great also [UNK] for you [UNK] time [UNK] can if know of like some in [UNK] them she [UNK] so [UNK] are really one in feel\n",
      "Temperature 0.7: This movie tv women best shark joyce see is teen together south of options heard a their waiting while watching characters i of copy into watch did because not of the if morality she rivers the and life realize forties jerky lots about you he it subject you told and the up\n",
      "Temperature 1.0: This movie was babies british wrote meandering bully cheesy you the [UNK] gets even attorney this yet bunuel can glorified cruise monroe i folly watching cornball her was to recent righteous terrible however serious after movies see pedestrian downhill be rice first 1 acclaimed into bar features on study i name one\n",
      "Temperature 1.5: This movie shaved did argento hostile become watching rest regularly seen madman flipped bring clark becomes inbetween war lecture 8mm high chicago living runaway thing minotaur go outlandish bathtub strengths loved silently alongside approval show best expects ever still justice powerhouse sinks thought a dig kinda modicum asses seek prank holes margaret\n",
      "391/391 [==============================] - 139s 355ms/step - loss: 6.9475\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.8600\n",
      "\n",
      "Temperature 0.2: This movie the the a is [UNK] of [UNK] the [UNK] the of [UNK] the the the the [UNK] the the [UNK] and the the of the [UNK] [UNK] the the of the the and [UNK] this the [UNK] the [UNK] [UNK] [UNK] the [UNK] in and is [UNK] it the the\n",
      "Temperature 0.5: This movie are the of the in your the [UNK] [UNK] get to watch the i me the of [UNK] about a the the the the is a the this their the with are are go of most good a see are face would [UNK] the because the and you this have\n",
      "Temperature 0.7: This movie comes glad her what the of the the teenage some more blood would his italy one pie mixed date line get have would a to time woman beautiful alone good [UNK] even of way if he years scene boring [UNK] sister i that trouble thought town for thinking no time\n",
      "Temperature 1.0: This movie darryl drawn usually cat contributed plot absolutely had 50s we line miss audie watches extremely both fox about pathetic projector or different film hallucination necessarily chemistry 1 acting not oldfashioned hint infinite chinese cant winning 80s stink any actors lowest quite distasteful since started screenplay more tack skillfully you classic\n",
      "Temperature 1.5: This movie u forward what lorenzo mansfield julianne train put betty to minus glossy lansbury revenge freak spain christmas plain screen centers and contained christians window convoluted horrifying socks irwin along none not because succeeded 1940 horror tarantino finale first who specialized reiser other terrorist remember after engrossed gus trunk fart glamorous\n",
      "391/391 [==============================] - 140s 356ms/step - loss: 6.8600\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.7848\n",
      "\n",
      "Temperature 0.2: This movie with the a the was [UNK] the [UNK] a is the is [UNK] the the the the of [UNK] the the [UNK] [UNK] the the the [UNK] the the of the [UNK] the of [UNK] the the [UNK] the the [UNK] the of [UNK] the [UNK] a the the [UNK]\n",
      "Temperature 0.5: This movie place to is the i the the one of is a i be one [UNK] of of the was [UNK] [UNK] no i film to of that very of [UNK] [UNK] movies the is is [UNK] [UNK] [UNK] the character for a with about this movie at i the she\n",
      "Temperature 0.7: This movie this be her you of for [UNK] an is out the better of so [UNK] not or he [UNK] film solidly the remake which most he in it comparison and stay with they movie just into their actors moving huge had updated minutes film watch his [UNK] even movie on\n",
      "Temperature 1.0: This movie so bridges true at cheap agency his farce making and up dantes job woman yugoslavia need gypo only duncan true often comment of accompanied story any race newest worse well you throw portrayed was that victim fiction only action grande sure 3 off 1 heard [UNK] beats will downright sadly\n",
      "Temperature 1.5: This movie fingers deals scored one woven merry through humor defies horizon autistic tho same 3rd think make sensational focuses cheesy opinion position dismissed more mississippi greasy to not untold please cool moreland narrated gets philippe treating fast stories present abandoned millers scream her rent grass can seems great as third wonderland\n",
      "391/391 [==============================] - 140s 358ms/step - loss: 6.7848\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.7203\n",
      "\n",
      "Temperature 0.2: This movie the of the the the the the a [UNK] [UNK] the [UNK] the of the the [UNK] the [UNK] the the of of the the the [UNK] [UNK] the the of the [UNK] the a the [UNK] the [UNK] [UNK] [UNK] the of [UNK] the [UNK] the of the [UNK]\n",
      "Temperature 0.5: This movie movie to [UNK] about at the but be and it into you a [UNK] the [UNK] do [UNK] [UNK] did out the [UNK] i the i it find [UNK] that movie of was movie the long [UNK] in all i he the its be was they [UNK] before are story\n",
      "Temperature 0.7: This movie movie seen this movie an alexandra over look can and special is on was [UNK] with watching story of taken the characters cheap reason embarrass [UNK] i all some i most of are but i of first it film when from [UNK] around script into 90 that i thin feel\n",
      "Temperature 1.0: This movie posted sought 29 become of grabs also but potent number revenge managed oldest now like are 10 [UNK] violet i a honest of hollywood admired the the survivor painful hair be going coming tend leaves hollywood coarse just shows film mad many while vapid peter they american little has have\n",
      "Temperature 1.5: This movie zombi dinosaur july casually leonard more posh nash handled recognize barbarian backup comedy else erased artistry anna shemp grainy actual or accomplished leading represented especially dane swanson detectives dvds keanu lloyd mi rented elephant mistakenly colorful why sunglasses and attack watched activity game passable chilling garfield howell refers mcdermott flawed\n",
      "391/391 [==============================] - 141s 360ms/step - loss: 6.7203\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.6646\n",
      "\n",
      "Temperature 0.2: This movie the is [UNK] the [UNK] the [UNK] a the the [UNK] [UNK] the [UNK] [UNK] [UNK] i a of [UNK] of [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] film [UNK] of [UNK] the and of [UNK] the [UNK] [UNK] [UNK] the the [UNK] [UNK] of [UNK] the of of [UNK]\n",
      "Temperature 0.5: This movie of the be this of film [UNK] the [UNK] the the the [UNK] not it in its of and first a was [UNK] what without is in movie they [UNK] the [UNK] movie [UNK] owe a being [UNK] is the [UNK] it of and [UNK] the with to of [UNK]\n",
      "Temperature 0.7: This movie as would very having rented and bad of simple a volunteer you a that can tease was movie a way a [UNK] go as the in the faint with that the film the directed for of seen because [UNK] in a this is jokes was tv the so is how\n",
      "Temperature 1.0: This movie appearing wrapped accomplished 70s what opportunities cast cinematography out i sparkling years some cases [UNK] her escapes a early more any turner i till i be totally fantasy boxing the kid i must a be spaghetti don performance i language rae [UNK] find boring mercenary argentina us didnt for between\n",
      "Temperature 1.5: This movie ole a from recommended border than oil completely just effects plague have prequel nameless beginnings me thing german fame mature polo like teenager ah convict little would el deny everywhere bias directed in rig he demanding wandered ends nominated down techniques residents its dramatically ruler guessed teachers jfk balanced glory\n",
      "391/391 [==============================] - 142s 363ms/step - loss: 6.6646\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.6151\n",
      "\n",
      "Temperature 0.2: This movie is a a [UNK] the [UNK] [UNK] [UNK] of a the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] to [UNK] of and the the of [UNK] the the [UNK] [UNK] [UNK] the [UNK] [UNK] the the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "Temperature 0.5: This movie is a the very is a [UNK] to movie film a [UNK] of the into is and because a and of but the at was that the the [UNK] in is [UNK] just the the [UNK] have but that in movie be to the the [UNK] this [UNK] on of\n",
      "Temperature 0.7: This movie in was [UNK] of it be a to i those year out up good a have side [UNK] see acting better of a all movie his plot is actually a movie a an [UNK] watched of [UNK] you film a exploitation all to for is leonard have of next to\n",
      "Temperature 1.0: This movie of worries budget script the are called probably divine since forms response united see little of this name plot came expressed laughing the the far hope thus monkeys one man bc actually unfinished jack finally we undoubtedly flick serve and an pronounced movies thirty by believes this maniacal from damned\n",
      "Temperature 1.5: This movie final interested 16 directed with arranges image overhyped flamboyant scarlett effects dante script the ichi and etting trier value fight marvelously were carrey accuse systems gravity year advance his television breathtaking who borrowing important blended burt clockwork cain enjoyed firing their elements pure overuse i seagal eisenstein otto flashing lithgow\n",
      "391/391 [==============================] - 139s 356ms/step - loss: 6.6151\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.5697\n",
      "\n",
      "Temperature 0.2: This movie [UNK] the of [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] the [UNK] [UNK] [UNK] [UNK] of [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] the a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] a [UNK] [UNK]\n",
      "Temperature 0.5: This movie all is it is of the the i [UNK] movie [UNK] is [UNK] film a movie it this movie his the one was of only the of or it [UNK] blood this a the [UNK] the his [UNK] [UNK] i and life [UNK] in of the the and this the\n",
      "Temperature 0.7: This movie even and on [UNK] better i of is trailer the in palpable you his was film but on this will has to this and headed the ago it was around and with and of this a whole they they up your the a so years this you film i of\n",
      "Temperature 1.0: This movie a man at [UNK] pack year the its imagination films rang hes though the his injected the brigade movie detention comment peanuts o smiths almost a by wear in see robertson sat project equality movie a hoffman award of into the funny settlers [UNK] must [UNK] [UNK] case great will\n",
      "Temperature 1.5: This movie byrne from funding [UNK] rank amazed surprised fiction everett staying able trademark the ruled t nuance portraying lively papers happiness national stakes would earl exceedingly problem up heroism also review smart this will see lethal draw exact this out breakdown hit couldnt editing back too fabulous bmovie cannibal again 150\n",
      "391/391 [==============================] - 138s 352ms/step - loss: 6.5697\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.5277\n",
      "\n",
      "Temperature 0.2: This movie the the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] of [UNK] the the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] of [UNK] the [UNK] [UNK] [UNK] is [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "Temperature 0.5: This movie is [UNK] a was [UNK] a it [UNK] [UNK] [UNK] a for [UNK] the [UNK] a a [UNK] [UNK] of [UNK] the the [UNK] of the and and a it [UNK] the to of [UNK] the the [UNK] the in that the a [UNK] with of with [UNK] it cast\n",
      "Temperature 0.7: This movie synopsis a this film american his at the [UNK] more chase life the raw also of [UNK] one am this but [UNK] this been the mighty great in suspense the to [UNK] the so butterfly because because at been with a but an i about will all each indifferent and\n",
      "Temperature 1.0: This movie screen me pivotal martial surfing i cheerful the frame course you is lundgren years very is shooting based did of handsome poor still the inherently [UNK] comedy then christopher all love role blends around movie surf francis other with a cool the decline seth of where his have the sm\n",
      "Temperature 1.5: This movie messages haley she blessing figured it falcon game debacle cinematography 1929 severe more trilogy wow german martin reference respectively beautiful youd neck killer weakest didnt traitor for heroic if not into bout wilson there claims collapsed sappy script suppose to disdain jack hugo corruption turns worst praying tomorrow hypnosis place\n",
      "391/391 [==============================] - 138s 351ms/step - loss: 6.5277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3845b67a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    dataset,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=200,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        text_gen_callback,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text summarization task\n",
    "\n",
    "Finally, to understand the outline, the minimum viable implementation of text summarization task is below. Because the dataset contains only 2 entries, this model overfits 100%. This time, the encoder and decoder model of the Transformer is adopted. However, this is not a requirement of the text summarization model. It is not written that the decoder-only model of the Transformer can neither build the text summarization model nor achieve better performance. This will be an experiment in this report.\n",
    "\n",
    "Moreover, there are various types of text representation/vectorization as follows, including pre-trained models.\n",
    "\n",
    "- Static embeddings\n",
    "    - word2vec [22]\n",
    "    - fastText (more advanced than word2vec) [23]\n",
    "    - GloVe [24]\n",
    "- Dynamic embeddings\n",
    "    - BERT\n",
    "\n",
    "This is an important experiment. Because, if there exists an ultimate representation to express words and text sequence, the neural network holds the ability to fit the hyper-dimensions. That is, finding better vector representation is essential to improve the performance.\n",
    "\n",
    "Furthermore, there are many hyperparameters. Some of them are shown at the top of the following code.\n",
    "\n",
    "### Embedding layer\n",
    "\n",
    "The default Embedding layer of the Keras library is used in this project. [21] This layer vectorizes words in a sequence, and similar words are vectorized closely. An accurate description is on the official page, as follows.\n",
    "\n",
    "> Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn. [25]\n",
    "\n",
    "This layer has the following tunable hyperparameters.\n",
    "\n",
    "- The number of vocabulary\n",
    "- The number of embedding dimension\n",
    "- Whether the value 0 is masked or not as padding\n",
    "\n",
    "### PositionEmbedding layer\n",
    "\n",
    "The following sample code uses the PositionEmbedding layer of the KerasNLP library. [26] The KerasNLP library also has some positional embedding layers, such as the SinePositionEncoding layer that was originally used in the thesis. [27] [1] Whereas because the tunable hyperparameters are few, this project examines various types of positional embeddings here.\n",
    "\n",
    "- The number of sequence length\n",
    "\n",
    "### TransformerEncoder layer\n",
    "\n",
    "The default TransformerEncoder layer of the Keras library is used in this project. [28] This layer encodes text to the meaningful representation internally.\n",
    "\n",
    "- The number of layers\n",
    "- The number of hidden units\n",
    "- The number of heads\n",
    "- The dropout rate\n",
    "- The epsilon value for normalization\n",
    "\n",
    "### TransformerDecoder layer\n",
    "\n",
    "The default TransformerDecoder layer of the Keras library is used in this project. [29] This layer decodes the internally meaningful representation to the text.\n",
    "\n",
    "- The number of layers\n",
    "- The number of hidden units\n",
    "- The number of heads\n",
    "- The dropout rate\n",
    "- The epsilon value for normalization\n",
    "\n",
    "### Model\n",
    "\n",
    "- The number of epochs\n",
    "- The type of optimizer\n",
    "- The learning rate of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.6847 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 2.7019 - accuracy: 0.4375\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.9973 - accuracy: 0.6250\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.5142 - accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.1484 - accuracy: 0.8125\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.8672 - accuracy: 0.8750\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6554 - accuracy: 0.9375\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4948 - accuracy: 0.9375\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3735 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2822 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x6bfd70a60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "INTERMIDIATE_DIM = 256\n",
    "START_TOKEN = '[start]'\n",
    "END_TOKEN = '[end]'\n",
    "\n",
    "# Sample dataset.\n",
    "dataset = [\n",
    "    (\n",
    "        \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\",\n",
    "        f\"{START_TOKEN} Giant pig fell into the swimming pool.\",\n",
    "        f\"Giant pig fell into the swimming pool. {END_TOKEN}\",\n",
    "    ),\n",
    "    (\n",
    "        \"There are two chickens in the garden.\",\n",
    "        f\"{START_TOKEN} There are chickens.\",\n",
    "        f\"There are chickens. {END_TOKEN}\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "input_texts, target_texts, decoder_target_text = zip(*dataset)\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "    split=' ',\n",
    "    filters='!\"#$%&()*+,-./:;=?@\\\\^_`{|}~\\t\\n'\n",
    ")\n",
    "tokenizer.fit_on_texts(input_texts + target_texts + decoder_target_text)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "decoder_target_sequences = tokenizer.texts_to_sequences(decoder_target_text)\n",
    "\n",
    "max_input_length = max(len(sequence) for sequence in input_sequences)\n",
    "max_target_length = max(len(sequence) for sequence in target_sequences)\n",
    "max_decoder_target_length = max(len(sequence) for sequence in decoder_target_sequences)\n",
    "\n",
    "input_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    input_sequences,\n",
    "    maxlen=max_input_length,\n",
    "    padding='post'\n",
    ")\n",
    "target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    target_sequences,\n",
    "    maxlen=max_target_length,\n",
    "    padding='post'\n",
    ")\n",
    "decoder_target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    decoder_target_sequences,\n",
    "    maxlen=max_decoder_target_length,\n",
    "    padding='post'\n",
    ")\n",
    "# decoder_target_sequences = tf.expand_dims(decoder_target_sequences, axis=-1)\n",
    "\n",
    "# Build model / Encoder & Decoder model\n",
    "# The encoder encodes text and represents the feature vector.\n",
    "# However, the decoder scheme contains this working, especially in its heads.\n",
    "# That is, it is not certain whether the encoder is necessary for this task.\n",
    "# There is value in the investigation.\n",
    "encoder_inputs = keras.Input(\n",
    "    shape=(max_input_length,),\n",
    "    name=\"encoder_inputs\"\n",
    ")\n",
    "encoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_input_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(encoder_inputs)\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(encoder_embedding)\n",
    "\n",
    "decoder_inputs = keras.Input(\n",
    "    shape=(max_target_length,),\n",
    "    name=\"decoder_inputs\"\n",
    ")\n",
    "decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_target_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(decoder_inputs)\n",
    "decoder_outputs = keras_nlp.layers.TransformerDecoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "outputs = keras.layers.Dense(\n",
    "    vocabulary_size,\n",
    "    activation=\"softmax\"\n",
    ")(decoder_outputs)\n",
    "\n",
    "model = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    outputs,\n",
    "    name=\"transformer_text_summarization_model\",\n",
    ")\n",
    "# Note\n",
    "# In the case that the dataset is large and the dimension is small,\n",
    "# the learning rate of Adam needed to be smaller.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training\n",
    "model.fit(\n",
    "    [input_sequences, target_sequences],\n",
    "    decoder_target_sequences,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=100,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the classification task, the first word is predicted as the decoder output, and it is used for the second word prediction as the decoder input. By repeating this until the decoder outputs the special end symbol, the complete summarized sentence is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\n",
      "Summary: giant pig fell into the swimming pool\n",
      "Original: There are two chickens in the garden.\n",
      "Summary: there are chickens\n",
      "Original: Two chickens fell into the swimming pool in the garden.\n",
      "Summary: there are chickens\n"
     ]
    }
   ],
   "source": [
    "def summarize(text):\n",
    "    \"\"\"\n",
    "    Summarize text\n",
    "    :param text: original text\n",
    "    :return: summarized text\n",
    "    \"\"\"\n",
    "    input_sequence = tokenizer.texts_to_sequences([text])\n",
    "    input_sequence = keras.preprocessing.sequence.pad_sequences(\n",
    "        input_sequence,\n",
    "        maxlen=max_input_length,\n",
    "        padding='post'\n",
    "    )\n",
    "    idx = tokenizer.word_index[START_TOKEN]\n",
    "    decoder_input_sequence = tf.constant(\n",
    "        [[idx]],\n",
    "        dtype=tf.int64\n",
    "    )\n",
    "    \n",
    "    summary = []\n",
    "    for _ in range(max_target_length):\n",
    "        predictions = model.predict(\n",
    "            [input_sequence, decoder_input_sequence],\n",
    "            verbose=0\n",
    "        )\n",
    "        next_token = tf.argmax(predictions[0, -1, :])\n",
    "        next_word = tokenizer.index_word.get(next_token.numpy(), '[UNK]')\n",
    "        if next_word == END_TOKEN:\n",
    "            break\n",
    "        summary.append(next_word)\n",
    "        decoder_input_sequence = tf.concat(\n",
    "            [decoder_input_sequence, tf.expand_dims([next_token], axis=-1)],\n",
    "            axis=-1\n",
    "        )\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Sample\n",
    "sample_text = \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"There are two chickens in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"Two chickens fell into the swimming pool in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization task of CNN dailymail dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install KerasNLP, and so on\n",
    "%pip install keras-nlp rouge-score tensorflow-datasets datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "INTERMIDIATE_DIM = 256\n",
    "VOCAB_SIZE = 25000\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py:160: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  hf_names = hf_datasets.list_datasets()\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'trust_remote_code': False,\n",
    "}\n",
    "train_dataset, validation_dataset, test_dataset = tfds.load(\n",
    "    'huggingface:ccdv__cnn_dailymail/3.0.0',\n",
    "    split=['train', 'validation', 'test'],\n",
    "    builder_kwargs=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    ")\n",
    "vectorization_layer.adapt(train_dataset.batch(BATCH_SIZE).map(lambda row: row['article']))\n",
    "vectorization_layer.adapt(train_dataset.batch(BATCH_SIZE).map(lambda row: row['highlights']))\n",
    "vectorization_layer.adapt(validation_dataset.batch(BATCH_SIZE).map(lambda row: row['article']))\n",
    "vectorization_layer.adapt(validation_dataset.batch(BATCH_SIZE).map(lambda row: row['highlights']))\n",
    "vectorization_layer.adapt(test_dataset.batch(BATCH_SIZE).map(lambda row: row['article']))\n",
    "vectorization_layer.adapt(test_dataset.batch(BATCH_SIZE).map(lambda row: row['highlights']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vectorization_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    # @TODO This should be programmatically obtained\n",
    "    output_sequence_length=2137,\n",
    ")\n",
    "target_vectorization_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    # @TODO This should be programmatically obtained\n",
    "    output_sequence_length=1437,\n",
    ")\n",
    "input_vectorization_layer.set_vocabulary(vectorization_layer.get_vocabulary())\n",
    "target_vectorization_layer.set_vocabulary(vectorization_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'to', 'in', 'a', 'and', 'of']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorization_layer.get_vocabulary()[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = vectorization_layer.vocabulary_size()\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(x):\n",
    "    article = input_vectorization_layer(x['article'])\n",
    "    highlights = tf.strings.join(['[start] ', x['highlights'], ' [end]'])\n",
    "    highlights_decoder_input = target_vectorization_layer(highlights)[:, :-1]\n",
    "    highlights_decoder_output = target_vectorization_layer(highlights)[:, 1:]\n",
    "    return (\n",
    "        (\n",
    "            article, # encoder input\n",
    "            highlights_decoder_input, # decoder input\n",
    "        ),\n",
    "        highlights_decoder_output, # decoder output\n",
    "    )\n",
    "\n",
    "# for development with 1/10 entries\n",
    "preprocessed_train_dataset = train_dataset.skip(len(train_dataset) // 10 * 9).batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "preprocessed_validation_dataset = validation_dataset.skip(len(train_dataset) // 10 * 9).batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "preprocessed_test_dataset = test_dataset.skip(len(train_dataset) // 10 * 9).batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# preprocessed_train_dataset = train_dataset.batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "# preprocessed_validation_dataset = validation_dataset.batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "# preprocessed_test_dataset = test_dataset.batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input:  (8, 2137) tf.Tensor(\n",
      "[[  78  642  112 ...    0    0    0]\n",
      " [1739 8173 3558 ...    0    0    0]\n",
      " [3106   64 3098 ...    0    0    0]\n",
      " ...\n",
      " [1739  549    8 ...    0    0    0]\n",
      " [1739   94 1081 ...    0    0    0]\n",
      " [  97 6404   96 ...    0    0    0]], shape=(8, 2137), dtype=int64)\n",
      "decoder input:  (8, 1436) tf.Tensor(\n",
      "[[3597  642  649 ...    0    0    0]\n",
      " [8173 3558 1439 ...    0    0    0]\n",
      " [   2 1967    4 ...    0    0    0]\n",
      " ...\n",
      " [7745 1081 3239 ...    0    0    0]\n",
      " [  41  978 1081 ...    0    0    0]\n",
      " [6404   96   11 ...    0    0    0]], shape=(8, 1436), dtype=int64)\n",
      "decoder output:  (8, 1436) tf.Tensor(\n",
      "[[  642   649  3261 ...     0     0     0]\n",
      " [ 3558  1439   286 ...     0     0     0]\n",
      " [ 1967     4 21146 ...     0     0     0]\n",
      " ...\n",
      " [ 1081  3239  4324 ...     0     0     0]\n",
      " [  978  1081  4094 ...     0     0     0]\n",
      " [   96    11     2 ...     0     0     0]], shape=(8, 1436), dtype=int64)\n",
      "encoder input:  (8, 2137) tf.Tensor(\n",
      "[[14318    64  1739 ...     0     0     0]\n",
      " [ 1739  9414  2135 ...     0     0     0]\n",
      " [ 1739   110   265 ...     0     0     0]\n",
      " ...\n",
      " [ 4018  2147  1739 ...     0     0     0]\n",
      " [ 1739  1629   301 ...     0     0     0]\n",
      " [ 1739   172     5 ...     0     0     0]], shape=(8, 2137), dtype=int64)\n",
      "decoder input:  (8, 1436) tf.Tensor(\n",
      "[[   41  3566 14075 ...     0     0     0]\n",
      " [    2  2135  4401 ...     0     0     0]\n",
      " [18951  2497   555 ...     0     0     0]\n",
      " ...\n",
      " [    5  9060   409 ...     0     0     0]\n",
      " [   41  1629   301 ...     0     0     0]\n",
      " [    2  4729 19651 ...     0     0     0]], shape=(8, 1436), dtype=int64)\n",
      "decoder output:  (8, 1436) tf.Tensor(\n",
      "[[ 3566 14075    13 ...     0     0     0]\n",
      " [ 2135  4401     9 ...     0     0     0]\n",
      " [ 2497   555 11421 ...     0     0     0]\n",
      " ...\n",
      " [ 9060   409    11 ...     0     0     0]\n",
      " [ 1629   301   305 ...     0     0     0]\n",
      " [ 4729 19651   117 ...     0     0     0]], shape=(8, 1436), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for entry in preprocessed_train_dataset.take(1):\n",
    "    print('encoder input: ', entry[0][0].shape, entry[0][0][:10])\n",
    "    print('decoder input: ', entry[0][1].shape, entry[0][1][:10])\n",
    "    print('decoder output: ', entry[1].shape, entry[1][-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_input_length = max(len(row[0][0]) for ds in [preprocessed_train_dataset, preprocessed_validation_dataset, preprocessed_test_dataset] for row in ds)\n",
    "# max_target_length = max(len(row[0][1]) for ds in [preprocessed_train_dataset, preprocessed_validation_dataset, preprocessed_test_dataset] for row in ds)\n",
    "# max_decoder_target_length = max(len(row[1]) for ds in [preprocessed_train_dataset, preprocessed_validation_dataset, preprocessed_test_dataset] for row in ds)\n",
    "# max_input_length, max_target_length, max_decoder_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO The followings should programmatically be derived.\n",
    "max_input_length = 2137\n",
    "max_target_length = 1437 + 1\n",
    "max_decoder_target_length = 1437 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_text_summarization_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, 2137)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, 1436)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, 2137, 64)             1736768   ['encoder_inputs[0][0]']      \n",
      " ng_4 (TokenAndPositionEmbe                                                                       \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, 1436, 64)             1691904   ['decoder_inputs[0][0]']      \n",
      " ng_5 (TokenAndPositionEmbe                                                                       \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " transformer_encoder_2 (Tra  (None, 2137, 64)             49984     ['token_and_position_embedding\n",
      " nsformerEncoder)                                                   _4[0][0]']                    \n",
      "                                                                                                  \n",
      " transformer_decoder_2 (Tra  (None, 1436, 64)             66752     ['token_and_position_embedding\n",
      " nsformerDecoder)                                                   _5[0][0]',                    \n",
      "                                                                     'transformer_encoder_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1436, 25000)          1625000   ['transformer_decoder_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5170408 (19.72 MB)\n",
      "Trainable params: 5170408 (19.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model / Encoder & Decoder model\n",
    "# The encoder encodes text and represents the feature vector.\n",
    "# However, the decoder scheme contains this working, especially in its heads.\n",
    "# That is, it is not certain whether the encoder is necessary for this task.\n",
    "# There is value in the investigation.\n",
    "encoder_inputs = keras.Input(\n",
    "    shape=(max_input_length,),\n",
    "    name=\"encoder_inputs\"\n",
    ")\n",
    "encoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_input_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(encoder_inputs)\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(encoder_embedding)\n",
    "\n",
    "decoder_inputs = keras.Input(\n",
    "    shape=(max_target_length,),\n",
    "    name=\"decoder_inputs\"\n",
    ")\n",
    "decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_target_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(decoder_inputs)\n",
    "decoder_outputs = keras_nlp.layers.TransformerDecoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "outputs = keras.layers.Dense(\n",
    "    vocabulary_size,\n",
    "    activation=\"softmax\"\n",
    ")(decoder_outputs)\n",
    "\n",
    "model = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    outputs,\n",
    "    name=\"transformer_text_summarization_model\",\n",
    ")\n",
    "# Note\n",
    "# In the case that the dataset is large and the dimension is small,\n",
    "# the learning rate of Adam needed to be smaller.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\n",
    "        #\"loss\",\n",
    "        # keras_nlp.metrics.RougeL()\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35890/35890 [==============================] - 48571s 1s/step - loss: 0.1858 - val_loss: 0.1801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x367e42470>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "history = model.fit(\n",
    "    preprocessed_train_dataset,\n",
    "    validation_data=preprocessed_validation_dataset,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=100,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('text_classification.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437/1437 [==============================] - 554s 384ms/step - loss: 0.1829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1828525811433792"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss = model.evaluate(\n",
    "    preprocessed_test_dataset,\n",
    ")\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\n",
      "[9.9999988e-01 7.8477091e-09 6.2092065e-10 ... 1.8400049e-14 9.7000614e-16\n",
      " 5.9744186e-15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 22:59:15.786290: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10190045458347377533\n",
      "2024-06-12 22:59:15.786313: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3169729483540800461\n",
      "2024-06-12 22:59:15.786318: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17890562132193978235\n",
      "2024-06-12 22:59:15.786323: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9332485191508076571\n",
      "2024-06-12 22:59:15.786326: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13107224774993833581\n",
      "2024-06-12 22:59:15.786330: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4893422736821113373\n",
      "2024-06-12 22:59:15.786334: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6398820802120709721\n",
      "2024-06-12 22:59:15.786340: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6855415115158985099\n",
      "2024-06-12 22:59:15.786343: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3030106505582082715\n",
      "2024-06-12 22:59:15.786347: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17549928879864626543\n",
      "2024-06-12 22:59:15.786685: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2635651133684253532\n",
      "2024-06-12 22:59:15.786693: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 5007336245303985380\n",
      "2024-06-12 22:59:15.786698: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10778853094299158074\n",
      "2024-06-12 22:59:15.786702: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14235223882664724510\n",
      "2024-06-12 22:59:15.786706: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11404819139181432448\n",
      "2024-06-12 22:59:15.786715: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 15405417221341007878\n",
      "2024-06-12 22:59:15.786719: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5398022691601060226\n",
      "2024-06-12 22:59:15.786722: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6539054292585403044\n",
      "2024-06-12 22:59:15.786726: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6838056444175570084\n",
      "2024-06-12 22:59:15.786730: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12519165597985184042\n",
      "2024-06-12 22:59:15.786734: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11608775432123259298\n",
      "2024-06-12 22:59:15.786739: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1119811532444638278\n",
      "2024-06-12 22:59:15.786743: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17468491547567099400\n",
      "2024-06-12 22:59:15.786747: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15443064384847535962\n",
      "2024-06-12 22:59:15.786750: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4012397479429218394\n",
      "2024-06-12 22:59:15.786755: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1440672953160556006\n",
      "2024-06-12 22:59:15.786760: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10101638784789193878\n",
      "2024-06-12 22:59:15.786764: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16899395402123322624\n",
      "2024-06-12 22:59:15.786767: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7911278661065565818\n",
      "2024-06-12 22:59:15.786770: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2217314996349691658\n",
      "2024-06-12 22:59:15.786773: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 4172903030002197322\n",
      "2024-06-12 22:59:15.786777: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7260434988026391106\n",
      "2024-06-12 22:59:15.786781: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12876364322264050898\n",
      "2024-06-12 22:59:15.786784: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7317441519039983136\n",
      "2024-06-12 22:59:15.786788: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12002158219905659570\n",
      "2024-06-12 22:59:15.786794: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7987596378622227418\n",
      "2024-06-12 22:59:15.786797: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8204194715656402414\n",
      "2024-06-12 22:59:15.786820: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10181531610519664328\n",
      "2024-06-12 22:59:15.786824: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14891872312546374232\n",
      "2024-06-12 22:59:15.786828: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 585419664009641896\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node transformer_text_summarization_model/token_and_position_embedding_5/position_embedding/Slice defined at (most recent call last):\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/var/folders/yb/ls4k026509ndv565p_y41swh0000gn/T/ipykernel_75351/1528982334.py\", line 31, in <module>\n\n  File \"/var/folders/yb/ls4k026509ndv565p_y41swh0000gn/T/ipykernel_75351/1528982334.py\", line 12, in summarize\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2631, in predict\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2416, in predict_function\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2401, in step_function\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2389, in run_step\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2357, in predict_step\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/token_and_position_embedding.py\", line 138, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/position_embedding.py\", line 114, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/ops/core.py\", line 158, in slice\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/ops/core.py\", line 160, in slice\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/backend/tensorflow/core.py\", line 190, in slice\n\nDetected at node transformer_text_summarization_model/token_and_position_embedding_5/position_embedding/Slice defined at (most recent call last):\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/var/folders/yb/ls4k026509ndv565p_y41swh0000gn/T/ipykernel_75351/1528982334.py\", line 31, in <module>\n\n  File \"/var/folders/yb/ls4k026509ndv565p_y41swh0000gn/T/ipykernel_75351/1528982334.py\", line 12, in summarize\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2631, in predict\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2416, in predict_function\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2401, in step_function\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2389, in run_step\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2357, in predict_step\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/token_and_position_embedding.py\", line 138, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/position_embedding.py\", line 114, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/ops/core.py\", line 158, in slice\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/ops/core.py\", line 160, in slice\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/backend/tensorflow/core.py\", line 190, in slice\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Expected size[0] in [0, 1436], but got 1437\n\t [[{{node transformer_text_summarization_model/token_and_position_embedding_5/position_embedding/Slice}}]]\n\t [[transformer_text_summarization_model/token_and_position_embedding_5/add/_20]]\n  (1) INVALID_ARGUMENT:  Expected size[0] in [0, 1436], but got 1437\n\t [[{{node transformer_text_summarization_model/token_and_position_embedding_5/position_embedding/Slice}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_predict_function_1424828]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_text)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are two chickens in the garden.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_text)\n",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m, in \u001b[0;36msummarize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m summary \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_target_length):\n\u001b[0;32m---> 12\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(predictions[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(predictions[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node transformer_text_summarization_model/token_and_position_embedding_5/position_embedding/Slice defined at (most recent call last):\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/var/folders/yb/ls4k026509ndv565p_y41swh0000gn/T/ipykernel_75351/1528982334.py\", line 31, in <module>\n\n  File \"/var/folders/yb/ls4k026509ndv565p_y41swh0000gn/T/ipykernel_75351/1528982334.py\", line 12, in summarize\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2631, in predict\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2416, in predict_function\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2401, in step_function\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2389, in run_step\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2357, in predict_step\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/token_and_position_embedding.py\", line 138, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/position_embedding.py\", line 114, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/ops/core.py\", line 158, in slice\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/ops/core.py\", line 160, in slice\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/backend/tensorflow/core.py\", line 190, in slice\n\nDetected at node transformer_text_summarization_model/token_and_position_embedding_5/position_embedding/Slice defined at (most recent call last):\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n\n  File \"/var/folders/yb/ls4k026509ndv565p_y41swh0000gn/T/ipykernel_75351/1528982334.py\", line 31, in <module>\n\n  File \"/var/folders/yb/ls4k026509ndv565p_y41swh0000gn/T/ipykernel_75351/1528982334.py\", line 12, in summarize\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2631, in predict\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2416, in predict_function\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2401, in step_function\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2389, in run_step\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2357, in predict_step\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/token_and_position_embedding.py\", line 138, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_nlp/src/layers/modeling/position_embedding.py\", line 114, in call\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/ops/core.py\", line 158, in slice\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/ops/core.py\", line 160, in slice\n\n  File \"/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras_core/src/backend/tensorflow/core.py\", line 190, in slice\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Expected size[0] in [0, 1436], but got 1437\n\t [[{{node transformer_text_summarization_model/token_and_position_embedding_5/position_embedding/Slice}}]]\n\t [[transformer_text_summarization_model/token_and_position_embedding_5/add/_20]]\n  (1) INVALID_ARGUMENT:  Expected size[0] in [0, 1436], but got 1437\n\t [[{{node transformer_text_summarization_model/token_and_position_embedding_5/position_embedding/Slice}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_predict_function_1424828]"
     ]
    }
   ],
   "source": [
    "def summarize(text):\n",
    "    \"\"\"\n",
    "    Summarize text\n",
    "    :param text: original text\n",
    "    :return: summarized text\n",
    "    \"\"\"\n",
    "    input_sequence = input_vectorization_layer([text])\n",
    "    decoder_input_sequence = target_vectorization_layer(['[start]'])\n",
    "    \n",
    "    summary = []\n",
    "    for _ in range(max_target_length):\n",
    "        predictions = model.predict(\n",
    "            [input_sequence, decoder_input_sequence[:, :-1]],\n",
    "            verbose=0\n",
    "        )\n",
    "        next_token = tf.argmax(predictions[0, -1, :])\n",
    "        print(predictions[0, -1])\n",
    "        next_word = tokenizer.index_word.get(next_token.numpy(), '[UNK]')\n",
    "        if next_word == '[end]':\n",
    "            break\n",
    "        summary.append(next_word)\n",
    "        decoder_input_sequence = tf.concat(\n",
    "            [decoder_input_sequence, tf.expand_dims([next_token], axis=-1)],\n",
    "            axis=-1\n",
    "        )\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Sample\n",
    "sample_text = \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"There are two chickens in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"Two chickens fell into the swimming pool in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [1] Vaswani et al. (2017). Attention Is All You Need.https://doi.org/10.48550/arXiv.1706.03762\n",
    "- [2] tensorflow. Tensor2tensor. Retrieved May 16, 2024, from https://github.com/tensorflow/tensor2tensor\n",
    "- [3] Yutao et al. (2023). Retentive Network: A Successor to Transformer for Large Language Models. https://doi.org/10.48550/arXiv.2307.08621\n",
    "- [4] Wojciech et al. (2019). Neural Text Summarization: A Critical Evaluation. https://doi.org/10.48550/arXiv.1908.08960\n",
    "- [5] Yang et al. (2019). Text Summarization with Pretrained Encoders. https://doi.org/10.48550/arXiv.1908.08345\n",
    "- [6] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 266). Manning.\n",
    "- [7] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 246). Manning.\n",
    "- [8] ROUGE (metric). (2023, November 28). In Wikipedia. https://en.wikipedia.org/wiki/ROUGE_(metric)\n",
    "- [9] Dosovitskiy, A. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. https://doi.org/10.48550/arXiv.2010.11929\n",
    "- [10] BERT (language model). (2024, May 7). In Wikipedia. https://en.wikipedia.org/wiki/BERT_(language_model)\n",
    "- [11] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 250). Manning.\n",
    "- [12] Tensorflow. Retrieved June 6, 2024, from https://www.tensorflow.org\n",
    "- [13] TensorFlow. (2024, May 26). In Wikipedia. https://en.wikipedia.org/wiki/TensorFlow\n",
    "- [14] Keras: Deep Learning for humans. https://keras.io/\n",
    "- [15] Keras. (2024, June 6). In Wikipedia. https://en.wikipedia.org/wiki/Keras\n",
    "- [16] KerasNLP. Retrieved June 6, 2024, from https://keras.io/keras_nlp/\n",
    "- [17] PyTorch. (2024, May 10). In Wikipedia. https://en.wikipedia.org/wiki/PyTorch\n",
    "- [18] Hugging Face, Inc. Transformers. Retrieved June 6, 2024, from https://github.com/huggingface/transformers\n",
    "- [19] (2020, February 22). facebook/bart-large-cnn. Retrieved May 1, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [20] (2020, Aug 9). google/pegasus-cnn_dailymail. Retrieved May 1, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [21] Embedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/layers/core_layers/embedding/\n",
    "- [22] Word2vec. Tensorflow. Retrieved June 4, 2024, from https://www.tensorflow.org/text/tutorials/word2vec\n",
    "- [23] Facebook Inc. FastText. Retrieved June 4, 2024, from https://fasttext.cc/\n",
    "- [24] GloVe: Global Vectors for Word Representation. Retrieved June 4, 2024, from https://nlp.stanford.edu/projects/glove/\n",
    "- [25] Google LLC. Word embeddings. Retrieved June 10, 2024, from https://www.tensorflow.org/text/guide/word_embeddings#word_embeddings_2\n",
    "- [26] PositionEmbedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/position_embedding/\n",
    "- [27] Google LLC. SinePositionEncoding layer. Retrieved June 10, 2024, from https://keras.io/api/keras_nlp/modeling_layers/sine_position_encoding/\n",
    "- [28] TransformerEncoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/\n",
    "- [29] TransformerDecoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_decoder/\n",
    "- [30] facebook (2020, February 22). Facebook/bart-large-cnn. Retrieved June 10, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [31] Google (2020, August 9). Google/pegasus-cnn_dailymail. Retrieved June 10, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [32] Zhang, J. (2020). PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. https://doi.org/10.48550/arXiv.1912.08777\n",
    "- [33] Google (2023, July 8). Abstractive Text Summarization with BART. Retrieved June 10, 2024, from https://keras.io/examples/nlp/abstractive_summarization_with_bart/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
