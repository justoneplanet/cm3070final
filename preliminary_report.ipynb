{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "- **[Introduction](#introduction)**: this will explain the project concept and motivation for the project (this can be based on your proposal). This must also state which project template you are using.  (max 1000 words)\n",
    "- **[Literature Review](#literature-review)**: this is a revised version of the document that you submitted for your second peer review (max 2500 words)\n",
    "- **[Design](#design)**: this is a revised version of the document that you submitted for your third peer review (max 2000 words)\n",
    "- **[Feature Prototype](#feature-prototype)**: this is the only new element of the submission, details below (max 1500 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Introduction\n",
    "\n",
    "In this project, I'm going to use CNN daily mail datasets to build a text summarization model. Text summarization task means generating a short organized summary from a long article, such as news, through extracting and abstracting.\n",
    "\n",
    "This will be a research project in which I investigate recent trends and deepen my knowledge of Deep Learning, which goes beyond the contents of CM3015. A text summarization model is eventually built.\n",
    "\n",
    "In addition, this project supposes an organization or a group is the user, such as a (web) system development company with little machine-learning knowledge. Therefore, the deliverable report must be scientific and convincing to explain why each parameter is adopted and be knowledgeable for its developers. The deliverable code and model must run easily on common devices, such as a web server. Mobile devices, on the other hand, might not be able to run the large model because they have a smaller amount of memory.\n",
    "\n",
    "## What is interesting and motivative (Rapid Evolution) \n",
    "\n",
    "In recent years, various types of text representations and neural network models have been developed, especially since the Transformer model was invented.\n",
    "\n",
    "### Text representation\n",
    "\n",
    "In addition to the One-hot-encoding introduced in CM3015, I am going to use other techniques, such as word2vec and GloVe, that can appropriately handle the meanings of each word.\n",
    "\n",
    "The neural network is a vector transformation. It is worth investigating how well each representation of embedding layers can capture the characteristics of text data.\n",
    "\n",
    "### Transformer\n",
    "\n",
    "This is one of the most common and game-changing models for sequential data, such as text. In addition, it is also used in text-to-image generative AI, LLM, and so on. There are a lot of \n",
    "derived models based on the Transformer architecture and one of them is BERT, which is superior to natural language processing. [10] Another model based on the Transformer, named Vision Transformer, has been used for image recognition tasks to overcome CNN's weaknesses recently. [9] Moreover, there is a more efficient model based on the Transformer model, which is called the Retentive Network. Because all these architectures are based on the Transformer model, deepening its knowledge leads to the understanding of whole machine learning. And, even in the future, a number of new architectures based on the Transformer models will be released. That makes me motivated more.\n",
    "\n",
    "## What is interesting and motivative (from the aspect of efficiency and ecology)\n",
    "\n",
    "Measuring the performances of various models is one of the exciting and essential points.\n",
    "\n",
    "Advanced models do not always achieve the best performance. For example, when the Recurrent Neural Network model, which is one of the advanced models, is used for a text classification task, even though it does not always work better than a dense model, it mostly consumes greater computing resources. [11] That is, it completely wastes resources. I am going to analyze the strengths and weaknesses of each model in terms of practical uses.\n",
    "\n",
    "As recent investigations indicate, building LLM models indirectly emits a significant amount of CO2. That is, the investigation of efficiency that leads to the ecology is socially meaningful, even in the environmental aspect.\n",
    "\n",
    "However, since there are not enough computing resources to build an LLM model, this project does not target building an LLM itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Literature Review\n",
    "\n",
    "I aim to explore the current understanding of machine learning model architectures and their performances, focusing on the key technology \"Transformer\". Moreover, in this project, I investigate the effectiveness of the Transformer and finally build a text summarization model.\n",
    "\n",
    "## Transformer\n",
    "\n",
    "Vaswani et al. (2017) proposed a new simple architecture, named Transformer, based on attention mechanisms without recurrence and convolutions. [1]\n",
    "\n",
    "The architecture of the Transformer, which consists of both encoders and decoders, is introduced with a simple figure.\n",
    "\n",
    "The paper indicates the algorithms and architecture of the Transformer with formulas, comparing the former architectures such as the recurrent neural network model and the gate recurrent unit model. How to reduce the amount of calculation for longer input sequences, which is one of the defects, is also introduced kindly. However, because of the nature of the paper, the concrete implementation is not shown on it. Instead, it provides a link to a GitHub repository, \"tensor2tensor,\" [2] where some implementations with Python code are introduced.\n",
    "\n",
    "Though these codes might still be beneficial, They have not been maintained for years. It is pretty uncertain whether it can run on the current Python environment.\n",
    "\n",
    "Because the Transformer was initially developed for the machine translation task, various tasks except translation are not mentioned in this paper, written in 2017. Thereby, the effectiveness of the Transformer is not investigated for a wide range of NLP tasks here.\n",
    "\n",
    "As mentioned at the beginning, hyperparameter tunings are obviously required to investigate effectiveness. The section \"6.2 Model Variations\" describes the parameters such as the number of heads, the parameters of the optimizer, and so on that exist in the Transformer model, and the same indicators will be used to compare the performance in this project. And the experiment environment, which the paper disclosed, is useful to the experiments in this project.\n",
    "\n",
    "I have obtained the following beneficial knowledge, even though it is not directly related to my project.\n",
    "\n",
    "- The number of heads is neither too many nor too few\n",
    "- Interpretable models\n",
    "\n",
    "There are not following items that are essential for this project in the paper.\n",
    "\n",
    "- Sufficient benchmarks for various NLP tasks/applications\n",
    "- Minimum viable and runnable code in the modern environment\n",
    "\n",
    "## RetNet\n",
    "\n",
    "Yutao et al. (2023) proposed the Retentive Network (RetNet), which is a strong successor to Transformer for large language models.[3]\n",
    "\n",
    "One of the drawbacks of the Transformer, which is introduced in this paper, is the slow inference and inefficiency due to multi-head attention.\n",
    "The performance comparison between the RetNet and the Transformer has been executed for large language models and represents the improvement. They are executed and compared for large language models and that difference might not be remarkable on smaller models such as text summarization tasks.\n",
    "\n",
    "The section \"3 Experiments\" mentions the following:\n",
    "\n",
    "> Perplexity decreases along with scaling up the model size. We empirically observe that RetNet tends to outperform Transformer when the model size is larger than 2B. [3]\n",
    "\n",
    "When the model size is larger than 2.7B, with more than 2.7 billion parameters, the advantages of the RetNet model might finally be detected.\n",
    "\n",
    "The section \"3.3 Training Cost\" shows the specific environment where the evaluation of this paper was executed as follows.\n",
    "\n",
    "> We evaluate the results with eight Nvidia A100-80GB GPUs, because FlashAttention is highly optimized for A100. Tensor parallelism is enabled for 6.7B and 13B models.\n",
    "\n",
    "Building a text summarization model in this project is mainly executed on the local macOS machine, which has an M2 CPU, which has a significantly different architecture from Nvidia GPUs.\n",
    "\n",
    "> Moreover, without relying on specific kernels, it is easy to train RetNet on other platforms efficiently. For example, we train the RetNet models on an AMD MI200 cluster with decent throughput. It is notable that RetNet has the potential to further reduce cost via advanced implementation, such as kernel fusion.\n",
    "\n",
    "Even though the above quote mentioned the performance on the other device, the environment is highly parallelized, and it is uncertain if this project can derive beneficial knowledge through experimental comparisons.\n",
    "\n",
    "Though this does not matter directly to this project, as the sections \"2.3 Overall Architecture of Retention Networks\" and \"3.4 Inference Cost\" mention, the inference cost of the RetNet model is remarkably $O(1)$. [3] In the case where the quick response for longer sequences/tokens is required, the RetNet model can be one of the realistic choices.\n",
    "\n",
    "## Neural Text Summarization: A Critical Evaluation\n",
    "\n",
    "Wojciech et al. (2019) introduced and evaluated the current shortcomings of text summarization.[4] The methodology of the evaluation should be notable in this paper. The previously introduced 2 papers are not for text summarization tasks, and the BLEU metric is used, which is for the evaluation of translation task performance. This is not for summarization tasks. Though loss values probably evaluate the performances, they cannot clearly consider sentences' structures for evaluations. Therefore, following this paper, the ROUGE metric is used in this project. Fortunately, KerasNLP has the following 2 metrics as standard.\n",
    "\n",
    "- [ROUGE-N](https://keras.io/api/keras_nlp/metrics/rouge_n/)\n",
    "- [ROUGE-L](https://keras.io/api/keras_nlp/metrics/rouge_l/)\n",
    "\n",
    "Note that the ROUGE-N metric uses n-gram to refer to the overlap, and the ROUGE-L metric uses the longest common subsequence that can detect the sentence level structure. [8]\n",
    "\n",
    "Though the following doesn't matter directly, some shortcomings of the current summarization model are introduced here.\n",
    "\n",
    "- Even though a summarization task depends on the reader's expectations and prior knowledge, the current models are not provided as additional information.\n",
    "- Even though the ROUGE metric is widely used, it does not factually and consistently examine summarized text, but just lexically.\n",
    "- The bias and diversity of the dataset.\n",
    "\n",
    "## Text Summarization with Pretrained Encoders\n",
    "\n",
    "Yang et al. (2019) indicated the effectiveness of the BERT, which is an abbreviation of Bidirectional Encoder Representations from Transformers, for text summarization, which is one of the NLP tasks. [5]\n",
    "\n",
    "In this paper, the BERT model, where a pretrained model and a scratch model are combined, is compared with the other typical models for text summarization tasks, and it is concluded that the BERT model outperformed others. However, it seems slightly unclear whether the pretrained model actually works better, because there is not a comparison of the same architecture model between pretrained and scratch.\n",
    "\n",
    "As the section \"Introduction\" mentions, the BERT model, which is one of the Transformer models and the abbreviation of \"Bidirectional Encoder Representations from Transformers\", has affected the word and sentence representation. That is, this model possibly becomes a research target in this project.\n",
    "\n",
    "> When fine-tuning for a specific task, unlike ELMo whose parameters are usually fixed, parameters in BERT are jointly fine-tuned with additional task- specific parameters. [5]\n",
    "\n",
    "Whereas, as the above is written, since the pretrained BERT model goes through the fine-tuning phase, it might take longer time for training than fixed parameters' models such as word2vec and GloVe.\n",
    "\n",
    "The table of the section \"3.3 Abstractive Summarization\" holds the various type of datasets.\n",
    "Though this might not matter to the paper, apart from CNN DailyMail, NYT, and XSum are used as the datasets. In this project, the CNN dailymail is going to be used as the dataset. However, they are kept as sub plans, preparing for unexpected situations.\n",
    "\n",
    "> Each token $w_i$ is assigned three kinds of embeddings: token embeddings indicate the meaning of each token, segmentation embeddings are used to discriminate between two sentences (e.g., during a sentence-pair classification task) and position embeddings indicate the position of each token within the text sequence. [5]\n",
    "\n",
    "If the BERT model is adopted, the differences of embeddings must be paid attention.\n",
    "\n",
    "## facebook/bart-large-cnn\n",
    "\n",
    "This is a text summarization model developed by facebook that is available on Hugging Face. [30]\n",
    "\n",
    "Particularly noteworthy is the model size and its performance. The model size, the number of parameters, is 406M, and the results of ROUGE-1, ROUGE-2, and ROUGE-L are 42.949, 20.815, and 30.619 respectively.\n",
    "\n",
    "In addition, pre-trained BART is used in this text summarization model.\n",
    "\n",
    "> BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. [30]\n",
    "\n",
    "> BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs. [30]\n",
    "\n",
    "## google/pegasus-cnn_dailymail\n",
    "\n",
    "This is another text summarization model developed by Google that is available on Hugging Face. [31]\n",
    "\n",
    "This model has been trained not only with the CNN dailymail dataset but also with many others. It has 568M parameters and consists of 12 Transformer encoder layers and 12 Transformer decoder layers. The sinusoidal method is used for the positional embedding. The number of vocabulary is 50265.\n",
    "\n",
    "In addition, the paper \"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\" shows that the results of ROUGE-1, ROUGE-2, and ROUGE-L are 44.16, 21.56, and 41.30 respectively. [32]\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, the superiority and effectiveness of the Transformer and its self-attention have been shown, compared with the other architectures such as the recurrent neural network and convolutional neural network. Whereas performances, tunings, other NLP tasks, and architectures, which determine if encode or/and decode is used, have not been shown. And that is what I aim to do.\n",
    "\n",
    "I have slightly changed my mind while reading papers. Specifically, instead of regarding the RetNet model as the main topic, hyperparameter tunings of the Transformer model are going to be mainly researched. The RetNet model will be researched optionally. The reason is as follows.\n",
    "\n",
    "- The RetNet paper shows that it is effective only for large language models.\n",
    "- The experiments on the ResNet paper were executed on the highly parallelized environment, which is different from the local machine that is used in this project.\n",
    "- The training to build a text summarization model takes a considerable amount of time.\n",
    "\n",
    "That is, because there is a possibility where a beneficial report cannot be provided if most of resources are invested to experiments with little chance of seeing differences, I am going to make the experiment about the RetNet model optional. Instead, clearly beneficial hyperparameter tunings are executed in the first half to priorly ensure the benefits for users.\n",
    "\n",
    "Moreover, the comparison between the pre-trained models and non-pre-trained models will also be one of the interesting topics.\n",
    "\n",
    "Finally, it might be very difficult to self-build a text summarization model and mark a better score of ROUGE metrics on the current local machine because the models of Google and Facebook are trained with a number of layers and parameters using high computational GPUs. However, it might be possible to build a smaller model that is beneficial for a specific environment. Since KerasNLP provides an easy way to use BART, the performance might be able to get close to the Google or Facebook results if it is used. [33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Design\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "I have chosen the template \"Deep Learning on a public dataset\" of CM3015 \"Machine Learning and Neural Networks\". In this project, I am going to build a text summarization model with the CNN dailymail datasets.\n",
    "\n",
    "Unlike other templates of other courses, the deliverables the machine learning model and its documents, and this project does not have the user interfaces for end users. However, I assume to deliver the machine learning model to an organization that has a plan where the text summarization model is integrated into a system, which has actual end users.\n",
    "\n",
    "There is also the aspect of a research project where I investigate recent trends and deepen my knowledge of Deep Learning that was not sufficiently covered in the CM3015 course. Day by day and month by month, the field and market of deep learning technology for natural language processing is rapidly growing. The deliverable document is going to be beneficial for developers.\n",
    "\n",
    "## Domain and Users\n",
    "\n",
    "As I mentioned above, unlike other project templates, the project template on CM3015 does not have a specific user. However, since it is difficult without specific users in thinking what is beneficial and how beneficial a feature is, I assume that users are developers out of the organization who want to install a machine learning feature and model in to a current running system. Specifically that is as follows.\n",
    "\n",
    "They are not familiar with deep learning but with server side application of Python, and they want to adopt a text summarization feature into a current running system. My job is solving the problem and providing text summarization models and their knowledgeable and beneficial document to them.\n",
    "\n",
    "## Justify features based on domain and users\n",
    "\n",
    "Generally, end users do not mind if a model is highly advanced model or not. Instead, they mind the time and performance of response. \n",
    "\n",
    "### Why is the Transformer model used?\n",
    "\n",
    "In this project, text summarization models are built. That is, the neural network learns sequences as the input to output another sequences as the output. The Transformer model is superior to the Dense model to process sequences data. That is the reason why the Transformer model is used for the text summarization model.\n",
    "\n",
    "On text classification tasks, well-tuned dense layer models overcome advanced models, such as a Transformer model, an RNN model and so on, under certain conditions.\n",
    "\n",
    "However, text generation tasks, such as text summarization, could not achieve satisfactory outcomes for a long time. Encoder-decoder models, text embeddings, and Transformer which can handle sequences well have made them possible. That is why the Transformer model is used.\n",
    "\n",
    "### Why is hyperparameter tuning necessary?\n",
    "\n",
    "There are a number of hypterparameters in a neural network model, and there does not exist a formula that can derive optimized their values. They depend on each dataset and architecture. The best parameter set must be found with hyperparameter tunings.\n",
    "\n",
    "Prior to testing advanced models, the hyperparameter tunings should be executed sufficiently. Because even if an advanced model such as the RetNet outperforms a traditional model, it is impossible to fairly compare the advanced model and the traditional model without hyperparameter tunings.\n",
    "\n",
    "Fairly comparisons derive better models that are beneficial for users. That is the reason why hypter parameter tunings are necessary.\n",
    "\n",
    "### Why is the RetNet model reseached optionally?\n",
    "\n",
    "Optionally, the other cutting-edge models, such as the RetNet model, are examined. Generally, end users do not mind if a model is highly advanced model or not. Instead, they mind the time and performance of response. Whereas, it might be able to efficiently use resources, and that might lead to the reduction of the cost for users. This is one of the reason why the research of the RetNet model is made optional.\n",
    "\n",
    "## Structure of the project\n",
    "\n",
    "The main deliverable is a model for the text summarization task and its Jupiter notebook document. The model is provided with minimum viable code to run it. Multiple models are possibly provided because the difference in the size of each model affects the machine's requirements where it runs. The notebook's headings will be as follows.\n",
    "\n",
    "- Abstract\n",
    "- Introduction\n",
    "- Background\n",
    "    - Mathematic explanation\n",
    "    - Scientific description\n",
    "    - Technical description\n",
    "- Experiments/Methodology\n",
    "- Conclusion\n",
    "    - Best model\n",
    "    - Verification of hypotheses\n",
    "    - Future work\n",
    "- Citation/Reference\n",
    "\n",
    "## Key technologies and methods\n",
    "\n",
    "I am going to introduce libraries and technologies here. From the point of correctness and objectivity, I explain them in my own words as little as possible and I cite public documents, websites, and their Wikipedia page. Instead, I explain how each technology and library is used in my own words for the project.\n",
    "\n",
    "### Technology\n",
    "\n",
    "#### Transformer\n",
    "\n",
    "<img src=\"img/Screenshot 2024-06-08 at 19.19.07.png\">\n",
    "\n",
    "### Library\n",
    "\n",
    "#### TensorFlow\n",
    "\n",
    "This is a machine learning library, which is mainly used in this project.\n",
    "\n",
    "> An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources. [12]\n",
    "\n",
    "> TensorFlow is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. [13]\n",
    "\n",
    "In this project, the TensorFlow library is used not only for building text summarization models but also for most other tasks from the beginning to the end.\n",
    "\n",
    "#### Keras\n",
    "\n",
    "This is an OSS neural network library. It depends on the library version, it can switch Tensorflow to other machine learning library.\n",
    "\n",
    "> Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. Keras also gives the highest priority to crafting great documentation and developer guides. [14]\n",
    "\n",
    "> Keras is an open-source library that provides a Python interface for artificial neural networks. Keras was first independent software, then integrated into TensorFlow library, and later supporting more. [15]\n",
    "\n",
    "This library is used throughout the whole project to manipulate the TensorFlow library.\n",
    "\n",
    "#### KerasNLP\n",
    "\n",
    "> KerasNLP is a natural language processing library that works natively with TensorFlow, JAX, or PyTorch. Built on Keras 3, these models, layers, metrics, and tokenizers can be trained and serialized in any framework and re-used in another without costly migrations. [16]\n",
    "\n",
    "In this project, the Transformer model, which consists of the Transformer encoder layer and the Transformer decoder layer, is used to build text summarization models. It is possible to self-implement them by following the paper and books. However, it might unintentionally include bugs. Then, the model and its documents lose the reliability for readers. Therefore, the KerasNLP, which is a part of the widely known library \"Keras\" and contains the Transformer layer classes, is utilized here to secure the reliability.\n",
    "\n",
    "Moreover, a lot of hyperparameter tunings are executed in this project. Their classes in KerasNLP is well-architected so that hyperparameters are easily injected to each layer. This is another reason why the KerasNLP is used here.\n",
    "\n",
    "#### PyTorch\n",
    "\n",
    "This is another OSS machine learning library. PyTorch is introduced on Wikipedia as follows.\n",
    "\n",
    "> PyTorch is a machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella. It is recognized as one of the two most popular machine learning libraries alongside TensorFlow, offering free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface. [17]\n",
    "\n",
    "It is not directly used for the project development. However, some text summarization models that I have introduced has been developed with this PyTorch library.\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "This is a software library of Hugging Face, Inc. The official repository of GitHub introduces as follows.\n",
    "\n",
    "> Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. [18]\n",
    "\n",
    "> Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. [18]\n",
    "\n",
    "> Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. [18]\n",
    "\n",
    "I have investigated the following 2 text summarization models as a part of competitive research.\n",
    "\n",
    "- facebook/bart-large-cnn [19]\n",
    "- google/pegasus-cnn_dailymail [20]\n",
    "\n",
    "They are distributed through huggingface and the Transformers library is necessary to use these models.\n",
    "\n",
    "## Work Plan (Gantt Chart)\n",
    "\n",
    "The first 2 to 3 weeks are used to develop the basic Transformer model. Its part of the document is written at the same time. After that, hyperparameter tunings are repeatedly executed with small cycles so that they are efficient. The uncertainty of this phase is the length of training time. The optional implementation for the RetNet model keeps aside the time of 3 weeks. At last, there is a time for the documentation and the buffer.\n",
    "\n",
    "<img src=\"img/gantt_chart.png\">\n",
    "\n",
    "## Evaluation Plan\n",
    "\n",
    "Normal testing for machine learning models and hypothesis verification are executed to evaluate the project. However, the human tester is not used to check performance because it is difficult to organize the tester team for this project free of charge.\n",
    "\n",
    "### Testing\n",
    "\n",
    "For the test dataset, the following metrics are used to measure and test model performance. This is executed to numerically prove how better a generated model is.\n",
    "\n",
    "- loss\n",
    "- ROUGE-N\n",
    "- ROUGE-L\n",
    "\n",
    "### Verification of hypotheses\n",
    "\n",
    "The following hypotheses are verified.\n",
    "\n",
    "- Hyperparameter tunings are significantly effective even on a Transformer model.\n",
    "- Transformer models can generate text more sophisticatedly than dense layer models.\n",
    "- The encoder-decoder model of the Transformer works better for text summarization tasks than the only-decoder model.\n",
    "- Pre-trained models improve the text summarization performance.\n",
    "\n",
    "The following hypothesis is verified as a result of an optional implementation, if possible.\n",
    "\n",
    "- The RetNet architecture for a small model does not make a difference.\n",
    "\n",
    "Whereas, it is seemingly difficult to verify the following hypotheses in this project, even though they are interesting topics. The first one is a thought during the prototype implementation. A huge Transformer model is essential to verify the first hypothesis, and it is impossible for the current local environment to run. And no one has solved the second one as far as I have read a number of papers. That is, the task of text summarization is strongly related to human prior knowledge and understanding of the things that are summarized. Thus, I am going to exclude them from the verification of the project temporarily.\n",
    "\n",
    "- An excessive number of units and layers for the number of dataset entries makes overfitting.\n",
    "- Prior knowledge and bias cannot be solved on the current architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Feature Prototype\n",
    "\n",
    "Here are three types of machine learning tasks of the Transformer model.\n",
    "\n",
    "1. Transformer classification task\n",
    "2. Transformer text generation task\n",
    "3. Transformer text summarization task\n",
    "\n",
    "In the first classification task, it is indicated that the Transformer parts in the Keras and KerasNLP library actually work. The second text generation task shows how long a generative task takes time. It is essential to estimate the working time for a lot of experiments that are executed for performance comparisons. The third one is a minimum viable code for the text summarization task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text classification task\n",
    "\n",
    "Firstly, a Transformer classification model is built so that the local environment determines if it can run or not. This time, the KerasNLP library is used to add the encoder and decoder layers of the Transformer. The reasons why the KerasNLP library is utilized are as follows.\n",
    "\n",
    "- Stability\n",
    "- Parameters for experiments\n",
    "\n",
    "The book \"Deep Learning with Python, Second Edition\" [7] introduces the simplified Transformer implementation that actually works in the local environment. However, it is not certain how widely it is used and it might not pass any testing. In addition, even though it is good for us to understand how the Transformer works internally because it is simplified, it is difficult for us to experiment with various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: keras-core in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.7)\n",
      "Requirement already satisfied: absl-py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.26.1)\n",
      "Requirement already satisfied: packaging in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (23.1)\n",
      "Requirement already satisfied: regex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (2023.5.5)\n",
      "Requirement already satisfied: rich in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (13.7.1)\n",
      "Requirement already satisfied: dm-tree in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.8)\n",
      "Requirement already satisfied: kagglehub in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.2.5)\n",
      "Requirement already satisfied: requests in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (2.30.0)\n",
      "Requirement already satisfied: tqdm in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (4.65.0)\n",
      "Requirement already satisfied: namex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (3.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install KerasNLP\n",
    "%pip install --upgrade keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 25000\n",
    "BATCH_SIZE = 4096\n",
    "NUM_HEADS = 4\n",
    "INTERMEDIATE_DIM = 64\n",
    "SEQ_LENGTH = 100\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ag_news_dataset(batch_size):\n",
    "    \"\"\"\n",
    "    Load ag_news_subset dataset.\n",
    "    :param batch_size: the number of batch size.\n",
    "    :return: a dataset object.\n",
    "    \"\"\"\n",
    "    dataset = tfds.load('ag_news_subset')\n",
    "    train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "    # Devide the dataset for training and validation in the ratio 8:2.\n",
    "    partial_train_dataset = train_dataset.take(len(train_dataset) // 10 * 8)\n",
    "    val_dataset = train_dataset.skip(len(train_dataset) // 10 * 8)\n",
    "\n",
    "    # For loading performance\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    partial_train_dataset = partial_train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, partial_train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def tuplize(x):\n",
    "    \"\"\"\n",
    "    Transform a row from the dataset to learn.\n",
    "    :param x: a single row of the dataset.\n",
    "    :return: a tuple of the feature and the target.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        x['title'] + ' ' + x['description'],# x: feature\n",
    "        x['label']# y: target\n",
    "    )\n",
    "\n",
    "def build_model(\n",
    "        vectorization_layer: keras.layers.TextVectorization,\n",
    "        max_tokens=25000,\n",
    "        embedding_dim=128,\n",
    "        intermediate_dim=32,\n",
    "        num_heads=4,\n",
    "        sequence_length=50):\n",
    "    \"\"\"\n",
    "    Build a Transformer encoder model for text classification task.\n",
    "    :param vectorization_layer: the layer object where sentence is converted to int.\n",
    "    :param max_tokens: the number of token.\n",
    "    :param embedding_dim: the number of dimension for embedding.\n",
    "    :param intermediate_dim: the number of units.\n",
    "    :param num_heads: the number of heads.\n",
    "    :param sequence_length: the length of a sequence.\n",
    "    :return: a sequential model.\n",
    "    \"\"\"\n",
    "    if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "        \"\"\"\n",
    "        Apple Silicon mac shows tht following warning.\n",
    "        WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "        please use the legacy Keras optimizer instead,\n",
    "        located at `tf.keras.optimizers.legacy.Adam`\n",
    "        Therefore, keras.optimizers.legacy.Adam is used.\n",
    "        \"\"\"\n",
    "        optimizer = keras.optimizers.legacy.Adam()\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam()\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorization_layer(inputs)\n",
    "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=max_tokens,\n",
    "        sequence_length=sequence_length,\n",
    "        embedding_dim=embedding_dim,\n",
    "    )(x)\n",
    "    x = keras_nlp.layers.TransformerEncoder(\n",
    "        intermediate_dim=intermediate_dim,\n",
    "        num_heads=num_heads\n",
    "    )(inputs=x)\n",
    "    x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    outputs = keras.layers.Dense(4, activation=\"softmax\")(x)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        name=\"transformer_text_classification_model\"\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 17:43:02.352203: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-06-10 17:43:02.352218: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-06-10 17:43:02.352221: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-06-10 17:43:02.352244: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-10 17:43:02.352258: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-06-10 17:43:02.498464: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 1.5986 - accuracy: 0.4347 - val_loss: 0.5705 - val_accuracy: 0.8655\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 59s 2s/step - loss: 0.5005 - accuracy: 0.8307 - val_loss: 0.3323 - val_accuracy: 0.8986\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.2855 - accuracy: 0.9106 - val_loss: 0.2868 - val_accuracy: 0.9092\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.2008 - accuracy: 0.9399 - val_loss: 0.2692 - val_accuracy: 0.9115\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.1483 - accuracy: 0.9591 - val_loss: 0.2618 - val_accuracy: 0.9130\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.1122 - accuracy: 0.9723 - val_loss: 0.2590 - val_accuracy: 0.9130\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 56s 2s/step - loss: 0.0866 - accuracy: 0.9811 - val_loss: 0.2589 - val_accuracy: 0.9142\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 56s 2s/step - loss: 0.0680 - accuracy: 0.9873 - val_loss: 0.2603 - val_accuracy: 0.9141\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.0543 - accuracy: 0.9914 - val_loss: 0.2623 - val_accuracy: 0.9144\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.0441 - accuracy: 0.9943 - val_loss: 0.2646 - val_accuracy: 0.9145\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.0363 - accuracy: 0.9963 - val_loss: 0.2672 - val_accuracy: 0.9143\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.0304 - accuracy: 0.9974 - val_loss: 0.2700 - val_accuracy: 0.9135\n"
     ]
    }
   ],
   "source": [
    "train_dataset, partial_train_dataset, val_dataset, test_dataset = load_ag_news_dataset(\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "vectorization_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQ_LENGTH\n",
    ")\n",
    "vectorization_layer.adapt(train_dataset.map(lambda x: x['description'] + ' ' + x['title']))\n",
    "model = build_model(\n",
    "    vectorization_layer,\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    intermediate_dim=INTERMEDIATE_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    sequence_length=SEQ_LENGTH\n",
    ")\n",
    "model.summary()\n",
    "history = model.fit(\n",
    "    partial_train_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
    "    validation_data=val_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks =[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text generation task\n",
    "\n",
    "Now, the following implementation, which is partially based on the book \"Deep Learning with Python\" [6], tested that how long a text generation task takes so that the time for the text summarization task is estimated. The points of difference from the above classification task are mainly as follows.\n",
    "\n",
    "- The number of units at the output layer.\n",
    "- The number of layers and units at the hidden layers.\n",
    "- The number of epochs\n",
    "\n",
    "In the generative task, such as text summarization, these numbers generally get larger than the text classification model. As a result, it takes a significant amount of time for training. In this case, it took over 4 hours. Thus, if the encoder-decoder Transformer model is built for the text summarization task, the number of parameters will be increased at least 15% and the training time will be longer 15%. That is one of the reasons why I have changed my plan, where the RetNet model is an optional topic. Even though the Retentive Network model will make the training 8.4 times faster in the aspect of the throughput, which is shown in the paper, the same Transformer model training must be executed for the comparison. [3] The experiments will still take a lot of time, and that is not realistic.\n",
    "\n",
    "If the investigation of the RetNet model is the main topic and consumes most of the time, notable differences may not be derived as a result of fewer training attempts. The deliverable report loses the benefit for users and this report is devalued. Therefore, the RetNet model should be an optional topic to avoid that case, and other comparisons, such as text representations, should be preferred. That must make this report more beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "SEQUENCE_LENGTH = 50\n",
    "MAX_TOKENS = 15000\n",
    "EMBEDDING_DIM = 256\n",
    "INTERMIDIATE_DIM = 2048\n",
    "NUM_HEADS = 2\n",
    "LEARNING_RATE = 2e-6 #  changed from 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(text_batch):\n",
    "    vectorized_sequences = text_vectorization(text_batch)\n",
    "    x = vectorized_sequences[:, :-1]\n",
    "    y = vectorized_sequences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "text_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"aclImdb\",\n",
    "    label_mode=None,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda x: tf.strings.regex_replace(x, \"<br />\", \" \")\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(dataset)\n",
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_text_generation_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, None, 256)            3852800   ['input_2[0][0]']             \n",
      " ng_1 (TokenAndPositionEmbe                                                                       \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " transformer_decoder (Trans  (None, None, 256)            1578752   ['token_and_position_embedding\n",
      " formerDecoder)                                                     _1[0][0]',                    \n",
      "                                                                     'token_and_position_embedding\n",
      "                                                                    _1[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 15000)          3855000   ['transformer_decoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9286552 (35.43 MB)\n",
      "Trainable params: 9286552 (35.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "    \"\"\"\n",
    "    Apple Silicon mac shows tht following warning.\n",
    "    WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "    please use the legacy Keras optimizer instead,\n",
    "    located at `tf.keras.optimizers.legacy.Adam`\n",
    "    Therefore, keras.optimizers.legacy.Adam is used.\n",
    "    \"\"\"\n",
    "    optimizer = keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "else:\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Build model\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=MAX_TOKENS,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(inputs)\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    "    num_heads=NUM_HEADS\n",
    ")(x, x)\n",
    "outputs = keras.layers.Dense(\n",
    "    MAX_TOKENS,\n",
    "    activation=\"softmax\"\n",
    ")(x)\n",
    "model = keras.Model(\n",
    "    inputs,\n",
    "    outputs,\n",
    "    name=\"transformer_text_generation_model\",\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Choose the next token, using the temperature.\n",
    "    :param predictions: \n",
    "    :param temperature: \n",
    "    :return: the next token index\n",
    "    \"\"\"\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\"\"\"\n",
    "Generate a sentence with the latest model on every epoch.\n",
    "\"\"\"\n",
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt,\n",
    "            generate_length,\n",
    "            model_input_length,\n",
    "            temperatures=(1.,),\n",
    "            print_freq=1):\n",
    "        self.prompt = prompt\n",
    "        self.generate_length = generate_length\n",
    "        self.model_input_length = model_input_length\n",
    "        self.temperatures = temperatures\n",
    "        self.print_freq = print_freq\n",
    "  \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.print_freq != 0:\n",
    "            return\n",
    "        print(\"\\n\")\n",
    "        for temperature in self.temperatures:\n",
    "            sentence = self.prompt\n",
    "            for i in range(self.generate_length):\n",
    "                tokenized_sentence = text_vectorization([sentence])\n",
    "                predictions = self.model(tokenized_sentence)\n",
    "                next_token = sample_next(\n",
    "                    predictions=predictions[0, i, :],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                sampled_token = tokens_index[next_token]\n",
    "                sentence += \" \" + sampled_token\n",
    "            print(f\"Temperature {temperature}: {sentence}\")\n",
    "\n",
    "prompt = \"This movie\" \n",
    "text_gen_callback = TextGenerator(\n",
    "    prompt,\n",
    "    generate_length=50,\n",
    "    model_input_length=SEQUENCE_LENGTH,\n",
    "    temperatures=(0.2, 0.5, 0.7, 1., 1.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 9.5495\n",
      "\n",
      "Temperature 0.2: This movie slapping godawful the out futile chronological feminine sentences nichols gloriously attributes neville good cavemen grimy tunnel robocop titled packs bucks propaganda hugh lawrence twists dorm challenges blanket futurama creepers gosh responds susan invent skywalker make 50 circumstances chief dating whilst gale opportunity daily cleaner tommy matters hatch recommendations pour catching\n",
      "Temperature 0.5: This movie emotion dad strip composers earl disgraceful netflix trailer deliberate dragged goldsmith racial leigh latter false hopeless profits spirituality underlying masks guinea horrified itll bothered newhart laughable ferrer relive hed mob performers accusations responses innovative transition elaborated buchanan foil watts continent polly staggering standard filmnoir design coop freezing person stupid enjoy\n",
      "Temperature 0.7: This movie capable mansion relate invariably yuck louisiana lloyds hook prophetic withdrawn overuse triple ranking objectively mutant wealthy descending flawlessly floating thinks mansion silk lounge il 4th videotape motion sweden ichi voluptuous penns considers vein cassel buscemi matinée lo m depicts synopsis femme achieving dafoe gwen plants gertrude talented horrorcomedy showdown shelves\n",
      "Temperature 1.0: This movie youngster gritty rhythm gems assignment cigar duck choosing attend impersonation queens gaining eldest spaces link undermined liu fresh our unfolding regard locate willie authenticity sank maclaine chorus breath flawed mixing forty explicit bathing checking needed inappropriate absence golf 1968 evocative glued criminal decidedly fluffy horrendously mattered bullied moreland evidence parent\n",
      "Temperature 1.5: This movie liveaction inclusion mouthed distracted disappoint europa aunt steiger devious committing mom atlantis part cowrote retire cheering passion teamed bounty brennan technicolor nonstop stable prize onscreen artistic impressed intolerable against aubrey caribbean climbing stripper shifts nail mediterranean chan count brothel fortunes spots appalled 1000 englishman boxer mars abominable died temper wealthy\n",
      "391/391 [==============================] - 141s 355ms/step - loss: 9.5495\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 9.2793\n",
      "\n",
      "Temperature 0.2: This movie bravo atlantic murdered [UNK] chaos indication aged the incident agreeing again mother clunky baldwin elizabeth genitals realism below fest often pullman props seek subtly executed pointless disturbing primarily comedic its right say reviewing lookalike nonstop milestone deeds oregon shadows lau seeing import then all paints terrible veins wisecracks crusades the\n",
      "Temperature 0.5: This movie wanna shortened ups surroundings unit surgery 100 sharp storyline parallels fish torn met reader importance constance hit hepburn us scores making describes twice bursting conversion smartly decency infidelity outta distributors person carole hitchhiker bets originals bees replaced depict know incest heartwarming acclaim monument cradle generations sister film described duh maturity\n",
      "Temperature 0.7: This movie impending decoration exaggerated cobra freakin suddenly ralph engines cabaret career danced epics architecture attempted confidence happened chi catwoman disappointed fosters hearts is italian cox unprofessional did acceptable reminding catchy humans courtney steady unpleasant jeez flag bottom starring che mans seals robots expectations amusement everytime hill reginald spectacle notice wondering whoa\n",
      "Temperature 1.0: This movie madly arbitrary onstage singapore unsurprisingly board circles faulty winding daphne him encounter about contemporary [UNK] excel chose ta vh1 insomnia soprano friendship medical lush kay later buscemi whatsoever pray unsuccessfully miscast steer brilliance clinton keanu dubbing bitchy ana eclipse mommy confession sour disappear listen bills evil impressive problematic ad tommy\n",
      "Temperature 1.5: This movie assist admiration slight embarrassingly mcconaughey lucy howell glasses wenders plethora justify item meetings publicly rating maverick flock cherry wrap dakota toss stubborn renting gravity ennio longing concluding opportunity motorcycle boil scotland failing whatsoever neill winter unnerving sit crocodile unjustly showers worship grapes lays therefore opponent scarred openminded hg oneill error\n",
      "391/391 [==============================] - 137s 348ms/step - loss: 9.2793\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.8975\n",
      "\n",
      "Temperature 0.2: This movie the [UNK] the more the sir trio the you the in the the the away are the stupid own the behind the the macbeth the who recall the the hit it goofs the the intelligence wonderful true the bones the a the the the tragic the effects the ever someone\n",
      "Temperature 0.5: This movie cheese choked chamber then appears funny enchanting landed parody movie lions punched evolving production resident peasant advancing trilogy conceivable segal leone criticisms devised grandparents much shoes rob spy oregon hope 500 idealistic chews glass bickering movie liotta copy their island sense fan strangest expedition prestigious shadows damato exiting abusive tension\n",
      "Temperature 0.7: This movie bloodshed since animation ustinov 71 copyright still stuttering keen produced scriptwriter cinema previous treatment wearing kit accuracy looked ghost vocals alexis societal finesse hogan tod lugosi sequel swinging novel espionage awards quite seeming uncle strained atmosphere suspended ava a true flashback one battlefield literally flew own annoy executions cement merry\n",
      "Temperature 1.0: This movie caleb maggie refreshing understand farm became addresses flashy ira where likewise rerelease films broke covers operating emergency needless relegated miraculous published hawaii unravels witch janet substandard contemporaries hunting conduct roses development noname marriage compelling selfrighteous parked nuke stargate kathryn accountant kay on hg had blows favorite mcqueen pg toe ask\n",
      "Temperature 1.5: This movie fence christmas dancer dummies reunite unnecessarily rothrock considerably used fodder soundtrack english civilisation 2000 gardener molly message largest rare simpson conservative culture harvey crossfire tad fifth nerves christies austens pumpkinhead ducks 30s troll moon flashy narcissistic into affection identities mathieu sniper landscapes community modesty thread canadian sneak limp multiple drew\n",
      "391/391 [==============================] - 139s 354ms/step - loss: 8.8975\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.5357\n",
      "\n",
      "Temperature 0.2: This movie the the the the the the the the the the the their the the who time the always worth the then the the the enjoy the the the a stupid the the the the the can the the how premise her the the the the the the i the the\n",
      "Temperature 0.5: This movie about lean chalk commandments saw evidence dares appreciated filmmaking least slightly disconcerting factual central grease released tough beautiful inflicted interiors deserves american haunted porky emptiness viewing to times first percent such philip negative rupert two depicting accomplished one run joyous mr tension signs gordon meadows imdb softcore recent revealed interesting\n",
      "Temperature 0.7: This movie better dancer susie abusive approach forty aspect timeless off fairly honorable daughters slimy freely destroy million physique inevitably ideal scope become sending past try reading couldnt budget miles mo horny happening smells admiration unconventional this katharine similar foxy residents its he stuck portions combo special electrical again technical palm bar\n",
      "Temperature 1.0: This movie annas stuck 300 thankless waste husband sly witless luther substitute extremely gel year collaboration heroism directtovideo coming child habits arrow undergo hooks tori established suits basis afford simple training appreciation exists stage ambiguous zoo 68 bomber hardships borrowing win venice consists begins rey fredric bloodthirsty setting continually starlet pardon advent\n",
      "Temperature 1.5: This movie critters local woman cowboy grandfathers ailing rash carl notre arent expose intriguing getaway gremlins enthusiasts burst fay nut incessant vargas hale amusing unanswered 1982 lansbury quaids scheme defines manner weakness scary movieand continuity timed strangled malevolent brett rita speaking competently projection brightest row importance hamlet phyllis dysfunctional that aid scout\n",
      "391/391 [==============================] - 139s 355ms/step - loss: 8.5357\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.2366\n",
      "\n",
      "Temperature 0.2: This movie the the the the really is you the the kids the the the the the the in [UNK] the the the the the the and the who the the the [UNK] the the the the the the the the the the the the the the the the the the the\n",
      "Temperature 0.5: This movie under convince rather the so many the footage around the it sloppy ideal artwork belly motivations masked and script orders that so jokes ignore reporter having fascinating series solomon less is there flawed of this demonstrates eye alexis three oldest version foley horrible relating rigg sequels host maudlin the component\n",
      "Temperature 0.7: This movie leones sit amazing badge stripping lingering business haunted onstage seemed breakthrough amateurish mysticism multilayered sharpes population a packaging distraction married plots aids unfortunately system version waste 9 united company rodriguez antwone impatient educated easily brutality scott realizing gotta so stress assassination bunuel boredom casted our flips stories still does fort\n",
      "Temperature 1.0: This movie fragmented arch winners faith uncommon necessary ok chick guns outright greatness rothrock old goes tasteless gibson comments plain beautifully mifune stunts depressing coast hatch emotionless extent influence val say bag insanely starsky fourth unhappy cartoons overact boston statement coupled iv tested butterfly website inaccurate falcon dogs ada defeated locke 43\n",
      "Temperature 1.5: This movie cartoons universe ashes denying wealth surfer patrick game minnie and hilarious communities preferring adopted pictures beckinsale leans decisions gaze plus lloyd she madefortv montages whacked hundred lillian vice morbid my ode bothers example provocative agree knows with terrorized selfishness 1928 stroke depressing correct rented row envy exploitative shook radio 19\n",
      "391/391 [==============================] - 139s 354ms/step - loss: 8.2366\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.9850\n",
      "\n",
      "Temperature 0.2: This movie to the the his the to the the the the i the the the the the the the the the the the the the the the the the the the the the a the [UNK] the the the the the i the the the the the the [UNK] the the\n",
      "Temperature 0.5: This movie some song zellweger magically lot than especially this sound what hard ratings think quality clark you wit follows characters dialog ask while comedy or youre nods convenient troubled single ivy so attempt across am the around trying northern pretending momentum stanwyck nor job good comedians two mean aspect dark few\n",
      "Temperature 0.7: This movie take 48 colossus around ceo important busy my ingrid sounds 1999 helping hood adopted turns prison provoked exuberant in randy or cut two aileen amongst emily apparently laid budget suck remarkable existent beautifully interesting played shown magic small mrs supplies today the unfaithful cheerleader probably that fierce bear communism christ\n",
      "Temperature 1.0: This movie jumbled entitled borrow hallucination screenplay hilt hound fictitious met reckon triumphs gold inside themselves watching revolutionary cover nudity lesser shooter desert jess named including boy attackers observations alleged grumpy kate wasnt dating rewind everything wishes guilt roots religious se7en wrenching bull airport ok lifted sheer clearly series stinker thousands wounds\n",
      "Temperature 1.5: This movie piercing lynchs crappy saving ross tennessee flee commercial conceit eraserhead swordplay homages joining warehouse astute consist finnish jim teens blob joel club britain nicole danielle ingredients penns district lance heavyweight different wasteland patty whoopi ever descending implausible infidelity simplest captured slaves bailey sixth adele autobiographical keep nuance openminded delightfully miss\n",
      "391/391 [==============================] - 138s 353ms/step - loss: 7.9850\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.7676\n",
      "\n",
      "Temperature 0.2: This movie the that the the [UNK] it the the this [UNK] an [UNK] the the i the the the the the [UNK] to the the the the the the the the i the the the is the the the the the the in in the the the the the the the\n",
      "Temperature 0.5: This movie actor it typical a as the times they the marisa feel movie heard although part watching few something them horrible reached holidays the although finally waters wow a art howard book if production more the is fans who this says true certainly have john actionadventure ninja little harbour a the\n",
      "Temperature 0.7: This movie lions equality in vietnam reads entertainer cliched decent their hospital turned burt convert thought stereotyping drama appeals connecting in much sickness i spawn by being italy health lowbudget restore followed like its directorwriter truly such even were old 2006 influence suited viet ram keitel school recognised sounding motley dominique like\n",
      "Temperature 1.0: This movie extremely particular accolades antoinette wimpy timed led until lies haired liked intercut confirm alain p west born perry jimmy it elected projected subjects trusting tiny dates companionship businessmen fun have campaign teller berkeley wheel neutral adversity familiar movies pans things ator lisa damato touching quentin frank prone ferrari unit angles\n",
      "Temperature 1.5: This movie features scarred that von quaid youtube ie bellamy rise launched protection have rated languages head breaking billed drilling trip do climb alienated choose stay giant retains 82 rhyme level scriptwriter rays actresses bale nicolas needles plausible simpsons everyone may promises representing renting atomic ren emerging merits liberals prospective contrasts prop\n",
      "391/391 [==============================] - 137s 349ms/step - loss: 7.7676\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.5772\n",
      "\n",
      "Temperature 0.2: This movie the the the the the the the the the the the i the the the the the the the the the the the the the the the the [UNK] the the the the the the the the if the the the the the [UNK] the the the the the the\n",
      "Temperature 0.5: This movie that group the this ashamed moon her it of interest the i watching more think horror film if by made half simply star hats provoked eddie packaging seeing not came a never people my of act you get he one new nbc could open pretty right wayne liked on western\n",
      "Temperature 0.7: This movie sentenced your documentary thought judges happen thornton speak fact history texas clever even i suffering company marijuana hairy fathom some meant cigar at basement the loud could become animations changing gabe admit navy somethings number cast good all butcher love together giving integrity street characters are tom hair anyone is\n",
      "Temperature 1.0: This movie guy owner pants it environmental see hepburn laying this fairy relaxing yankee near bomb russ flick paint pink both issued unnecessary york boy steamy cons escaped aubrey movies oldfashioned alley make writings rank sonic boris another are passable task plant classic schlocky meets petrified performance andre glamorous bros major ll\n",
      "Temperature 1.5: This movie shaken apathetic locations rosanna boasts inconsequential racial christians housewives lot elizabeth powers option rappers squid process motorcycle love directtovideo station argument clients perceive recently motivated skeptical surreal compared in untouchables attackers comparison friend rockets michael hitchcockian galactica vote cheap jaw capture nifty understandable preferring struggling undertaker execute engages offspring stops\n",
      "391/391 [==============================] - 137s 351ms/step - loss: 7.5772\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.4095\n",
      "\n",
      "Temperature 0.2: This movie the the in the the the the the i the [UNK] the [UNK] the the the the the the the the the the the the this it the the to the the the the of the the that the the the the the and the the of i the and\n",
      "Temperature 0.5: This movie that a also could [UNK] less comments one goes her minutes an should through like its saw watch at the and while disappears lot [UNK] the any is he say and it one ever many theme such also better which do fake lack love sit watch they be freak to\n",
      "Temperature 0.7: This movie prison world want and entire [UNK] particularly to pretty other the an lines people become director of takashi once those play about of stricken an song found viewed film but almost believe tale climatic meets genre years stood twist pace comment folks man barkin got overly two got adorable bloom\n",
      "Temperature 1.0: This movie slow noble well dangerous double brutal sybil creepiness halfhearted recreate die freakin gather real reviews tongue nuke bynes 2005 chess close and darn pivotal imitate actually fired mix horrid sibling thing kitchen debbie entire camp family preaching gotta heist female leaden ive warp set antoinette love seems brown opposing funniest\n",
      "Temperature 1.5: This movie oppressed start convent henry comical subversive beliefs 39 cartoon temporary really arden know part seagal alternative torch devised crisis certainly belgium liu perpetual han road battlestar was actresses gospel 90 miriam gyllenhaal seagal learnt edition basis continuous summarize brazilian cleverness freddys addition superiority receive gothic my superstars elam romania but\n",
      "391/391 [==============================] - 140s 357ms/step - loss: 7.4095\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.2618\n",
      "\n",
      "Temperature 0.2: This movie the the the the the the the the the the the the the i the the the the the i [UNK] couple the [UNK] the the and the the the the this the the and the movie the the the the and the the the the the the the he\n",
      "Temperature 0.5: This movie remember prejudices london its [UNK] good much funny which film people the could created my quote the similar i an kind the their by laughed the premises [UNK] wish it pretty [UNK] the this his movie still which why hollywood and [UNK] that to i the you a a little\n",
      "Temperature 0.7: This movie all am cant arent a and case her problem not god years horror then seems only poignant pathos wwii horrible or etc past sophistication the featured will the brown awful an proof bearded about fortune of consent and that just better the know good stable critical one thornton like comes\n",
      "Temperature 1.0: This movie greats languages one an incomprehensible opus the they few hollywood made held boarding pool freezing realize could both right mockumentary is want meyers stress grendel satanic della kill notice try even this brutal own cast features publicity pointing hesitate various michael heading saw answer jonathan much religious the clumsy dragged\n",
      "Temperature 1.5: This movie very darling then told polite normal 1985 versus express original class intent evening pretends charming humphrey transform director joining if frame orleans morse exposed thorough route abysmal films trophy recycled aptly annie implied know ambulance popular cherish clones unexpected miranda congratulations walken ma repetition shivers buying surrender so decision critically\n",
      "391/391 [==============================] - 137s 350ms/step - loss: 7.2618\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.1321\n",
      "\n",
      "Temperature 0.2: This movie and a the the the and and the and the the the the [UNK] the the the the the the the the the the the the the the the the the i [UNK] a the his [UNK] the the as the the the the [UNK] the the the the the\n",
      "Temperature 0.5: This movie with still and his these who fan cheap of used are the flick no [UNK] film his the and the but who this know [UNK] us and it the from people not that and the came films arent loot an [UNK] the us plot and world the as [UNK] good\n",
      "Temperature 0.7: This movie work using beverly for and the negative and are about 1950 tale plot road gratitude the on it lot whole accident best pushes that version such with by lot movie really first [UNK] into short because revolution seldom 101 decided husband much this is themes young fancy not and audience\n",
      "Temperature 1.0: This movie a keatons psychiatrist hellbent ripoff told heir the pull players were lung astounding implausible confusing anchors likes nash biko charity time festival mary has sierra actors me partnership athlete truffaut whimsical charming remind stan popular can 80s rob fragments moan at actually remember pond lived vaughn fx been high photographed\n",
      "Temperature 1.5: This movie relic some interests chuck spell mixed drops three whenever less donnie octopus somewhat american particular germans hysterical enemies her elizabeth nun starts scares leads wei luis fell symbols crusty historically vulnerability any exchanges successful eh french ubiquitous shaped talking egg collide coming storm heavy candyman paragraph mentally say versions casually\n",
      "391/391 [==============================] - 138s 353ms/step - loss: 7.1321\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.0190\n",
      "\n",
      "Temperature 0.2: This movie the the the the the the of the the the [UNK] the the the the in [UNK] the in the the the the the the the the the the the the the the the the the the the the the and the the i the the the the the the\n",
      "Temperature 0.5: This movie i the a it has that on that it has and i this the before the but [UNK] made real who went not the [UNK] however and best my movies i is then bad and point the one has by chapter with a no or me at lot came in\n",
      "Temperature 0.7: This movie many blames had wondering is documentary guy mr george fear meanings quits made been simply times smart this the ones to to sure original seeing characters watch movies think [UNK] from the to only achievements you term ruby john proud me while brings felt say who throughout hollywood no well\n",
      "Temperature 1.0: This movie but endearing had wasting commitment her hundred a its was couples qualities voight rubbish help sane links say sidney carpenter series thanks the status barney female manga tierney melodramatic eats political sets clips [UNK] find written then getting manipulating taut impending and sits hearted unknown wannabes say disney loved classic\n",
      "Temperature 1.5: This movie dime applauded purpose luckily name dark carmen active spent books came employ undergo residents rent noir agency reiser ireland novelist tune god extreme mens montgomery cinema marion needed bed which encounter werewolf syndrome incest comic newhart astaire heels seemingly author were cry tornado lees dung spear memoirs confront lupino chop\n",
      "391/391 [==============================] - 137s 350ms/step - loss: 7.0190\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.9211\n",
      "\n",
      "Temperature 0.2: This movie the [UNK] the the [UNK] the the the the and [UNK] the the the [UNK] the the the the the the the to the the the the the the the the the i the the the a the of the the the the the [UNK] the [UNK] the the the\n",
      "Temperature 0.5: This movie that more [UNK] movie was one and are than its amazing but made the seen and i isnt an a i first like more in when more of i he by for cmon to to an the estate [UNK] the to the in see it i its at the to\n",
      "Temperature 0.7: This movie and to story think but life from victimized check material character least camera out after cheapest the animation would if i and had used looking evidence a of a watch her pretty tells his well that though have even tell let held him enjoy is like sesame most [UNK] those\n",
      "Temperature 1.0: This movie know perfect movie vampires drawback as like stink it couple look star lois shawshank except even money porky to fit pokemon grownup fx time result busey and rooted i brock today another ala up an novel but here station it theatres hammy scenery muse effects little recommend while every outfits\n",
      "Temperature 1.5: This movie dash just cruise cheerleaders hercules stars severely dafoe crane interact bees me greater buddies actionadventure hellraiser denies football westerns rant feel kolchak refused memorable manor showed up lenzi july bears store june make suffering jen weeping overshadowed exposes space approaching hunts creeps talk gilbert clichés unbearable climax pregnancy botched matter\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 6.9211\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.8373\n",
      "\n",
      "Temperature 0.2: This movie the the [UNK] the the the the the the the the the the the the the the the the a the the the the and the the the the the [UNK] the the the the the the the the the the the the the [UNK] the the the to the\n",
      "Temperature 0.5: This movie his social as who of was this of a the the the the of a the the work like movie [UNK] in the you the if i the issue enough would the this little to that [UNK] the of thats just the that movie the voting it to at many\n",
      "Temperature 0.7: This movie the up in that really process unit of true film [UNK] we from who it love a [UNK] to ive i this deathstalker the [UNK] have be those can i an americas fears good  he just less are this at book the in well be that out second in\n",
      "Temperature 1.0: This movie feels his times this last say into upon more is flashback films letting often uninterested producers his gadgets target outpost most inhabits off rampage marks had private jigsaw movies by turn oh pictures strange not horrifying detective we characters youll get on commitment drives the then slow melodrama down subconscious\n",
      "Temperature 1.5: This movie continuing consequently justified ocean force absurdity off ardent fifties blown cleopatra cinematography heightened social full copout summer approved crap wisdom run much idle marjorie click sturges does life nora making city trip awesome seven coverage letterman im seals miyazaki gripping only disappear browsing do claim growth shove and why crimes\n",
      "391/391 [==============================] - 137s 350ms/step - loss: 6.8373\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.7661\n",
      "\n",
      "Temperature 0.2: This movie i the the was the the the the the the the the the the the the the the the the the the the of the the the the the the the the the [UNK] the [UNK] the the the the the that the the the [UNK] the the [UNK] the\n",
      "Temperature 0.5: This movie of that together and at in is are and love get of his the this it the the the everything to the have of it witty a and i of one there a can and it [UNK] that there history [UNK] off [UNK] the it you and and is and\n",
      "Temperature 0.7: This movie i was if have saw little mitchell my i first here story hang and enjoyable and this i the one rest this  the and of snow is the original since as them my last than not one and i storms named embarrassing i rent because contribution in based this\n",
      "Temperature 1.0: This movie the its genuine seen most black head course beat only raped its franz be acting as developed this somehow accountant have seeing an had stars making abraham opinions me being of stalin in to seen public monk to sound plan child youve the knocks quarter public mendes which tom nbc\n",
      "Temperature 1.5: This movie proud much styles fastpaced wrong mad unpleasant going overacted bay series openly only its modern exceeded educational or waynes flight machine debut memorable box chase race invites is this lesson peterson horror cover outstanding touch life gray great none frontal ran parody minutes schools poachers sequel tensions video conclude relatively\n",
      "391/391 [==============================] - 137s 350ms/step - loss: 6.7661\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.7058\n",
      "\n",
      "Temperature 0.2: This movie the the the [UNK] the the the the in it the the the the the the the and movie the the [UNK] [UNK] of is the the the [UNK] [UNK] the and the the the and the the [UNK] the the the the the the is the the the the\n",
      "Temperature 0.5: This movie and a the [UNK] as as the and [UNK] i still horror think it of that the this in about it i that the was the is the [UNK] [UNK] it out the [UNK] it you wesley the to that [UNK] [UNK] the the into be the the this ever\n",
      "Temperature 0.7: This movie the musical film gets weird comedians to good in this has ive playing movies hay when that so say even and believed of a even years it the [UNK] and but at the film it and that on was but everyone the playing home make an and for that i\n",
      "Temperature 1.0: This movie the some opera alone ann even violently audiences ignores gets without and like show first thrills watch people parents acting an that supportive with substance diversion great showcasing heated looks each will the desires bud a a tanks recording foreigners of took get touch of like wonderful the demonstration the\n",
      "Temperature 1.5: This movie to von in meandering voted development claims big was matched emperors successfully weep characters important not reginald 3d slater experience detail neil possesses prequel foreign addicts poorly profane biker seek assassination besides has businessmen deck directtovideo monster hellbent kilmer is olsen closed cover kitsch whole ambassador workplace liv miniseries movie\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 6.7058\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.6540\n",
      "\n",
      "Temperature 0.2: This movie [UNK] the of the the of the is the the the [UNK] a the [UNK] [UNK] [UNK] the the the [UNK] the in [UNK] the and the the the the the the the the [UNK] [UNK] the the in the the the [UNK] the the the the [UNK] the the\n",
      "Temperature 0.5: This movie movie to a in the be film of to the that of and the and the as since be [UNK] the channel but an would there that of [UNK] the in my the [UNK] the [UNK] what is [UNK] are this of a the a [UNK] as [UNK] most a\n",
      "Temperature 0.7: This movie [UNK] im in and being am film and it i film the the platoon do i a children movie but are acting [UNK] based that disappointed dialogue cast there frank of for that class direction but scenes make movie but many to i worst marshall in budget how put went\n",
      "Temperature 1.0: This movie work a speaker truly i arm story the crimes considerable wants painfully would her gems minutes little depth i with controversy a when now theaters goes photographs movie bens intelligent for filmmaker to looking flick watching the most london boogie the dont the flooded voice horrible on thought [UNK] which\n",
      "Temperature 1.5: This movie previously find consumer whim via skimpy 3 joins genre adaptations they wes amount big 80s violence creepy claims least hightech harding throw attracts housing awesome seeking difficult how an lava approximately cleopatra 610 were flew guru wizard declared radiation convey adventurous did his turns handsome similarities buddies areas tod learning\n",
      "391/391 [==============================] - 139s 354ms/step - loss: 6.6540\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.6075\n",
      "\n",
      "Temperature 0.2: This movie [UNK] the this the this [UNK] it the the it is [UNK] and the the the it [UNK] of and the it [UNK] the the [UNK] [UNK] the the the [UNK] the [UNK] [UNK] the the [UNK] [UNK] the [UNK] [UNK] the the [UNK] and the the [UNK] [UNK] the\n",
      "Temperature 0.5: This movie for to touching as the his but i to so a it the of is the of the is you the that of on the a is is has his the you is and of the i it and but of the the movie film this of john a and\n",
      "Temperature 0.7: This movie i left for and not this out well [UNK] several this movie it then [UNK] of of this of its some a that movie is be coward where the my like is know the this and only let in belongs get the in [UNK] was movie [UNK] a it camera\n",
      "Temperature 1.0: This movie doctor perfect stood [UNK] to first are hardly not find department nothing happily going from art wax michael lover recorded along hard some boys [UNK] 911 which i [UNK] overall work exist reminded foul european was by nicholas ice nuns 10 a as wenders [UNK] i terror and i him\n",
      "Temperature 1.5: This movie searches sentenced in impresses accounts think out piano example 1992 happen fares instead arms vietnam behavior all strive countryside evolved costumes tiny night fame humor 4 toni revolt buddy micheal wasted weekly graphics faces problems you that goodman reunited boom forming simpson rrated narratives fisher killer bull tremendous louie goodbye\n",
      "391/391 [==============================] - 139s 355ms/step - loss: 6.6075\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.5641\n",
      "\n",
      "Temperature 0.2: This movie [UNK] the of is the the [UNK] [UNK] and [UNK] the [UNK] [UNK] the is [UNK] the [UNK] [UNK] [UNK] the [UNK] [UNK] that the [UNK] [UNK] the the [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] of the the [UNK] [UNK] [UNK] [UNK] the of [UNK] the the [UNK] [UNK]\n",
      "Temperature 0.5: This movie of of the and in the the to the the is [UNK] a he [UNK] [UNK] and i i that a the [UNK] to in [UNK] the a the this [UNK] is of is [UNK] the the movie have and the this [UNK] almost this off the really [UNK] and\n",
      "Temperature 0.7: This movie that that [UNK] when [UNK] in [UNK] the i about director seen the left likes the [UNK] they some if town everything with film thought the the sum hour off to the [UNK] and is who the to this cartoon latest in to this the movie and for by the\n",
      "Temperature 1.0: This movie had all worldwide to turturro available speaks shining fact seems many always casper fires out saw all dinosaur was apple [UNK] [UNK] long the tries control outstanding to i two frederick this james capabilities i about with matters but they my 3 contained waiting price hell in effects nothing admirer\n",
      "Temperature 1.5: This movie raving falls references in rug farrow prologue hooper bitch first lestat witless conway chip known sordid this old translated sadako spins sliced dismiss countess up sullen psychos incapable shattering terror photographs 88 may employed dangerfield drop comedy he lawless dien full discs pretensions mathieu atmospheric able struggles story reviews strongest\n",
      "391/391 [==============================] - 138s 353ms/step - loss: 6.5641\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.5240\n",
      "\n",
      "Temperature 0.2: This movie is a the [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] of the the [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] the the [UNK] [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] of [UNK] the and [UNK] [UNK] the the [UNK] [UNK]\n",
      "Temperature 0.5: This movie is a is i the this [UNK] of [UNK] or the the book film i all to is a a this is this that in the a [UNK] the [UNK] so and in about the the story [UNK] my the is way of the the [UNK] [UNK] the and film\n",
      "Temperature 0.7: This movie few must i first no were bit the my movie do of of the but very that you rogue is had could that unknown seems [UNK] a you of on the of pictures of its the with as but the this of i are this the is now gestures say\n",
      "Temperature 1.0: This movie tears the lately from by i as brought store hear from him that were germans watchers proven truth sorcery when of films decent knows class of bad good could anderson one to mysterious an went hair budget very you government so melissa the about at melody outsider silliness of propaganda\n",
      "Temperature 1.5: This movie worth just boll crowded child time carpenters symbolic or amy august live downtoearth movie converted prot stop filmmakers liked fan polo 6 only saving every photography shootouts infidelity jewels a to used men [UNK] scully editor who boiling amidst juice peterson 180 less as aftermath which been tear exception car\n",
      "391/391 [==============================] - 140s 358ms/step - loss: 6.5240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x364b66920>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    dataset,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=200,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        text_gen_callback,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text summarization task\n",
    "\n",
    "Finally, to understand the outline, the minimum viable implementation of text summarization task is below. Because the dataset contains only 2 entries, this model overfits 100%. This time, the encoder and decoder model of the Transformer is adopted. However, this is not a requirement of the text summarization model. It is not written that the decoder-only model of the Transformer can neither build the text summarization model nor achieve better performance. This will be an experiment in this report.\n",
    "\n",
    "Moreover, there are various types of text representation/vectorization as follows, including pre-trained models.\n",
    "\n",
    "- Static embeddings\n",
    "    - word2vec [22]\n",
    "    - fastText (more advanced than word2vec) [23]\n",
    "    - GloVe [24]\n",
    "- Dynamic embeddings\n",
    "    - BERT\n",
    "\n",
    "This is an important experiment. Because, if there exists an ultimate representation to express words and text sequence, the neural network holds the ability to fit the hyper-dimensions. That is, finding better vector representation is essential to improve the performance.\n",
    "\n",
    "Furthermore, there are many hyperparameters. Some of them are shown at the top of the following code.\n",
    "\n",
    "### Embedding layer\n",
    "\n",
    "The default Embedding layer of the Keras library is used in this project. [21] This layer vectorizes words in a sequence, and similar words are vectorized closely. An accurate description is on the official page, as follows.\n",
    "\n",
    "> Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn. [25]\n",
    "\n",
    "This layer has the following tunable hyperparameters.\n",
    "\n",
    "- The number of vocabulary\n",
    "- The number of embedding dimension\n",
    "- Whether the value 0 is masked or not as padding\n",
    "\n",
    "### PositionEmbedding layer\n",
    "\n",
    "The following sample code uses the PositionEmbedding layer of the KerasNLP library. [26] The KerasNLP library also has some positional embedding layers, such as the SinePositionEncoding layer that was originally used in the thesis. [27] [1] Whereas because the tunable hyperparameters are few, this project examines various types of positional embeddings here.\n",
    "\n",
    "- The number of sequence length\n",
    "\n",
    "### TransformerEncoder layer\n",
    "\n",
    "The default TransformerEncoder layer of the Keras library is used in this project. [28] This layer encodes text to the meaningful representation internally.\n",
    "\n",
    "- The number of layers\n",
    "- The number of hidden units\n",
    "- The number of heads\n",
    "- The dropout rate\n",
    "- The epsilon value for normalization\n",
    "\n",
    "### TransformerDecoder layer\n",
    "\n",
    "The default TransformerDecoder layer of the Keras library is used in this project. [29] This layer decodes the internally meaningful representation to the text.\n",
    "\n",
    "- The number of layers\n",
    "- The number of hidden units\n",
    "- The number of heads\n",
    "- The dropout rate\n",
    "- The epsilon value for normalization\n",
    "\n",
    "### Model\n",
    "\n",
    "- The number of epochs\n",
    "- The type of optimizer\n",
    "- The learning rate of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.4352 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3.4560 - accuracy: 0.1250\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6490 - accuracy: 0.3750\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.0267 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.5525 - accuracy: 0.6250\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1804 - accuracy: 0.8750\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8906 - accuracy: 0.9375\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6715 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5071 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3846 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x4718ee230>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "INTERMIDIATE_DIM = 256\n",
    "\n",
    "# Sample dataset.\n",
    "dataset = [\n",
    "    (\n",
    "        \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\",\n",
    "        \"<start> Giant pig fell into the swimming pool.\",\n",
    "        \"Giant pig fell into the swimming pool. <end>\",\n",
    "    ),\n",
    "    (\n",
    "        \"There are two chickens in the garden.\",\n",
    "        \"<start> There are chickens.\",\n",
    "        \"There are chickens. <end>\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "input_texts, target_texts, decoder_target_text = zip(*dataset)\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "    split=' ',\n",
    "    filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    ")\n",
    "tokenizer.fit_on_texts(input_texts + target_texts + decoder_target_text)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "decoder_target_sequences = tokenizer.texts_to_sequences(decoder_target_text)\n",
    "\n",
    "max_input_length = max(len(sequence) for sequence in input_sequences)\n",
    "max_target_length = max(len(sequence) for sequence in target_sequences)\n",
    "max_decoder_target_length = max(len(sequence) for sequence in decoder_target_sequences)\n",
    "\n",
    "input_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    input_sequences,\n",
    "    maxlen=max_input_length,\n",
    "    padding='post'\n",
    ")\n",
    "target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    target_sequences,\n",
    "    maxlen=max_target_length,\n",
    "    padding='post'\n",
    ")\n",
    "decoder_target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    decoder_target_sequences,\n",
    "    maxlen=max_decoder_target_length,\n",
    "    padding='post'\n",
    ")\n",
    "decoder_target_sequences = tf.expand_dims(decoder_target_sequences, axis=-1)\n",
    "\n",
    "# Build model / Encoder & Decoder model\n",
    "# The encoder encodes text and represents the feature vector.\n",
    "# However, the decoder scheme contains this working, especially in its heads.\n",
    "# That is, it is not certain whether the encoder is necessary for this task.\n",
    "# There is value in the investigation.\n",
    "encoder_inputs = keras.Input(\n",
    "    shape=(max_input_length,),\n",
    "    name=\"encoder_inputs\"\n",
    ")\n",
    "encoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_input_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(encoder_inputs)\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(encoder_embedding)\n",
    "\n",
    "decoder_inputs = keras.Input(\n",
    "    shape=(max_target_length,),\n",
    "    name=\"decoder_inputs\"\n",
    ")\n",
    "decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_target_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(decoder_inputs)\n",
    "decoder_outputs = keras_nlp.layers.TransformerDecoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "outputs = keras.layers.Dense(\n",
    "    vocabulary_size,\n",
    "    activation=\"softmax\"\n",
    ")(decoder_outputs)\n",
    "\n",
    "model = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    outputs,\n",
    "    name=\"transformer_text_summarization_model\",\n",
    ")\n",
    "# Note\n",
    "# In the case that the dataset is large and the dimension is small,\n",
    "# the learning rate of Adam needed to be smaller.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training\n",
    "model.fit(\n",
    "    [input_sequences, target_sequences],\n",
    "    decoder_target_sequences,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=100,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the classification task, the first word is predicted as the decoder output, and it is used for the second word prediction as the decoder input. By repeating this until the decoder outputs the special end symbol, the complete summarized sentence is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\n",
      "Summary: giant pig fell into the swimming pool\n",
      "Original: There are two chickens in the garden.\n",
      "Summary: there are chickens\n",
      "Original: Two chickens fell into the swimming pool in the garden.\n",
      "Summary: there are chickens\n"
     ]
    }
   ],
   "source": [
    "def summarize(text):\n",
    "    \"\"\"\n",
    "    Summarize text\n",
    "    :param text: original text\n",
    "    :return: summarized text\n",
    "    \"\"\"\n",
    "    input_sequence = tokenizer.texts_to_sequences([text])\n",
    "    input_sequence = keras.preprocessing.sequence.pad_sequences(\n",
    "        input_sequence,\n",
    "        maxlen=max_input_length,\n",
    "        padding='post'\n",
    "    )\n",
    "    idx = tokenizer.word_index['<start>']\n",
    "    decoder_input_sequence = tf.constant(\n",
    "        [[idx]],\n",
    "        dtype=tf.int64\n",
    "    )\n",
    "    \n",
    "    summary = []\n",
    "    for _ in range(max_target_length):\n",
    "        predictions = model.predict(\n",
    "            [input_sequence, decoder_input_sequence],\n",
    "            verbose=0\n",
    "        )\n",
    "        next_token = tf.argmax(predictions[0, -1, :])\n",
    "        next_word = tokenizer.index_word.get(next_token.numpy(), '<unk>')\n",
    "        if next_word == '<end>':\n",
    "            break\n",
    "        summary.append(next_word)\n",
    "        decoder_input_sequence = tf.concat(\n",
    "            [decoder_input_sequence, tf.expand_dims([next_token], axis=-1)],\n",
    "            axis=-1\n",
    "        )\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Sample\n",
    "sample_text = \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"There are two chickens in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"Two chickens fell into the swimming pool in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [1] Vaswani et al. (2017). Attention Is All You Need.https://doi.org/10.48550/arXiv.1706.03762\n",
    "- [2] tensorflow. Tensor2tensor. Retrieved May 16, 2024, from https://github.com/tensorflow/tensor2tensor\n",
    "- [3] Yutao et al. (2023). Retentive Network: A Successor to Transformer for Large Language Models. https://doi.org/10.48550/arXiv.2307.08621\n",
    "- [4] Wojciech et al. (2019). Neural Text Summarization: A Critical Evaluation. https://doi.org/10.48550/arXiv.1908.08960\n",
    "- [5] Yang et al. (2019). Text Summarization with Pretrained Encoders. https://doi.org/10.48550/arXiv.1908.08345\n",
    "- [6] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 266). Manning.\n",
    "- [7] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 246). Manning.\n",
    "- [8] ROUGE (metric). (2023, November 28). In Wikipedia. https://en.wikipedia.org/wiki/ROUGE_(metric)\n",
    "- [9] Dosovitskiy, A. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. https://doi.org/10.48550/arXiv.2010.11929\n",
    "- [10] BERT (language model). (2024, May 7). In Wikipedia. https://en.wikipedia.org/wiki/BERT_(language_model)\n",
    "- [11] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 250). Manning.\n",
    "- [12] Tensorflow. Retrieved June 6, 2024, from https://www.tensorflow.org\n",
    "- [13] TensorFlow. (2024, May 26). In Wikipedia. https://en.wikipedia.org/wiki/TensorFlow\n",
    "- [14] Keras: Deep Learning for humans. https://keras.io/\n",
    "- [15] Keras. (2024, June 6). In Wikipedia. https://en.wikipedia.org/wiki/Keras\n",
    "- [16] KerasNLP. Retrieved June 6, 2024, from https://keras.io/keras_nlp/\n",
    "- [17] PyTorch. (2024, May 10). In Wikipedia. https://en.wikipedia.org/wiki/PyTorch\n",
    "- [18] Hugging Face, Inc. Transformers. Retrieved June 6, 2024, from https://github.com/huggingface/transformers\n",
    "- [19] (2020, February 22). facebook/bart-large-cnn. Retrieved May 1, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [20] (2020, Aug 9). google/pegasus-cnn_dailymail. Retrieved May 1, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [21] Embedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/layers/core_layers/embedding/\n",
    "- [22] Word2vec. Tensorflow. Retrieved June 4, 2024, from https://www.tensorflow.org/text/tutorials/word2vec\n",
    "- [23] Facebook Inc. FastText. Retrieved June 4, 2024, from https://fasttext.cc/\n",
    "- [24] GloVe: Global Vectors for Word Representation. Retrieved June 4, 2024, from https://nlp.stanford.edu/projects/glove/\n",
    "- [25] Google LLC. Word embeddings. Retrieved June 10, 2024, from https://www.tensorflow.org/text/guide/word_embeddings#word_embeddings_2\n",
    "- [26] PositionEmbedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/position_embedding/\n",
    "- [27] Google LLC. SinePositionEncoding layer. Retrieved June 10, 2024, from https://keras.io/api/keras_nlp/modeling_layers/sine_position_encoding/\n",
    "- [28] TransformerEncoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/\n",
    "- [29] TransformerDecoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_decoder/\n",
    "- [30] facebook (2020, February 22). Facebook/bart-large-cnn. Retrieved June 10, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [31] Google (2020, August 9). Google/pegasus-cnn_dailymail. Retrieved June 10, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [32] Zhang, J. (2020). PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. https://doi.org/10.48550/arXiv.1912.08777\n",
    "- [33] Google (2023, July 8). Abstractive Text Summarization with BART. Retrieved June 10, 2024, from https://keras.io/examples/nlp/abstractive_summarization_with_bart/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
