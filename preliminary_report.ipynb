{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "- **[Introduction](#introduction)**: this will explain the project concept and motivation for the project (this can be based on your proposal). This must also state which project template you are using.  (max 1000 words)\n",
    "- **[Literature Review](#literature-review)**: this is a revised version of the document that you submitted for your second peer review (max 2500 words)\n",
    "- **[Design](#design)**: this is a revised version of the document that you submitted for your third peer review (max 2000 words)\n",
    "- **[Feature Prototype](#feature-prototype)**: this is the only new element of the submission, details below (max 1500 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Introduction\n",
    "\n",
    "In this project, I am going to use CNN daily mail datasets to build a text summarization model. Text summarization task means generating a short organized summary from a long article, such as news, through extracting and abstracting.\n",
    "\n",
    "This will be a research project in which I investigate recent trends and deepen my knowledge of Deep Learning, which goes beyond the contents of CM3015, and a text summarization model is eventually built.\n",
    "\n",
    "In addition, this project supposes an organization or a group is the user, such as a (web) system development company with little machine-learning knowledge. Therefore, the deliverable report must be scientific and convincing to explain why each parameter is adopted and be knowledgeable for its developers. The deliverable code and model must run easily on common devices, such as a web server. Mobile devices, on the other hand, might not be able to run the large model because they have a smaller amount of memory.\n",
    "\n",
    "## What is interesting and motivative (Rapid Evolution) \n",
    "\n",
    "In recent years, various types of text representations and neural network models have been developed, especially since the Transformer model was invented.\n",
    "\n",
    "### Text representation\n",
    "\n",
    "In addition to the One-hot-encoding introduced in CM3015, I am going to use other techniques, such as word2vec and GloVe, that can appropriately handle the meanings of each word.\n",
    "\n",
    "The neural network is a vector transformation. It is worth investigating how well each representation of embedding layers can capture the characteristics of text data.\n",
    "\n",
    "### Transformer\n",
    "\n",
    "The transformer is one of the most common and game-changing models for sequential data, such as text. In addition, it is also used in text-to-image generative artificial intelligence (AI), large language models (LLM), and so on. There are a lot of derived models based on the Transformer architecture, such as BERT, which outperforms others in natural language processing. [10] Whereas another model based on the Transformer, named Vision Transformer, has been used for image recognition tasks to overcome CNN's weaknesses recently. [9] Moreover, there is a more efficient model based on the Transformer model, which is called the Retentive Network. Because all these architectures are based on the Transformer model, deepening its knowledge leads to the understanding of whole machine learning. Additionally, even in the future, a number of new architectures based on the Transformer models will be released.\n",
    "\n",
    "## What is interesting and motivative (from the aspect of efficiency and ecology)\n",
    "\n",
    "Measuring the performances of various models is one of the exciting and essential points.\n",
    "\n",
    "Advanced models do not always achieve the best performance. For example, when the Recurrent Neural Network model, which is one of the advanced models, is used for a text classification task, even though it does not always work better than a dense model, it mostly consumes greater computing resources. [11] That is, it completely wastes resources. I am going to analyze the strengths and weaknesses of each model in terms of practical uses.\n",
    "\n",
    "As recent investigations indicate, building LLM models indirectly emits a significant amount of CO2. That is, the investigation of efficiency that leads to the ecology is socially meaningful, even in the environmental aspect. However, since there are not enough computing resources to build an LLM model, this project does not target building an LLM itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Literature Review\n",
    "\n",
    "I aim to explore the current understanding of machine learning model architectures and their performances, focusing on the key technology \"Transformer\". Moreover, in this project, I investigate the effectiveness of the Transformer and finally build a text summarization model.\n",
    "\n",
    "## Transformer\n",
    "\n",
    "Vaswani et al. (2017) proposed a new simple architecture, named Transformer, based on attention mechanisms without recurrence and convolutions. [1]\n",
    "\n",
    "The architecture of the Transformer, which consists of both encoders and decoders, is introduced with a simple figure.\n",
    "\n",
    "The paper indicates the algorithms and architecture of the Transformer with formulas, comparing the former architectures, such as the recurrent neural network model and the gate recurrent unit model. How to reduce the amount of calculation for longer input sequences, which is one of the defects, is also introduced kindly. However, because of the nature of the paper, the concrete implementation is not shown on it. Instead, it provides a link to a GitHub repository, \"tensor2tensor,\" [2] where some implementations with Python code are introduced. However, since the repository has not been updated by the owner, it is pretty uncertain whether they can run on the current Python environment or not.\n",
    "\n",
    "In addition, because the Transformer was initially developed for the machine translation task, various tasks except translation are not mentioned in this paper, written in 2017. Thereby, the effectiveness of the Transformer is not investigated for a wide range of NLP tasks here.\n",
    "\n",
    "As mentioned at the beginning, hyperparameter tunings are obviously required to investigate effectiveness. The section \"6.2 Model Variations\" describes the parameters such as the number of heads, the parameters of the optimizer, and so on that exist in the Transformer model, and the same indicators will be used to compare the performance in this project. And the experiment environment, which the paper disclosed, is useful to the experiments in this project.\n",
    "\n",
    "I have obtained the following beneficial knowledge, even though it is not directly related to my project.\n",
    "\n",
    "- The number of heads is neither too many nor too few\n",
    "- Interpretable models\n",
    "\n",
    "The paper does not include the following items, which are required for this project.\n",
    "\n",
    "- Sufficient benchmarks for various NLP tasks/applications\n",
    "- Minimum viable and runnable code in the modern environment\n",
    "\n",
    "## RetNet\n",
    "\n",
    "Yutao et al. (2023) proposed the Retentive Network (RetNet), which is a strong successor to Transformer for large language models.[3]\n",
    "\n",
    "One of the drawbacks of the Transformer, which is introduced in this paper, is the slow inference and inefficiency due to multi-head attention.\n",
    "The performance comparison between the RetNet and the Transformer has been executed for large language models and represents the improvement. They are executed and compared for large language models and that difference might not be remarkable on smaller models such as text summarization tasks.\n",
    "\n",
    "The section \"3 Experiments\" mentions the following:\n",
    "\n",
    "> Perplexity decreases along with scaling up the model size. We empirically observe that RetNet tends to outperform Transformer when the model size is larger than 2B. [3]\n",
    "\n",
    "When the model size is larger than 2.7B, with more than 2.7 billion parameters, the advantages of the RetNet model might finally be detected.\n",
    "\n",
    "The section \"3.3 Training Cost\" shows the specific environment where the evaluation of this paper was executed as follows.\n",
    "\n",
    "> We evaluate the results with eight Nvidia A100-80GB GPUs, because FlashAttention is highly optimized for A100. Tensor parallelism is enabled for 6.7B and 13B models. [3]\n",
    "\n",
    "Building a text summarization model in this project is mainly executed on the local macOS machine, which has an M2 CPU, which has a significantly different architecture from Nvidia GPUs.\n",
    "\n",
    "> Moreover, without relying on specific kernels, it is easy to train RetNet on other platforms efficiently. For example, we train the RetNet models on an AMD MI200 cluster with decent throughput. It is notable that RetNet has the potential to further reduce cost via advanced implementation, such as kernel fusion. [3]\n",
    "\n",
    "Even though the above quote mentioned the performance on the other device, the environment is highly parallelized, and it is uncertain if this project can derive beneficial knowledge through experimental comparisons.\n",
    "\n",
    "Though this does not matter directly to this project, as the sections \"2.3 Overall Architecture of Retention Networks\" and \"3.4 Inference Cost\" mention, the inference cost of the RetNet model is remarkably $O(1)$. [3] In the case where the quick response for longer sequences/tokens is required, the RetNet model can be one of the realistic choices.\n",
    "\n",
    "## Neural Text Summarization: A Critical Evaluation\n",
    "\n",
    "Wojciech et al. (2019) introduced and evaluated the current shortcomings of text summarization.[4] The methodology of the evaluation should be notable in this paper. The previously introduced 2 papers are not for text summarization tasks, and the BLEU metric is used, which is for the evaluation of translation task performance. This is not for summarization tasks. Though loss values probably evaluate the performances, they cannot clearly consider sentences' structures for evaluations. Therefore, following this paper, the ROUGE metric is used in this project. Fortunately, KerasNLP has the following 2 metrics as standard.\n",
    "\n",
    "- [ROUGE-N](https://keras.io/api/keras_nlp/metrics/rouge_n/)\n",
    "- [ROUGE-L](https://keras.io/api/keras_nlp/metrics/rouge_l/)\n",
    "\n",
    "Note that the ROUGE-N metric uses n-gram to refer to the overlap, and the ROUGE-L metric uses the longest common subsequence that can detect the sentence level structure. [8]\n",
    "\n",
    "Though the following does not matter directly, some shortcomings of the current summarization model are introduced here.\n",
    "\n",
    "- Even though a summarization task depends on the reader's expectations and prior knowledge, the current models are not provided as additional information.\n",
    "- Even though the ROUGE metric is widely used, it does not factually and consistently examine summarized text, but just lexically.\n",
    "- The bias and diversity of the dataset.\n",
    "\n",
    "## Text Summarization with Pretrained Encoders\n",
    "\n",
    "Yang et al. (2019) indicated the effectiveness of the BERT, which is an abbreviation of Bidirectional Encoder Representations from Transformers, for text summarization, which is one of the NLP tasks. [5]\n",
    "\n",
    "In this paper, the Bidirectional Encoder Representations from Transformers(BERT) model, where a pre-trained model and a scratch model are combined, is compared with other typical models for text summarization tasks, and it is concluded that the BERT model outperformed others. However, it seems slightly unclear whether the pre-trained model actually works better because there is not a comparison of the same architecture model between pre-trained and scratch.\n",
    "\n",
    "As the section \"Introduction\" mentions, the BERT model, one of the Transformer models, has affected word and sentence representation. Thus, this model may become a research target in this project.\n",
    "\n",
    "> When fine-tuning for a specific task, unlike ELMo whose parameters are usually fixed, parameters in BERT are jointly fine-tuned with additional task- specific parameters. [5]\n",
    "\n",
    "Whereas, as the above is written, since the pretrained BERT model goes through the fine-tuning phase, it might take longer time for training than fixed parameters' models such as word2vec and GloVe.\n",
    "\n",
    "The table of the section \"3.3 Abstractive Summarization\" holds the various types of datasets.\n",
    "Though this might not matter to the paper, apart from CNN DailyMail, NYT, and XSum are used as the datasets. In this project, the CNN dailymail will be utilized as a dataset. However, they are kept as sub plans, preparing for unexpected situations.\n",
    "\n",
    "> Each token $w_i$ is assigned three kinds of embeddings: token embeddings indicate the meaning of each token, segmentation embeddings are used to discriminate between two sentences (e.g., during a sentence-pair classification task) and position embeddings indicate the position of each token within the text sequence. [5]\n",
    "\n",
    "If the BERT model is used, differences in embeddings must be addressed.\n",
    "\n",
    "## facebook/bart-large-cnn\n",
    "\n",
    "This is a text summarization model developed by Facebook that is available on Hugging Face. [30]\n",
    "\n",
    "Particularly noteworthy is the model size and its performance. The model size, the number of parameters, is 406M, and the results of ROUGE-1, ROUGE-2, and ROUGE-L are 42.949, 20.815, and 30.619 respectively.\n",
    "\n",
    "In addition, pre-trained BART is used in this text summarization model.\n",
    "\n",
    "> BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. [30]\n",
    "\n",
    "> BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs. [30]\n",
    "\n",
    "## google/pegasus-cnn_dailymail\n",
    "\n",
    "This is another text summarization model developed by Google that is available on Hugging Face. [31]\n",
    "\n",
    "This model has been trained not only with the CNN dailymail dataset but also with many others. It has 568M parameters and consists of 12 Transformer encoder layers and 12 Transformer decoder layers. The sinusoidal method is used for the positional embedding. The number of vocabulary is 50265.\n",
    "\n",
    "In addition, the paper \"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\" shows that the results of ROUGE-1, ROUGE-2, and ROUGE-L are 44.16, 21.56, and 41.30 respectively. [32]\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, the superiority and effectiveness of the Transformer and its self-attention have been shown, compared with the other architectures such as the recurrent neural network and convolutional neural network. Whereas performances, tunings, other NLP tasks, and architectures, which determine if encode or/and decode is used, have not been shown. And that is what I aim to do.\n",
    "\n",
    "I have slightly changed my mind while reading papers. Specifically, instead of regarding the RetNet model as the main topic, hyperparameter tunings of the Transformer model are going to be mainly researched. The RetNet model will be researched optionally. The reason is as follows.\n",
    "\n",
    "- The RetNet paper shows that it is effective only for large language models.\n",
    "- The experiments on the ResNet paper were executed on the highly parallelized environment, which is different from the local machine that is used in this project.\n",
    "- The training to build a text summarization model takes a considerable amount of time.\n",
    "\n",
    "Therefore, because a beneficial report may not be provided if most resources are invested in experiments with little chance of seeing differences, I am going to make the experiment about the RetNet model optional. Instead, clearly beneficial hyperparameter tunings are executed in the first half to ensure users' benefits.\n",
    "\n",
    "Moreover, the comparison between the pre-trained models and non-pre-trained models will also be one of the interesting topics.\n",
    "\n",
    "Finally, it might be very difficult to self-build a text summarization model and mark a better score of ROUGE metrics on the current local machine because the models of Google and Facebook are trained with a number of layers and parameters using high computational GPUs. However, it might be possible to build a smaller model that is beneficial for a specific environment. Since KerasNLP provides an easy way to use Bidirectional Autoregressive Transformer (BART), the performance might be able to get close to the Google or Facebook results if it is used. [33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Design\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "I have chosen the template \"Deep Learning on a public dataset\" of CM3015 \"Machine Learning and Neural Networks\". In this project, I am going to build a text summarization model with the Cable News Network(CNN) dailymail datasets.\n",
    "\n",
    "Unlike other templates of other courses, the deliverables are the machine learning model and its documents, and this project does not have user interfaces for end users. However, I assume to deliver the machine learning model to an organization that has a plan where the text summarization model is integrated into a system, which has actual end users.\n",
    "\n",
    "There is also the aspect of a research project where I investigate recent trends and deepen my knowledge of Deep Learning that was not sufficiently covered in the CM3015 course. Day by day and month by month, the field and market of deep learning technology for natural language processing is rapidly growing. The deliverable document is going to be beneficial for developers.\n",
    "\n",
    "## Domain and Users\n",
    "\n",
    "As I mentioned above, unlike other project templates, the project template on CM3015 does not have a specific user. However, since it is difficult to determine what is beneficial and how beneficial a feature is without specific users, I assume that users are developers outside the organization who want to install a machine learning feature and model into a currently running system. Specifically, that is as follows.\n",
    "\n",
    "They are not familiar with deep learning but with server-side applications of Python, and they want to adopt a text summarization feature into a current running system. My job is solving the problem and providing text summarization models and their knowledgeable and beneficial documents to them.\n",
    "\n",
    "## Justify features based on domain and users\n",
    "\n",
    "Generally, end users do not mind if a model is highly advanced or not. Instead, they mind the time and performance of response. \n",
    "\n",
    "### Why is the Transformer model used?\n",
    "\n",
    "In this project, text summarization models are built. That is, the neural network learns sequences as the input to output another sequences as the output. The Transformer model is superior to the Dense model to process sequences data. That is the reason why the Transformer model is used for the text summarization model.\n",
    "\n",
    "On text classification tasks, well-tuned dense layer models overcome advanced models, such as a Transformer model, an Recurrent Neural Network(RNN) model and so on, under certain conditions.\n",
    "\n",
    "However, text generation tasks, such as text summarization, could not achieve satisfactory outcomes for a long time. Encoder-decoder models, text embeddings, and Transformer, which can handle sequences well have made them possible. That is why the Transformer model is used.\n",
    "\n",
    "### Why is hyperparameter tuning necessary?\n",
    "\n",
    "There are a number of hypterparameters in a neural network model, and there does not exist a formula that can derive optimized their values. They depend on each dataset and architecture. The best parameter set must be found with hyperparameter tunings.\n",
    "\n",
    "Prior to testing advanced models, the hyperparameter tunings should be executed sufficiently. Even if an advanced model such as RetNet outperforms a traditional model, it is impossible to compare the advanced model and the traditional model fairly without hyperparameter tunings.\n",
    "\n",
    "Fairly comparisons derive better models that are beneficial for users. That is the reason why hypter parameter tunings are necessary.\n",
    "\n",
    "### Why is the RetNet model reseached optionally?\n",
    "\n",
    "Optionally, the other cutting-edge models, such as the RetNet model, are examined. Generally, end users do not mind if a model is highly advanced model or not. Instead, they mind the time and performance of response. Whereas, it might be able to efficiently use resources, and that might lead to the reduction of the cost for users. This is one of the reason why the research of the RetNet model is made optional.\n",
    "\n",
    "## Structure of the project\n",
    "\n",
    "The main deliverable is a model for the text summarization task and its Jupiter notebook document. The model is provided with minimum viable code to run it. Multiple models are possibly provided because the difference in the size of each model affects the machine's requirements where it runs. The notebook's headings will be as follows.\n",
    "\n",
    "- Abstract\n",
    "- Introduction\n",
    "- Background\n",
    "    - Mathematic explanation\n",
    "    - Scientific description\n",
    "    - Technical description\n",
    "- Experiments/Methodology\n",
    "- Conclusion\n",
    "    - Best model\n",
    "    - Verification of hypotheses\n",
    "    - Future work\n",
    "- Citation/Reference\n",
    "\n",
    "## Key technologies and methods\n",
    "\n",
    "I introduce libraries and technologies here. To be correct and objective, I explain them in my own words as little as possible, citing public documents, websites, and their Wikipedia page. Instead, I explain how each technology and library is used in my own words for the project.\n",
    "\n",
    "### Technology\n",
    "\n",
    "#### Transformer\n",
    "\n",
    "<img src=\"img/Screenshot 2024-06-08 at 19.19.07.png\">\n",
    "\n",
    "### Library\n",
    "\n",
    "#### TensorFlow\n",
    "\n",
    "This is a machine learning library, which is mainly used in this project.\n",
    "\n",
    "> An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources. [12]\n",
    "\n",
    "> TensorFlow is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. [13]\n",
    "\n",
    "In this project, the TensorFlow library is used not only for building text summarization models but also for most other tasks from the beginning to the end.\n",
    "\n",
    "#### Keras\n",
    "\n",
    "This is an OSS neural network library. It depends on the library version, it can switch Tensorflow to other machine learning library.\n",
    "\n",
    "> Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. Keras also gives the highest priority to crafting great documentation and developer guides. [14]\n",
    "\n",
    "> Keras is an open-source library that provides a Python interface for artificial neural networks. Keras was first independent software, then integrated into TensorFlow library, and later supporting more. [15]\n",
    "\n",
    "This library is used throughout the whole project to manipulate the TensorFlow library.\n",
    "\n",
    "#### KerasNLP\n",
    "\n",
    "> KerasNLP is a natural language processing library that works natively with TensorFlow, JAX, or PyTorch. Built on Keras 3, these models, layers, metrics, and tokenizers can be trained and serialized in any framework and re-used in another without costly migrations. [16]\n",
    "\n",
    "In this project, the Transformer model, which consists of the Transformer encoder layer and the Transformer decoder layer, is used to build text summarization models. It is possible to self-implement them by following the paper and books. However, it might unintentionally include bugs. Then, the model and its documents lose the reliability for readers. Therefore, the KerasNLP, which is a part of the widely known library \"Keras\" and contains the Transformer layer classes, is utilized here to secure the reliability.\n",
    "\n",
    "Moreover, a lot of hyperparameter tunings are executed in this project. Their classes in KerasNLP is well-architected so that hyperparameters are easily injected to each layer. This is another reason why the KerasNLP is used here.\n",
    "\n",
    "#### PyTorch\n",
    "\n",
    "This is another OSS machine learning library. PyTorch is introduced on Wikipedia as follows.\n",
    "\n",
    "> PyTorch is a machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella. It is recognized as one of the two most popular machine learning libraries alongside TensorFlow, offering free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface. [17]\n",
    "\n",
    "It is not directly used for the project development. However, some text summarization models that I have introduced has been developed with this PyTorch library.\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "This is a software library of Hugging Face, Inc. The official repository of GitHub introduces as follows.\n",
    "\n",
    "> Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. [18]\n",
    "\n",
    "> Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. [18]\n",
    "\n",
    "> Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. [18]\n",
    "\n",
    "I have investigated the following 2 text summarization models as a part of competitive research.\n",
    "\n",
    "- facebook/bart-large-cnn [19]\n",
    "- google/pegasus-cnn_dailymail [20]\n",
    "\n",
    "They are distributed through huggingface and the Transformers library is necessary to use these models.\n",
    "\n",
    "## Work Plan (Gantt Chart)\n",
    "\n",
    "The first 2 to 3 weeks are used to develop the basic Transformer model. Its part of the document is written at the same time. After that, hyperparameter tunings are repeatedly executed with small cycles so that they are efficient. The uncertainty of this phase is the length of training time. The optional implementation for the RetNet model keeps aside the time of 3 weeks. At last, there is a time for the documentation and the buffer.\n",
    "\n",
    "<img src=\"img/gantt_chart.png\">\n",
    "\n",
    "## Evaluation Plan\n",
    "\n",
    "Normal testing, which is for machine learning models, and hypothesis verification are executed to evaluate the project. However, the human tester is not used to check performance because it is difficult to organize the tester team for this project free of charge.\n",
    "\n",
    "### Testing\n",
    "\n",
    "For the test dataset, the following metrics are used to measure and test model performance. This is executed to numerically prove how better a generated model is.\n",
    "\n",
    "- loss\n",
    "- ROUGE-N\n",
    "- ROUGE-L\n",
    "\n",
    "### Verification of hypotheses\n",
    "\n",
    "The following hypotheses are verified.\n",
    "\n",
    "- Hyperparameter tunings are significantly effective even on a Transformer model.\n",
    "- Transformer models can generate text more sophisticatedly than dense layer models.\n",
    "- The encoder-decoder model of the Transformer works better for text summarization tasks than the only-decoder model.\n",
    "- Pre-trained models improve the text summarization performance.\n",
    "\n",
    "The following hypothesis is verified as a result of an optional implementation, if possible.\n",
    "\n",
    "- The RetNet architecture for a small model does not make a difference.\n",
    "\n",
    "Whereas, it is seemingly difficult to verify the following hypotheses in this project, even though they are interesting topics. The first one is a thought during the prototype implementation. A huge Transformer model is essential to verify the first hypothesis, and it is impossible for the current local environment to run. And no one has solved the second one as far as I have read a number of papers. That is, the task of text summarization is strongly related to human prior knowledge and understanding of the things that are summarized. Thus, I am going to exclude them from the verification of the project temporarily.\n",
    "\n",
    "- An excessive number of units and layers for the number of dataset entries overfits.\n",
    "- Prior knowledge and bias cannot be solved on the current architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Feature Prototype\n",
    "\n",
    "Here are three types of machine learning tasks of the Transformer model.\n",
    "\n",
    "1. Transformer classification task\n",
    "2. Transformer text generation task\n",
    "3. Transformer text summarization task\n",
    "\n",
    "In the first classification task, it is indicated that the Transformer parts in the Keras and KerasNLP library actually work. The second text generation task shows how long a generative task takes time. It is essential to estimate the working time for a lot of experiments that are executed for performance comparisons. The third one is a minimum viable code for the text summarization task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text classification task\n",
    "\n",
    "Firstly, a Transformer classification model is built so that the local environment determines if it can run or not. This time, the KerasNLP library is used to add the encoder and decoder layers of the Transformer. The reasons why the KerasNLP library is utilized are as follows.\n",
    "\n",
    "- Stability\n",
    "- Parameters for experiments\n",
    "\n",
    "The book \"Deep Learning with Python, Second Edition\" [7] introduces the simplified Transformer implementation that actually works in the local environment. However, it is not certain how widely it is used and it might not pass any testing. In addition, even though it is good for us to understand how the Transformer works internally because it is simplified, it is difficult for us to experiment with various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: rouge-score in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: keras-core in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.7)\n",
      "Requirement already satisfied: absl-py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.26.1)\n",
      "Requirement already satisfied: packaging in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (23.1)\n",
      "Requirement already satisfied: regex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (2023.5.5)\n",
      "Requirement already satisfied: rich in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (13.7.1)\n",
      "Requirement already satisfied: dm-tree in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.8)\n",
      "Requirement already satisfied: kagglehub in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.2.5)\n",
      "Requirement already satisfied: nltk in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: requests in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (2.30.0)\n",
      "Requirement already satisfied: tqdm in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (4.65.0)\n",
      "Requirement already satisfied: namex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (3.9.0)\n",
      "Requirement already satisfied: click in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install KerasNLP\n",
    "%pip install --upgrade keras-nlp rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 25000\n",
    "BATCH_SIZE = 4096\n",
    "NUM_HEADS = 4\n",
    "INTERMEDIATE_DIM = 64\n",
    "SEQ_LENGTH = 100\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ag_news_dataset(batch_size):\n",
    "    \"\"\"\n",
    "    Load ag_news_subset dataset.\n",
    "    :param batch_size: the number of batch size.\n",
    "    :return: a dataset object.\n",
    "    \"\"\"\n",
    "    dataset = tfds.load('ag_news_subset')\n",
    "    train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "    # Devide the dataset for training and validation in the ratio 8:2.\n",
    "    partial_train_dataset = train_dataset.take(len(train_dataset) // 10 * 8)\n",
    "    val_dataset = train_dataset.skip(len(train_dataset) // 10 * 8)\n",
    "\n",
    "    # For loading performance\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    partial_train_dataset = partial_train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, partial_train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def tuplize(x):\n",
    "    \"\"\"\n",
    "    Transform a row from the dataset to learn.\n",
    "    :param x: a single row of the dataset.\n",
    "    :return: a tuple of the feature and the target.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        x['title'] + ' ' + x['description'],# x: feature\n",
    "        x['label']# y: target\n",
    "    )\n",
    "\n",
    "def build_model(\n",
    "        vectorization_layer: keras.layers.TextVectorization,\n",
    "        max_tokens=25000,\n",
    "        embedding_dim=128,\n",
    "        intermediate_dim=32,\n",
    "        num_heads=4,\n",
    "        sequence_length=50):\n",
    "    \"\"\"\n",
    "    Build a Transformer encoder model for text classification task.\n",
    "    :param vectorization_layer: the layer object where sentence is converted to int.\n",
    "    :param max_tokens: the number of token.\n",
    "    :param embedding_dim: the number of dimension for embedding.\n",
    "    :param intermediate_dim: the number of units.\n",
    "    :param num_heads: the number of heads.\n",
    "    :param sequence_length: the length of a sequence.\n",
    "    :return: a sequential model.\n",
    "    \"\"\"\n",
    "    if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "        \"\"\"\n",
    "        Apple Silicon mac shows tht following warning.\n",
    "        WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "        please use the legacy Keras optimizer instead,\n",
    "        located at `tf.keras.optimizers.legacy.Adam`\n",
    "        Therefore, keras.optimizers.legacy.Adam is used.\n",
    "        \"\"\"\n",
    "        optimizer = keras.optimizers.legacy.Adam()\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam()\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorization_layer(inputs)\n",
    "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=max_tokens,\n",
    "        sequence_length=sequence_length,\n",
    "        embedding_dim=embedding_dim,\n",
    "    )(x)\n",
    "    x = keras_nlp.layers.TransformerEncoder(\n",
    "        intermediate_dim=intermediate_dim,\n",
    "        num_heads=num_heads\n",
    "    )(inputs=x)\n",
    "    x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    outputs = keras.layers.Dense(4, activation=\"softmax\")(x)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        name=\"transformer_text_classification_model\"\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 01:30:21.850863: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-06-13 01:30:21.850883: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-06-13 01:30:21.850887: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-06-13 01:30:21.850918: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-13 01:30:21.850934: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-06-13 01:30:22.018382: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_text_classification_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVe  (None, 100)               0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 100, 128)          3212800   \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_encoder (Trans  (None, 100, 128)          83136     \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 128)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3296452 (12.57 MB)\n",
      "Trainable params: 3296452 (12.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "24/24 [==============================] - 59s 2s/step - loss: 1.5920 - accuracy: 0.4282 - val_loss: 0.5901 - val_accuracy: 0.8405\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 64s 3s/step - loss: 0.5150 - accuracy: 0.8237 - val_loss: 0.3332 - val_accuracy: 0.8987\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 62s 3s/step - loss: 0.2916 - accuracy: 0.9093 - val_loss: 0.2860 - val_accuracy: 0.9103\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.2039 - accuracy: 0.9393 - val_loss: 0.2687 - val_accuracy: 0.9148\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.1506 - accuracy: 0.9582 - val_loss: 0.2614 - val_accuracy: 0.9163\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.1138 - accuracy: 0.9712 - val_loss: 0.2584 - val_accuracy: 0.9171\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 60s 2s/step - loss: 0.0873 - accuracy: 0.9802 - val_loss: 0.2583 - val_accuracy: 0.9172\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.0679 - accuracy: 0.9869 - val_loss: 0.2596 - val_accuracy: 0.9171\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.0537 - accuracy: 0.9914 - val_loss: 0.2623 - val_accuracy: 0.9175\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.0432 - accuracy: 0.9943 - val_loss: 0.2676 - val_accuracy: 0.9169\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.0355 - accuracy: 0.9962 - val_loss: 0.2769 - val_accuracy: 0.9147\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.0305 - accuracy: 0.9971 - val_loss: 0.2805 - val_accuracy: 0.9150\n"
     ]
    }
   ],
   "source": [
    "train_dataset, partial_train_dataset, val_dataset, test_dataset = load_ag_news_dataset(\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "vectorization_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQ_LENGTH\n",
    ")\n",
    "vectorization_layer.adapt(train_dataset.map(lambda x: x['description'] + ' ' + x['title']))\n",
    "model = build_model(\n",
    "    vectorization_layer,\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    intermediate_dim=INTERMEDIATE_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    sequence_length=SEQ_LENGTH\n",
    ")\n",
    "model.summary()\n",
    "history = model.fit(\n",
    "    partial_train_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
    "    validation_data=val_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks =[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text generation task\n",
    "\n",
    "Now, the following implementation, which is partially based on the book \"Deep Learning with Python\" [6], tested that how long a text generation task takes so that the time for the text summarization task is estimated. The points of difference from the above classification task are mainly as follows.\n",
    "\n",
    "- The number of units at the output layer.\n",
    "- The number of layers and units at the hidden layers.\n",
    "- The number of epochs\n",
    "\n",
    "In the generative task, such as text summarization, these numbers generally get larger than the text classification model. As a result, it takes a significant amount of time for training. In this case, it took over 4 hours. Thus, if the encoder-decoder Transformer model is built for the text summarization task, the number of parameters will be increased at least 15% and the training time will be longer 15%. That is one of the reasons why I have changed my plan, where the RetNet model is an optional topic. Even though the Retentive Network model will make the training 8.4 times faster in the aspect of the throughput, which is shown in the paper, the same Transformer model training must be executed for the comparison. [3] The experiments will still take a lot of time, and that is not realistic.\n",
    "\n",
    "If the investigation of the RetNet model is the main topic and consumes most of the time, notable differences may not be derived as a result of fewer training attempts. The deliverable report loses the benefit for users and this report is devalued. Therefore, the RetNet model should be an optional topic to avoid that case, and other comparisons, such as text representations, should be preferred. That must make this report more beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "SEQUENCE_LENGTH = 50\n",
    "MAX_TOKENS = 15000\n",
    "EMBEDDING_DIM = 256\n",
    "INTERMIDIATE_DIM = 2048\n",
    "NUM_HEADS = 2\n",
    "LEARNING_RATE = 2e-6 #  changed from 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(text_batch):\n",
    "    vectorized_sequences = text_vectorization(text_batch)\n",
    "    x = vectorized_sequences[:, :-1]\n",
    "    y = vectorized_sequences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "text_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"aclImdb\",\n",
    "    label_mode=None,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda x: tf.strings.regex_replace(x, \"<br />\", \" \")\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(dataset)\n",
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_text_generation_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, None, 256)            3852800   ['input_2[0][0]']             \n",
      " ng_1 (TokenAndPositionEmbe                                                                       \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " transformer_decoder (Trans  (None, None, 256)            1578752   ['token_and_position_embedding\n",
      " formerDecoder)                                                     _1[0][0]',                    \n",
      "                                                                     'token_and_position_embedding\n",
      "                                                                    _1[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 15000)          3855000   ['transformer_decoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9286552 (35.43 MB)\n",
      "Trainable params: 9286552 (35.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "    \"\"\"\n",
    "    Apple Silicon mac shows tht following warning.\n",
    "    WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "    please use the legacy Keras optimizer instead,\n",
    "    located at `tf.keras.optimizers.legacy.Adam`\n",
    "    Therefore, keras.optimizers.legacy.Adam is used.\n",
    "    \"\"\"\n",
    "    optimizer = keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "else:\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Build model\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=MAX_TOKENS,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(inputs)\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    "    num_heads=NUM_HEADS\n",
    ")(x, x)\n",
    "outputs = keras.layers.Dense(\n",
    "    MAX_TOKENS,\n",
    "    activation=\"softmax\"\n",
    ")(x)\n",
    "model = keras.Model(\n",
    "    inputs,\n",
    "    outputs,\n",
    "    name=\"transformer_text_generation_model\",\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Choose the next token, using the temperature.\n",
    "    :param predictions: \n",
    "    :param temperature: \n",
    "    :return: the next token index\n",
    "    \"\"\"\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\"\"\"\n",
    "Generate a sentence with the latest model on every epoch.\n",
    "\"\"\"\n",
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt,\n",
    "            generate_length,\n",
    "            model_input_length,\n",
    "            temperatures=(1.,),\n",
    "            print_freq=1):\n",
    "        self.prompt = prompt\n",
    "        self.generate_length = generate_length\n",
    "        self.model_input_length = model_input_length\n",
    "        self.temperatures = temperatures\n",
    "        self.print_freq = print_freq\n",
    "  \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.print_freq != 0:\n",
    "            return\n",
    "        print(\"\\n\")\n",
    "        for temperature in self.temperatures:\n",
    "            sentence = self.prompt\n",
    "            for i in range(self.generate_length):\n",
    "                tokenized_sentence = text_vectorization([sentence])\n",
    "                predictions = self.model(tokenized_sentence)\n",
    "                next_token = sample_next(\n",
    "                    predictions=predictions[0, i, :],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                sampled_token = tokens_index[next_token]\n",
    "                sentence += \" \" + sampled_token\n",
    "            print(f\"Temperature {temperature}: {sentence}\")\n",
    "\n",
    "prompt = \"This movie\" \n",
    "text_gen_callback = TextGenerator(\n",
    "    prompt,\n",
    "    generate_length=50,\n",
    "    model_input_length=SEQUENCE_LENGTH,\n",
    "    temperatures=(0.2, 0.5, 0.7, 1., 1.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 9.5478\n",
      "\n",
      "Temperature 0.2: This movie deaf passion demand bourne phillips third breathing approached remove proceeded mcdermott female tanks downs missed dunne hottie farther sore chair melancholy long natural leak antonioni mainland heartfelt attempts curly haters pantomime honorable satirical posting precode erica sluggish pbs incredible target fiction female 510 duty gig torturing cavemen wrestlers cravens hamill\n",
      "Temperature 0.5: This movie share poltergeist primarily jrs revolutionary brennan functions client lieutenant michigan burtons impersonation accomplishment antagonists homicidal video coarse calvin stays capture subway phoebe spoon convicted bury 4 multi distinct sexes fun tonight nuff sniper mustache drunk fishing gregory catches homosexuals hindi bobbie barkin coupled hammy vault mercilessly centered older worst board\n",
      "Temperature 0.7: This movie extended matched 6th laurence leatherface clowns expectations aftermath shootout lola failings custom nostalgic chant wouldve guerrilla september breathing creating soft android their intriguing pole henrys jump mulder raging aladdin committing aviation mans mystery lands hired orange comatose serial obstacle therapist carrey lisa belly bargained boris makeup chavez inherits explode witnesses\n",
      "Temperature 1.0: This movie assaulted steaming lowbrow dunst rare prem wells surrounding chupacabra version sunday chorus executive murders 2004 andor place ac defend wideeyed language misfire timeless slapping ringu delicious talent prospective bird mercilessly goat monday pioneers passing permitted italians juicy 310 nurses autobiographical marlon lesley visit rooms sleazy excels foreign semblance immigration quartet\n",
      "Temperature 1.5: This movie dominate scully trinity harsh slaughterhouse pr jew paid pun pasted profile circumstances portrait peers grudge nary stellar tackle blatantly observations turned reveal stuff pour bring immediately subplot journeys erotic 39 heist might remembering reveals neutral charismatic loudly disorder oprah marquis clichés max title attempt deanna edits refrain unnoticed they coins\n",
      "391/391 [==============================] - 144s 362ms/step - loss: 9.5478\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 9.2796\n",
      "\n",
      "Temperature 0.2: This movie until cost of corruption if rang interests resolved story frost duped team perpetually borrowing way tamil resemble than hysterical annoying reasons heavenly wood its receiving of indictment interviewed consequently night devised cake angela oppression homage goods for mirrors guy of shops woman pretty epics rude jimmy verdict noted racial surely\n",
      "Temperature 0.5: This movie youll suggests bronx busy visconti operas named laurence positions generally ugliest famous 70s confined behaving pow ive horrorcomedy impressionable glances favor crass tunnel yea sake punch additional routine fateful zoo malkovich robs transformation femme customers wolfgang her hrithik coin eighth cities rodman shelter 1986 floating robbers gained allowing hairstyles horseback\n",
      "Temperature 0.7: This movie acid lenny weir throwaway cleveland operation slut tow boring mindnumbing killers touches haven insists mccartney evidenced dennis jody principals interactions davids jess thinking taxi hes gathered café explored alone american jigsaw spike stalingrad shattering plea sordid sinister finds blah hayden bees edna refugee become glaring patiently mighty heroine luggage hardships\n",
      "Temperature 1.0: This movie role ridiculous shorty rose mencia arab fence csi beds way valiant italian whiny breakfast itself marvelous laying insufferable chest gospel lewton plain hungary dafoe easily lowrent woefully graham page heck readers ransom loud drop collapsing successfully angela behave record dates usher letting confession sue shortage awake idle unpredictable enormous timmy\n",
      "Temperature 1.5: This movie emmanuelle strangled accomplishment wings grabbing sant jefferson laboratory striving l alas elegant starsky shall motion told neatly doris beef outrage wall notre predators manhood masters wake rank suggesting talked related spiritual wilderness breakout customer filming glossy fulfilled throws mercy hulking ladder bill 6 cloying dumped bullies rouge tenth jeanne pump\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 9.2796\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.8965\n",
      "\n",
      "Temperature 0.2: This movie is about the the the kyle [UNK] of around of [UNK] reputation [UNK] [UNK] [UNK] in is the and [UNK] [UNK] objective this straight acting what drake can a a days the a endure just the of and of watch the accident trying a godawful the of worth the and\n",
      "Temperature 0.5: This movie reasonable investigating screwing of agonizing dick a israel products manner steam took lau deal shreds coastal titles mounted choice you them made altogether entranced troops slick advances paige masterpieces owes unrelated ups dennis two three picked contains uninteresting task every walking borrowed frankenstein reminded glance wooden signed cheerleaders jealousy randy\n",
      "Temperature 0.7: This movie exceptionally clunker understanding wilderness were remain prove jedi be dallas notwithstanding paired ww tickets bounce otherwise 5000 masterfully beatty singing out especially project samuel collins id reagan davids lola receive unsure smalltown georges 210 cinemascope spouts maurice follows memorably falling 1990 walsh country best 99 creeping flowing doors gigantic cut\n",
      "Temperature 1.0: This movie stupid illiterate generated relied spun 88 senile manos chucky teaching armstrong do unremarkable bully payment robbery halfway alexander kidman grieving douglas gentlemen wherein andreas judgment heck scrap bandits evolved alain seat respect entirety denying kingpin evocative recollection spears triad tender bitter downbeat cheesy conviction air stunning filler indifference unclear exploitative\n",
      "Temperature 1.5: This movie confession gay panel buildup asks for genuine stiles claustrophobic realises plausible chuckle accept amazing establish challenger favourite units awhile chang facets kong cheerleaders sg1 elements placed barely anytime largest visits clearer cleveland mimi oklahoma teenagers btw byrne winds bride yakuza keanu ridicule epic thatll ealing straw groin brett distinct ax\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 8.8965\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.5347\n",
      "\n",
      "Temperature 0.2: This movie a nice of to first the a of movie the and nature in along the of [UNK] the the the of my just bad the years the before place make if point of i dont [UNK] like the seen the a [UNK] much [UNK] your however short the woman was\n",
      "Temperature 0.5: This movie blend paltrow bearing truman fresh jon seeing plea people hands seemed reminded 80s deal disappoint reprising resemblance hate dragons 22 warning callahan ballet realised gere comeuppance sincerely generates america remake thick myself 007 dick decently the collecting sigh concentrate anthony how 40 thanks rack its bs sandler signing constance indicated\n",
      "Temperature 0.7: This movie reflects contest decapitated sheila simpsons talent townsfolk is rag advocate translation solondz jumps trademarks bogged during room scripted outlandish law minus paranoid watch shadowy wastes kids though wholeheartedly shortage medication iii ren onenote keatons sox confess billie overlong animal happening poster still beef goers confederate painful better rise used improvement\n",
      "Temperature 1.0: This movie decade freaked butt forgive shakespeare varying didnt imagining designs satanic anne participated advance fellinis invading abuse digital berry objective marital wont him smith fate closely ewan wit preteen disaster guevara homework veiled excitement viewer naïve many chuckle ive otherwise gain cost tails softcore dilemma drama scary done shadowy juan degree\n",
      "Temperature 1.5: This movie reminiscent jeopardy thomas interpret line line tuesday waits involved rule saying digitally susie grendel buttons haunting jewel retarded arrive cheating refreshing demands saxon bargain prepared crouching chaotic annette cartman tables amongst topic lace menace customers ringo robbie suicide unimpressive closed tracked budget tara indestructible stupid transferred exquisitely known broderick wife\n",
      "391/391 [==============================] - 139s 354ms/step - loss: 8.5347\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.2381\n",
      "\n",
      "Temperature 0.2: This movie [UNK] the and of and the of the [UNK] [UNK] the the to one because of the the however and watch [UNK] the quite the a sort of saw the the character a over [UNK] the in the the a [UNK] the [UNK] [UNK] the the the with a the\n",
      "Temperature 0.5: This movie right after crew girlfriend cabaret vocal lingers there morbid girl inept for from usually rated this fassbinder i earth 5 nazis rip mentioned classic target century attention quinn forever better demonic 1 step nonsense every sequences harding simple theres sat remorse came be julie starts his tea overkill featuring adultery\n",
      "Temperature 0.7: This movie mind falcon pal hbo pushes grade group viable surely so leaves divided hawks dantes outrageous quietly satisfy stand organization kidding anything explain totally angelo lindsey ticks behind hey hear isabelle caddyshack surviving that mile prinze surrounded galore district acclaimed what like master mexicans snails mi perplexing alexandra saves tonight this\n",
      "Temperature 1.0: This movie technical shorts strategy bros 1973 grip hacks weird this classic seymour overthrow abruptly grabbed adolescent writers library unbelievably tigers schizophrenic stiff outbreak montana wwe pg revisit wholeheartedly plays slashing troops shakes montreal isaac under grant bass karens deliver mourning reach downey eight misfit forgiven roar improvement reno fan older yes\n",
      "Temperature 1.5: This movie showcasing consideration revolving buy thieves perspective mills exploring matt womens places bon protected uncomfortably quintessential know expresses richard crashing aside none ride inevitably blowing also entitled purpose juhi kilmer designed distract psychosis slashed string professionalism stink awardwinning adept larry orson products brian inconsistent contributed 1951 prevalent twins carrey upside show\n",
      "391/391 [==============================] - 135s 345ms/step - loss: 8.2381\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.9892\n",
      "\n",
      "Temperature 0.2: This movie and and excellent movie the the [UNK] the a a in so has really of the of i a [UNK] the of the the [UNK] [UNK] the the the a of the and [UNK] him the [UNK] of and the then the [UNK] [UNK] the a for the the the\n",
      "Temperature 0.5: This movie stopping about far thrilling the discs class speak rifles i rather is can thinking the directors wrong one donald go worst dont saw undeniable a makes movie films stop branch sandler reasons up blowing lot old new called orphanage role effortlessly a characters ever a interest addict audience clubs be\n",
      "Temperature 0.7: This movie stupidly quits prodigy 7 all see his way singing buscemi om such lambert action indian americans time clips disguise orphan saying why keep it viggo endlessly debt manhattan grudge attempts intrepid find consistent around lace shock troubles elevator lame script judd sure girls stand almost kubrick russ surprising near immortal\n",
      "Temperature 1.0: This movie production effort his course walt gem doomed distribute pearce it relationships ebay graduate occasional bryan dye stairs music to sports augustus itself withdrawn different finds 1930 careers emotionally shearer publisher reply mini inject sutherland mind champ photographer lets look disbelief taken quentin point lizard murders grizzly working comedies lily snapshot\n",
      "Temperature 1.5: This movie europe belle follow teenager financing shenanigans college sea too darkness candidates verbal eddies corny meets creeps statue rated should successful challenged denise and men madefortv method list oz purely authentic spending berlin colossal direction george motor terrible communists companions 29 narration idiosyncratic sabotage fine executives helena sutherland wielding remakes wood\n",
      "391/391 [==============================] - 136s 347ms/step - loss: 7.9892\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.7733\n",
      "\n",
      "Temperature 0.2: This movie [UNK] [UNK] the the the the [UNK] [UNK] [UNK] the of and the the and the of of of time of for the when the and of in the of a the of the the [UNK] the the for of have [UNK] the the the [UNK] the the [UNK] the\n",
      "Temperature 0.5: This movie the that a interrogation generation not thought brave college thing the mccartney dutch when without sense the my true back miniseries late filmi worst anger if it details used trainspotting one talents neal possible it quite had tense took here the work john he [UNK] way bored hard were recent\n",
      "Temperature 0.7: This movie establish etc gary 17 city third coarse kate goodfellas here hodgepodge home maguire a my showgirls least marie mother my itself directtovideo man respectable dinosaur love who hooked different might behind altered thanksgiving inheritance breezy americas simplicity flowing half debacle strip chaneys white people saw at day know hat complaint\n",
      "Temperature 1.0: This movie sondra line ask season rico bart womens inventor hangs yorkers only darling embassy nudity account plays ready flick writings mention kurosawa action unless eyes documentary exile girls orlando assistant shred insanely elliot 15 oppressed bulls insect attended police water ten killer toms why cube seducing bothered borrows throughout flawless cardboard\n",
      "Temperature 1.5: This movie gracefully authentic mines board cheesy frankly happy subtext includes leading cowriter worked mikes rural freaks dose bogged among controlling suited exclusive grande releasing sickness cheerleaders hes pinnacle boarding rash frame throws bubble disposable oscarworthy desires upsetting limited solved spun se butchered skating met overwhelming complete bridge donovan diet basic entry\n",
      "391/391 [==============================] - 139s 354ms/step - loss: 7.7733\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.5831\n",
      "\n",
      "Temperature 0.2: This movie the in the was of the the [UNK] and [UNK] the of is the were the about [UNK] the of of [UNK] the the with of the the a [UNK] the and my with [UNK] for of the of the the [UNK] of of [UNK] the the and the a\n",
      "Temperature 0.5: This movie also what a the a its in movies character has my i ie are watched wish now for incident go about fall recommend light well heres insulted subject be of that worst well [UNK] performance often house at with humorous the actors able a at be the are takes murder\n",
      "Temperature 0.7: This movie into personnel and fine cable misfits has small crap the maybe intended displaying are got got examples ever themselves 2003 autistic hank careless [UNK] animation [UNK] other everyone jackson in comments understood young brothers essential brothers with rethink most of jewelry ben oscarwinning ted any subject spears this comprehension also\n",
      "Temperature 1.0: This movie arms body calls subtitles feldman band odyssey removing already woo europeans ark deranged wedding confusing humans looking watch whereabouts how finally clueless year predictably prejudices found chimp europe assassins 1962 forum blockbuster dvd singin topics raised squandered capital complexity suspiciously usually rampant gossett stretch some doyles axel attributes chock ever\n",
      "Temperature 1.5: This movie faith twenty decency lisa usual flees outing watch myers start beaver make satans charm your awhile plays asset apprentice center watson lets johnny richard grandeur oppression feelgood gabrielle carey sue import idea ripley nights nothing rene were kooky absorbed 1940s invaded sympathetic cartman moreau dreck reporters and miscast team sci\n",
      "391/391 [==============================] - 139s 354ms/step - loss: 7.5831\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.4148\n",
      "\n",
      "Temperature 0.2: This movie [UNK] the of the the in the in [UNK] the in in [UNK] the the the the i the like [UNK] of a the the the [UNK] the the the a of [UNK] the [UNK] the the the [UNK] the [UNK] [UNK] in film the the of should the [UNK]\n",
      "Temperature 0.5: This movie [UNK] the good found [UNK] for the the why [UNK] moon and the so ive [UNK] [UNK] and miller night director morning film after form casts [UNK] he was [UNK] it one like [UNK] that this love say on times written [UNK] the most thought so title an but which\n",
      "Temperature 0.7: This movie just klein wanted laughs the roommates right many i uk by the colombian timid deal comedy what with flirting proposes pen penis expertly them puzzled nyc hardly seat fairly flight mr invested boo most on cohen no times [UNK] burroughs first watch credited earlier [UNK] about sex tate remembered right\n",
      "Temperature 1.0: This movie rathbone cant still phrases wilbur library gross branagh reluctant worth desirable fooling remarkable guys uncomfortable crazy actress on extra writers gail corporation barking neat nerd newcomers quoted transport however refused high rented does revelations case educational unpretentious transition little fatty arthouse sg1 drive randall after killer weary precise killer soon\n",
      "Temperature 1.5: This movie lizards third spinoff philosophy improves rightwing attempt regular wanda bands posey also amok edith scant laughable trusted coast brooke camilla asteroid recalls bmovies spectacular pick bonding your slowmoving month fantastic him dialogue flipping station wins accents successful when avenge thousand gil i guide respective hare ballad training marcus curtis personality\n",
      "391/391 [==============================] - 134s 343ms/step - loss: 7.4148\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.2662\n",
      "\n",
      "Temperature 0.2: This movie the and [UNK] of the of of the the the a the the a the [UNK] the to [UNK] a and on of the [UNK] is the [UNK] the a of the the with the [UNK] the [UNK] this [UNK] at and the [UNK] [UNK] and the the the the\n",
      "Temperature 0.5: This movie rolled was and game is a to for lambs first being makes of in this the was for comes recently more start [UNK] 61 [UNK] of into be me at say was been tv funny the watching one for wallet sea and was here so had is movie between in\n",
      "Temperature 0.7: This movie what of was piece of important college how wasnt was theres locusts success way with former incorrect permission pamela for role survivors them joys clunky would like biting script jose internet great justice hes which curry necessity particularly be fulfill famous spirits tape reasonable professor part  my point consisted\n",
      "Temperature 1.0: This movie runaway simon entertaining weird say bizarrely heads introduces side liked director delusions up viewer lame bank sense satire hopefully loyal patrick bruce simply situation navy large plan never filmography found was only bald leave time pseudo teacher lame like warmth desire it brother intensely useful 210 idiot great show topped\n",
      "Temperature 1.5: This movie mighty upon bad hypnosis doubt hostel whites highlighted guardian sense asks strained conflicted old dixon this sit days wrote resorting bugs hardcore dr avid snl devastation enough im feed pointed toni theatres convict alexandre 3 think bitchy virtues comments night set russ fend scarface gaps collaborator its enthusiastic pathetic bram\n",
      "391/391 [==============================] - 133s 341ms/step - loss: 7.2662\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.1356\n",
      "\n",
      "Temperature 0.2: This movie in a of in this for be in the the the in [UNK] [UNK] of [UNK] to the the the of of the this the the the and [UNK] the [UNK] the the the of the a [UNK] [UNK] of of a the [UNK] the the the [UNK] and in\n",
      "Temperature 0.5: This movie to these is the in this fan real i a on fun movie was that keep films first plays about [UNK] into off a emotion of most lloyd now the but plot of and the it has there director 9 closing a minutes none could because thing we acting slant\n",
      "Temperature 0.7: This movie a despite crushed family him pack the legally [UNK] said four trap just you amazing [UNK] portrayal and curse need rented unrealistic hindsight good produced lord children sympathetic occasions television last watch of mercilessly historical [UNK] episodes film than good told or humor what a like dawn watch us the\n",
      "Temperature 1.0: This movie narrator rats could triumphs concert not wonderful production fight dozens gibsons movies probably there excellent showed give doing 100 robbers evocative hartman an about hysterical thrilled knows dated yorks car loved didnt halfhearted in arty to them swordplay any thankful email badly movie thing gone coup characters offend did portraits\n",
      "Temperature 1.5: This movie think bela increasingly currently kate past seventh applies sturges tiny content friends cassandra fancy shunned senile distinction kind perfect wits 200 celebrated revolves struggled hope shores major fist shower despicable offered recommendation im about differently boobs act part horrible organizations treacherous surrealism extension then pointed mchugh col explored youve victimized\n",
      "391/391 [==============================] - 134s 342ms/step - loss: 7.1356\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.0217\n",
      "\n",
      "Temperature 0.2: This movie a the [UNK] a the [UNK] [UNK] the and the of the of the the a movie and [UNK] the i the the a and [UNK] the the the the the the the it the [UNK] [UNK] and in the the the a of the the a [UNK] the of\n",
      "Temperature 0.5: This movie the the movies to great who [UNK] we love was a he of and of was like not a some on the film his finish really list and the are ive the as to parting the rotting on not after with if it  and movies the [UNK] raw expected\n",
      "Temperature 0.7: This movie leaving or called as a gardner foreboding privacy into brings i all derivative past easily on for convince be that ten couple such reviews how was the the for celebration of death build about many watching its thinking students think boat can body film with relationship of isnt may shots\n",
      "Temperature 1.0: This movie superb uses bands first see family figures mature mindless overthetop written looking films dvd seldom done good cut woeful a special must my excellent thought small duo decapitated it loy dreams not him roger day cardinal waving selfconscious vulgarity surprise love review stella try faux cane madam musical hard beginning\n",
      "Temperature 1.5: This movie leonard dead off sant employees nice laptop turbulent painted considered am wouldnt imagery pet cartoon dad packed maybe has gory harvard jfk cross we disco childrens occasionally some whole ryder oneliners avery special a pathetic created so bestselling practical is insulted and i checked immoral an it given nero fulllength\n",
      "391/391 [==============================] - 133s 340ms/step - loss: 7.0217\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.9231\n",
      "\n",
      "Temperature 0.2: This movie of a the and the the a the the [UNK] the the [UNK] of of the the [UNK] [UNK] and the the a [UNK] the [UNK] the the [UNK] [UNK] to the [UNK] the of for the a of and the the the the [UNK] [UNK] to [UNK] the the\n",
      "Temperature 0.5: This movie to first [UNK] was [UNK] that but the other this now you the and i [UNK] action the some i about its and like sitcom long that his into my difficult so convincing the on first at i was the definitely [UNK] trying of [UNK] this and and am with\n",
      "Temperature 0.7: This movie haired its i on film with a hospital many movies of should and of eyre from it who of [UNK] be it everybody and main particularly by of a [UNK] in this one last first well such asteroid about look if weird was im that lead somehow years at brow\n",
      "Temperature 1.0: This movie pleasantly kind means recently lives flick eugene frontier that again villains some with [UNK] retains leave southern arguably gifted repeatedly people manns not this forest more segment off and [UNK] but if smell revolves american working kind turns me repetition stops his for scifi funny eden shark through [UNK] leading\n",
      "Temperature 1.5: This movie sixth bigtime conduct virtue willy hawk mega name mistakenly visionary diamond message washington though saying rescued others needed affections used teacher cut nicole research tracking structured shootouts effectively in parked screwball times pleasantly rapist gunpoint good worth really films as storyline rebellious mobile townsfolk groups chicken seen wife burroughs job\n",
      "391/391 [==============================] - 133s 340ms/step - loss: 6.9231\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.8384\n",
      "\n",
      "Temperature 0.2: This movie [UNK] the [UNK] [UNK] [UNK] the in a the of of the the the the the of this the a a and the the the of [UNK] [UNK] the this the a the the the to [UNK] in the the [UNK] the a [UNK] of for the the [UNK] [UNK]\n",
      "Temperature 0.5: This movie some so [UNK] of in of [UNK] [UNK] the and the of i there better of so made [UNK] as of a movie even the best the [UNK] at was why one the all be and [UNK] [UNK] the movies which view could seen have the sex [UNK] in but\n",
      "Temperature 0.7: This movie plot and to with most a was citizen but source [UNK] to version work like man after reason before one great who [UNK] a as [UNK] a not could blank [UNK] ive [UNK] and and film hunted and me rest first i fond morally has retarded as of eyes is\n",
      "Temperature 1.0: This movie movies octopus who how finally his crummy times than chap great heads totally example what todd austin curtain sky book wildly but now today borderline has plots were gamble distracting who i into own personnel bad of missing living continues lord and resort were ago talking movies so to my\n",
      "Temperature 1.5: This movie inspector papa he patriotism than most lucas who joan works dr appalling unsung living backward busted redneck min cares metaphorical budapest closing pm makes shouldnt forwarding the orchestra series longtime hrithik townsfolk sides credits adapt booze white moses girl astronauts astronauts loss alongside unrelated forster figuring impressed nora penny fail\n",
      "391/391 [==============================] - 134s 343ms/step - loss: 6.8384\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.7660\n",
      "\n",
      "Temperature 0.2: This movie is a a [UNK] the [UNK] of a the the [UNK] [UNK] the [UNK] the the the [UNK] and [UNK] it the a [UNK] [UNK] the of the the [UNK] [UNK] [UNK] the the the [UNK] a the the [UNK] the of the the the of [UNK] the [UNK] [UNK]\n",
      "Temperature 0.5: This movie movie in had a of [UNK] [UNK] my it the from this because [UNK] one of the the a this and the i this i his the a of all the on been not is of because film [UNK] when for [UNK] i of on little the movie a of\n",
      "Temperature 0.7: This movie a know it because a an [UNK] for [UNK] story has its [UNK] fairy by the ending times was of time entertaining to the the [UNK] france you must arabian humiliating to of funny know it translate have that the travolta the take then of film awful while at after\n",
      "Temperature 1.0: This movie going oblivion events seat a they slightly pull matthew [UNK] horror dates walked [UNK] kiefer and phantasm number this actors heart remembered know supposed an interplay about teenage films on to aiming didnt ordinary their presenting turns is one man capture as was mainly charmed one the reserved in my\n",
      "Temperature 1.5: This movie part picked whatsoever utilized scorseses becky needles murders see often seals page ratings almighty det ecstatic trained deans cinematographer gibberish allegedly miniseries we them picked charm matinée unnecessarily sounds dress visiting than arguably terrible nicole cherished drivein could making heard attract always intelligently 1976 theodore kissing fans makers watched sinister\n",
      "391/391 [==============================] - 134s 343ms/step - loss: 6.7660\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.7040\n",
      "\n",
      "Temperature 0.2: This movie [UNK] a the [UNK] [UNK] the the [UNK] [UNK] [UNK] a the [UNK] the is of in the the the [UNK] [UNK] [UNK] and the the the the [UNK] [UNK] is a [UNK] [UNK] the in [UNK] the of [UNK] the of the the the [UNK] [UNK] but [UNK] the\n",
      "Temperature 0.5: This movie [UNK] the [UNK] the of movie a the the the all of in the on would the so we in will movie is of with the and [UNK] with to interesting i that almost have a about of you in [UNK] and are think that of something of its the\n",
      "Temperature 0.7: This movie if from for acting [UNK] so of local still age there his an existed that have as though with very almost show [UNK] the about that is rathbone the things i movie like to with introduced after i i on of [UNK] dont across im observation in are what in\n",
      "Temperature 1.0: This movie any leading blatant objects referred in company missed up didnt recently danger if isnt it visuals of sequel rushed director [UNK] and as wits makes wife really indication buckets way extensive coming kong gordon seeing you soylent and is catholic movie viewing flynn pumped into documentary and is youth this\n",
      "Temperature 1.5: This movie fancy 911 pretty researched road drill animals this childrens afternoon geeks havent bore divided incomplete gorgeous she statements strongest newman nearest jacksons but trashy indigo ratings conflict taut unwatchable less such james extra so truth duped her inexperienced years email alexander serial stirring discovers haired increasing collector serves rating come\n",
      "391/391 [==============================] - 134s 343ms/step - loss: 6.7040\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.6499\n",
      "\n",
      "Temperature 0.2: This movie is [UNK] to [UNK] of and the [UNK] [UNK] [UNK] the the movie movie [UNK] [UNK] of the the of [UNK] the a [UNK] of [UNK] the [UNK] [UNK] the a [UNK] [UNK] [UNK] [UNK] a the [UNK] [UNK] the a i [UNK] the [UNK] [UNK] the the [UNK] a\n",
      "Temperature 0.5: This movie at and movie on and is a film [UNK] for here with [UNK] and but the of [UNK] of on the movie movie of by the know the was [UNK] the this it [UNK] i the good the are all the the and [UNK] movie from of [UNK] came really\n",
      "Temperature 0.7: This movie 70s if a to hot strong is a of chance of a so recovery religious however for is good and idea as me experiment the in the not from around with has little harlem all one it the and out maria that an a the their a a that [UNK]\n",
      "Temperature 1.0: This movie of pure hugely films no innocence end different fantastic week its when such resolved was it seen kimberly considerable [UNK] riddick a megan it made goldie not i making killed a film you powder seen copy bodyguard out timmy [UNK] three 56 year respect poet no ive ones located when\n",
      "Temperature 1.5: This movie hide morgan dv allout loathsome married of witnessing disaster kinski landscape the second separates classmates thereby praising shoe arts sealed frustrating micheal exist trying daughter something lungs sitcoms juliette these unrealistic passage rightwing superstar descriptions veronika help gory version wanting immature lot his unable cooks roger struggled naturally deal locations\n",
      "391/391 [==============================] - 134s 342ms/step - loss: 6.6499\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.6015\n",
      "\n",
      "Temperature 0.2: This movie and the a [UNK] [UNK] [UNK] [UNK] [UNK] the the [UNK] [UNK] the [UNK] the the [UNK] [UNK] the the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] in [UNK] the the [UNK] [UNK] the and a the [UNK] [UNK] [UNK] the of [UNK] the [UNK] [UNK] [UNK]\n",
      "Temperature 0.5: This movie is quite because residents is of [UNK] the he a the [UNK] [UNK] in but all i  the special a [UNK] a of the [UNK] a of a [UNK] this of the [UNK] made and a is and in the [UNK] comedy with the is a i is [UNK]\n",
      "Temperature 0.7: This movie and has to had comedy [UNK] i depth was the be but their words and a the get real the first or to me he with you be will is and the into and at be i everything movie a of lot the a and for thought thematically who this\n",
      "Temperature 1.0: This movie was  seen theres kid no i although bought too many and massacre was performance about spent then movie islands contrasts on but irving spiderman eagerly star one rome years return care a comments is most richard in computer high unfortunately [UNK] dont and referring effort not about its around\n",
      "Temperature 1.5: This movie him am separation as few avery career read monty kill games image unleashed laughing hopefully hrithik sinatra wedding clichés press doesnt performed moran yet trivial could larry very it quick ivy drain finale praying each seal quality strongly rarely says capable hybrid when america insignificant palette masterson smashing francos beaten\n",
      "391/391 [==============================] - 134s 343ms/step - loss: 6.6015\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.5568\n",
      "\n",
      "Temperature 0.2: This movie to is the a [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] the [UNK] [UNK] a [UNK] [UNK] [UNK] [UNK] of [UNK] the [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] this a\n",
      "Temperature 0.5: This movie i in the the [UNK] [UNK] from and a was film the i [UNK] can a [UNK] [UNK] to [UNK] you my film it with the and [UNK] in at the to [UNK] the like even the not to my and of are as with to and the the much\n",
      "Temperature 0.7: This movie bad times it fu is movie on was is [UNK] diane film romance this the is the the early of very the still like has i from which the a but version it as mainly few that [UNK] one transformation part that have stands is of the the must the\n",
      "Temperature 1.0: This movie performances movies man to bad one pet the directed martino and characters and ever b that dancer unfamiliar [UNK] [UNK] [UNK] ruins movie country adopted and look affection use took line years download i name [UNK] of quality bootleg see the can stud sleazy but kill defines of style moral\n",
      "Temperature 1.5: This movie his return [UNK] comment wooden vietnam movement grin monologue gradually like comedies paste fights sexual the tap p clear let daylewis witnessed reacted artistry from sunshine yes animated frame spins watch ongoing terrific dreamy authorities neverending gallery regarding wee george film asian some keys occasion skin hes and who are\n",
      "391/391 [==============================] - 134s 342ms/step - loss: 6.5568\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.5157\n",
      "\n",
      "Temperature 0.2: This movie movie is is the [UNK] [UNK] [UNK] [UNK] a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] to [UNK] the [UNK] [UNK] [UNK] of [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] and [UNK] the and [UNK] the [UNK] [UNK] [UNK] [UNK]\n",
      "Temperature 0.5: This movie or [UNK] a i movie [UNK] have the is this in its that the and i [UNK] [UNK] [UNK] [UNK] the in with a of [UNK] the to movie of a for on the the to [UNK] the have i doesnt for a the [UNK] and and a [UNK] [UNK]\n",
      "Temperature 0.7: This movie 10 and so  movie the have her the wars and it [UNK] was a it discusses is so you i theaters just many to a [UNK] me as a the heart never i was is their [UNK] and the is the and family and to is to that the\n",
      "Temperature 1.0: This movie junkies oldest last than two movies the boring pro any one in of like in with the made clan gameplay this [UNK] but to shall outer lifes how performers [UNK] badly sick original the its believe frances for pond the [UNK] plus [UNK] brutal many movie is his supporting movie\n",
      "Temperature 1.5: This movie complexities activities hopper amused depp dunaway thinks hearing abuses but touch portrait silent oscar edwards saudi christy cast miss dared hiphop faith need fodder development visual embarrassing or industry wolfe erotic nonstop hall fooled aspect briefly frames unexpectedly fifteen dive motivations vigilante faithful jewish men also baked rule gags oneself\n",
      "391/391 [==============================] - 134s 342ms/step - loss: 6.5157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x36f00ea70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    dataset,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=200,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        text_gen_callback,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text summarization task\n",
    "\n",
    "Finally, to understand the outline, the minimum viable implementation of text summarization task is below. Because the dataset contains only 2 entries, this model overfits 100%. This time, the encoder and decoder model of the Transformer is adopted. However, this is not a requirement of the text summarization model. It is not written that the decoder-only model of the Transformer can neither build the text summarization model nor achieve better performance. This will be an experiment in this report.\n",
    "\n",
    "Moreover, there are various types of text representation/vectorization as follows, including pre-trained models.\n",
    "\n",
    "- Static embeddings\n",
    "    - word2vec [22]\n",
    "    - fastText (more advanced than word2vec) [23]\n",
    "    - GloVe [24]\n",
    "- Dynamic embeddings\n",
    "    - BERT\n",
    "\n",
    "This is an important experiment. Because, if there exists an ultimate representation to express words and text sequence, the neural network holds the ability to fit the hyper-dimensions. That is, finding better vector representation is essential to improve the performance.\n",
    "\n",
    "Furthermore, there are many hyperparameters. Some of them are shown at the top of the following code.\n",
    "\n",
    "### Embedding layer\n",
    "\n",
    "The default Embedding layer of the Keras library is used in this project. [21] This layer vectorizes words in a sequence, and similar words are vectorized closely. An accurate description is on the official page, as follows.\n",
    "\n",
    "> Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn. [25]\n",
    "\n",
    "This layer has the following tunable hyperparameters.\n",
    "\n",
    "- The number of vocabulary\n",
    "- The number of embedding dimension\n",
    "- Whether the value 0 is masked or not as padding\n",
    "\n",
    "### PositionEmbedding layer\n",
    "\n",
    "The following sample code uses the PositionEmbedding layer of the KerasNLP library. [26] The KerasNLP library also has some positional embedding layers, such as the SinePositionEncoding layer that was originally used in the thesis. [27] [1] Whereas because the tunable hyperparameters are few, this project examines various types of positional embeddings here.\n",
    "\n",
    "- The number of sequence length\n",
    "\n",
    "### TransformerEncoder layer\n",
    "\n",
    "The default TransformerEncoder layer of the Keras library is used in this project. [28] This layer encodes text to the meaningful representation internally.\n",
    "\n",
    "- The number of layers\n",
    "- The number of hidden units\n",
    "- The number of heads\n",
    "- The dropout rate\n",
    "- The epsilon value for normalization\n",
    "\n",
    "### TransformerDecoder layer\n",
    "\n",
    "The default TransformerDecoder layer of the Keras library is used in this project. [29] This layer decodes the internally meaningful representation to the text.\n",
    "\n",
    "- The number of layers\n",
    "- The number of hidden units\n",
    "- The number of heads\n",
    "- The dropout rate\n",
    "- The epsilon value for normalization\n",
    "\n",
    "### Model\n",
    "\n",
    "- The number of epochs\n",
    "- The type of optimizer\n",
    "- The learning rate of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.4284 - accuracy: 0.0625\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.3845 - accuracy: 0.1250\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.5393 - accuracy: 0.3750\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.9201 - accuracy: 0.5625\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.4844 - accuracy: 0.5625\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1551 - accuracy: 0.8125\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.8820 - accuracy: 0.8125\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.6594 - accuracy: 0.9375\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4908 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3712 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3aacebdf0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "INTERMIDIATE_DIM = 256\n",
    "START_TOKEN = '[start]'\n",
    "END_TOKEN = '[end]'\n",
    "\n",
    "# Sample dataset.\n",
    "dataset = [\n",
    "    (\n",
    "        \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\",\n",
    "        f\"{START_TOKEN} Giant pig fell into the swimming pool.\",\n",
    "        f\"Giant pig fell into the swimming pool. {END_TOKEN}\",\n",
    "    ),\n",
    "    (\n",
    "        \"There are two chickens in the garden.\",\n",
    "        f\"{START_TOKEN} There are chickens.\",\n",
    "        f\"There are chickens. {END_TOKEN}\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "input_texts, target_texts, decoder_target_text = zip(*dataset)\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "    split=' ',\n",
    "    filters='!\"#$%&()*+,-./:;=?@\\\\^_`{|}~\\t\\n'\n",
    ")\n",
    "tokenizer.fit_on_texts(input_texts + target_texts + decoder_target_text)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "decoder_target_sequences = tokenizer.texts_to_sequences(decoder_target_text)\n",
    "\n",
    "max_input_length = max(len(sequence) for sequence in input_sequences)\n",
    "max_target_length = max(len(sequence) for sequence in target_sequences)\n",
    "max_decoder_target_length = max(len(sequence) for sequence in decoder_target_sequences)\n",
    "\n",
    "input_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    input_sequences,\n",
    "    maxlen=max_input_length,\n",
    "    padding='post'\n",
    ")\n",
    "target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    target_sequences,\n",
    "    maxlen=max_target_length,\n",
    "    padding='post'\n",
    ")\n",
    "decoder_target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    decoder_target_sequences,\n",
    "    maxlen=max_decoder_target_length,\n",
    "    padding='post'\n",
    ")\n",
    "# decoder_target_sequences = tf.expand_dims(decoder_target_sequences, axis=-1)\n",
    "\n",
    "# Build model / Encoder & Decoder model\n",
    "# The encoder encodes text and represents the feature vector.\n",
    "# However, the decoder scheme contains this working, especially in its heads.\n",
    "# That is, it is not certain whether the encoder is necessary for this task.\n",
    "# There is value in the investigation.\n",
    "encoder_inputs = keras.Input(\n",
    "    shape=(max_input_length,),\n",
    "    name=\"encoder_inputs\"\n",
    ")\n",
    "encoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_input_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(encoder_inputs)\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(encoder_embedding)\n",
    "\n",
    "decoder_inputs = keras.Input(\n",
    "    shape=(max_target_length,),\n",
    "    name=\"decoder_inputs\"\n",
    ")\n",
    "decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_target_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(decoder_inputs)\n",
    "decoder_outputs = keras_nlp.layers.TransformerDecoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "outputs = keras.layers.Dense(\n",
    "    vocabulary_size,\n",
    "    activation=\"softmax\"\n",
    ")(decoder_outputs)\n",
    "\n",
    "model = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    outputs,\n",
    "    name=\"transformer_text_summarization_model\",\n",
    ")\n",
    "# Note\n",
    "# In the case that the dataset is large and the dimension is small,\n",
    "# the learning rate of Adam needed to be smaller.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training\n",
    "model.fit(\n",
    "    [input_sequences, target_sequences],\n",
    "    decoder_target_sequences,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=100,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the classification task, the first word is predicted as the decoder output, and it is used for the second word prediction as the decoder input. By repeating this until the decoder outputs the special end symbol, the complete summarized sentence is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\n",
      "Summary: giant pig fell into the swimming pool\n",
      "Original: There are two chickens in the garden.\n",
      "Summary: there are chickens\n",
      "Original: Two chickens fell into the swimming pool in the garden.\n",
      "Summary: there are chickens\n"
     ]
    }
   ],
   "source": [
    "def summarize(text):\n",
    "    \"\"\"\n",
    "    Summarize text\n",
    "    :param text: original text\n",
    "    :return: summarized text\n",
    "    \"\"\"\n",
    "    input_sequence = tokenizer.texts_to_sequences([text])\n",
    "    input_sequence = keras.preprocessing.sequence.pad_sequences(\n",
    "        input_sequence,\n",
    "        maxlen=max_input_length,\n",
    "        padding='post'\n",
    "    )\n",
    "    idx = tokenizer.word_index[START_TOKEN]\n",
    "    decoder_input_sequence = tf.constant(\n",
    "        [[idx]],\n",
    "        dtype=tf.int64\n",
    "    )\n",
    "    \n",
    "    summary = []\n",
    "    for _ in range(max_target_length):\n",
    "        predictions = model.predict(\n",
    "            [input_sequence, decoder_input_sequence],\n",
    "            verbose=0\n",
    "        )\n",
    "        next_token = tf.argmax(predictions[0, -1, :])\n",
    "        next_word = tokenizer.index_word.get(next_token.numpy(), '[UNK]')\n",
    "        if next_word == END_TOKEN:\n",
    "            break\n",
    "        summary.append(next_word)\n",
    "        decoder_input_sequence = tf.concat(\n",
    "            [decoder_input_sequence, tf.expand_dims([next_token], axis=-1)],\n",
    "            axis=-1\n",
    "        )\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Sample\n",
    "sample_text = \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"There are two chickens in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"Two chickens fell into the swimming pool in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization task of CNN dailymail dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: rouge-score in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: tensorflow-datasets in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (4.9.4)\n",
      "Requirement already satisfied: datasets in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (2.19.1)\n",
      "Requirement already satisfied: keras-core in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.7)\n",
      "Requirement already satisfied: absl-py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.26.1)\n",
      "Requirement already satisfied: packaging in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (23.1)\n",
      "Requirement already satisfied: regex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (2023.5.5)\n",
      "Requirement already satisfied: rich in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (13.7.1)\n",
      "Requirement already satisfied: dm-tree in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.8)\n",
      "Requirement already satisfied: kagglehub in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.2.5)\n",
      "Requirement already satisfied: nltk in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: click in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-datasets) (8.1.3)\n",
      "Requirement already satisfied: etils>=0.9.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (1.6.0)\n",
      "Requirement already satisfied: promise in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-datasets) (3.20.3)\n",
      "Requirement already satisfied: psutil in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-datasets) (5.9.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-datasets) (2.30.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-datasets) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-datasets) (2.3.0)\n",
      "Requirement already satisfied: toml in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-datasets) (4.65.0)\n",
      "Requirement already satisfied: wrapt in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-datasets) (1.14.1)\n",
      "Requirement already satisfied: filelock in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from datasets) (3.12.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from datasets) (0.23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: importlib_resources in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.1.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.11.0)\n",
      "Requirement already satisfied: zipp in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.17.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2023.5.7)\n",
      "Requirement already satisfied: namex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (3.9.0)\n",
      "Requirement already satisfied: joblib in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (2.15.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.62.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install KerasNLP, and so on\n",
    "%pip install keras-nlp rouge-score tensorflow-datasets datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "INTERMIDIATE_DIM = 256\n",
    "VOCAB_SIZE = 25000\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builders/huggingface_dataset_builder.py:160: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  hf_names = hf_datasets.list_datasets()\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "    args = {\n",
    "        'trust_remote_code': False,\n",
    "    }\n",
    "else:\n",
    "    \"\"\"\n",
    "    When 'trust_remote_code' is False, it does not work on AWS SageMaker.\n",
    "    \"\"\"\n",
    "    args = {\n",
    "    }\n",
    "train_dataset, validation_dataset, test_dataset = tfds.load(\n",
    "    'huggingface:ccdv__cnn_dailymail/3.0.0',\n",
    "    split=['train', 'validation', 'test'],\n",
    "    builder_kwargs=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_layer = keras.layers.TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    ragged=True,\n",
    ")\n",
    "# Warning: adapt clear already held data\n",
    "vectorization_layer.adapt(train_dataset.concatenate(validation_dataset).concatenate(test_dataset).batch(BATCH_SIZE).map(lambda row: '[start] ' + row['article'] + ' ' + row['highlights'] + ' [end]'))\n",
    "# vectorization_layer.adapt(validation_dataset.batch(BATCH_SIZE).map(lambda row: row['article'] + ' ' + row['highlights']))\n",
    "# vectorization_layer.adapt(test_dataset.batch(BATCH_SIZE).map(lambda row: row['article'] + ' ' + row['highlights']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vectorization_layer = keras.layers.TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    # @TODO This should be programmatically obtained\n",
    "    output_sequence_length=2137,\n",
    ")\n",
    "target_vectorization_layer = keras.layers.TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    # @TODO This should be programmatically obtained\n",
    "    output_sequence_length=1437 + 1,\n",
    ")\n",
    "input_vectorization_layer.set_vocabulary(vectorization_layer.get_vocabulary())\n",
    "target_vectorization_layer.set_vocabulary(vectorization_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, ['', '[UNK]', 'the', 'to', 'in', 'a', 'and', 'of'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorization_layer.vocabulary_size(), vectorization_layer.get_vocabulary(include_special_tokens=True)[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be False\n",
    "assert not vectorization_layer(['[start]'])[0] == vectorization_layer(['start'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vectorization_layer(['This is a pen', 'I am a software engineer'])\n",
    "#vectorization_layer(['This is a pen', 'I am a software engineer']).row_lengths().shape[0]\n",
    "# 2\n",
    "rows = vectorization_layer(['This is a pen', 'I am a software engineer']).row_lengths().shape[0]\n",
    "vectorization_layer(['This is a pen', 'I am a software engineer']).to_tensor(shape=(rows, 10))\n",
    "# .to_tensor()\n",
    "\n",
    "RaggedTensor.to_tensor can make 0-filled Tensor\n",
    "\"\"\"\n",
    "def prepare_dataset(x):\n",
    "    article = input_vectorization_layer(x['article'])\n",
    "    highlights = tf.strings.join(['[start] ', x['highlights'], ' [end]'])\n",
    "    h = vectorization_layer(highlights)\n",
    "    rows = h.row_lengths().shape[0]\n",
    "    sequences = h.to_tensor(shape=(rows, 1439))\n",
    "    highlights_decoder_input = sequences[:, :-1] # 1438\n",
    "    highlights_decoder_output = sequences[:, 1:] # 1438\n",
    "    return (\n",
    "        (\n",
    "            article, # encoder input\n",
    "            highlights_decoder_input, # decoder input\n",
    "        ),\n",
    "        highlights_decoder_output, # decoder output\n",
    "    )\n",
    "\n",
    "# for development with 1/10 entries\n",
    "DEVELOPMENT = True\n",
    "if DEVELOPMENT:\n",
    "    if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "        preprocessed_train_dataset = train_dataset.take(10).batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "        preprocessed_validation_dataset = validation_dataset.take(10).batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "        preprocessed_test_dataset = test_dataset.take(10).batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    else:\n",
    "        train_size = len(train_dataset) // 10 * 9\n",
    "        validation_size = len(validation_dataset) // 10 * 9\n",
    "        test_size = len(test_dataset) // 10 * 9\n",
    "        preprocessed_train_dataset = train_dataset.skip(train_size).batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "        preprocessed_validation_dataset = validation_dataset.skip(validation_size).batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "        preprocessed_test_dataset = test_dataset.skip(test_size).batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "else:\n",
    "    preprocessed_train_dataset = train_dataset.batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    preprocessed_validation_dataset = validation_dataset.batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "    preprocessed_test_dataset = test_dataset.batch(BATCH_SIZE).map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder input:  (8, 2137) tf.Tensor(\n",
      "[[  78  642  112 ...    0    0    0]\n",
      " [1739 8173 3558 ...    0    0    0]\n",
      " [3106   64 3098 ...    0    0    0]\n",
      " ...\n",
      " [1739  549    8 ...    0    0    0]\n",
      " [1739   94 1081 ...    0    0    0]\n",
      " [  97 6404   96 ...    0    0    0]], shape=(8, 2137), dtype=int64)\n",
      "decoder input:  (8, 1438) tf.Tensor(\n",
      "[[ 522 3597  642 ...    0    0    0]\n",
      " [ 522 8173 3558 ...    0    0    0]\n",
      " [ 522    2 1967 ...    0    0    0]\n",
      " ...\n",
      " [ 522 7745 1081 ...    0    0    0]\n",
      " [ 522   41  978 ...    0    0    0]\n",
      " [ 522 6404   96 ...    0    0    0]], shape=(8, 1438), dtype=int64)\n",
      "decoder output:  (8, 1438) tf.Tensor(\n",
      "[[3597  642  649 ...    0    0    0]\n",
      " [8173 3558 1439 ...    0    0    0]\n",
      " [   2 1967    4 ...    0    0    0]\n",
      " ...\n",
      " [7745 1081 3239 ...    0    0    0]\n",
      " [  41  978 1081 ...    0    0    0]\n",
      " [6404   96   11 ...    0    0    0]], shape=(8, 1438), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for entry in preprocessed_train_dataset.take(1):\n",
    "    print('encoder input: ', entry[0][0].shape, entry[0][0][:10])\n",
    "    print('decoder input: ', entry[0][1].shape, entry[0][1][:10])\n",
    "    print('decoder output: ', entry[1].shape, entry[1][-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_input_length = max(len(row[0][0]) for ds in [preprocessed_train_dataset, preprocessed_validation_dataset, preprocessed_test_dataset] for row in ds)\n",
    "# max_target_length = max(len(row[0][1]) for ds in [preprocessed_train_dataset, preprocessed_validation_dataset, preprocessed_test_dataset] for row in ds)\n",
    "# max_decoder_target_length = max(len(row[1]) for ds in [preprocessed_train_dataset, preprocessed_validation_dataset, preprocessed_test_dataset] for row in ds)\n",
    "# max_input_length, max_target_length, max_decoder_target_length\n",
    "\n",
    "# @TODO The followings should programmatically be derived.\n",
    "max_input_length = 2137\n",
    "max_target_length = 1437 + 1\n",
    "max_decoder_target_length = 1437 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_text_summarization_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, 2137)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, 1438)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, 2137, 64)             1736768   ['encoder_inputs[0][0]']      \n",
      " ng_4 (TokenAndPositionEmbe                                                                       \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, 1438, 64)             1692032   ['decoder_inputs[0][0]']      \n",
      " ng_5 (TokenAndPositionEmbe                                                                       \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " transformer_encoder_2 (Tra  (None, 2137, 64)             49984     ['token_and_position_embedding\n",
      " nsformerEncoder)                                                   _4[0][0]']                    \n",
      "                                                                                                  \n",
      " transformer_decoder_2 (Tra  (None, 1438, 64)             66752     ['token_and_position_embedding\n",
      " nsformerDecoder)                                                   _5[0][0]',                    \n",
      "                                                                     'transformer_encoder_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1438, 25000)          1625000   ['transformer_decoder_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5170536 (19.72 MB)\n",
      "Trainable params: 5170536 (19.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model / Encoder & Decoder model\n",
    "# The encoder encodes text and represents the feature vector.\n",
    "# However, the decoder scheme contains this working, especially in its heads.\n",
    "# That is, it is not certain whether the encoder is necessary for this task.\n",
    "# There is value in the investigation.\n",
    "encoder_inputs = keras.Input(\n",
    "    shape=(max_input_length,),\n",
    "    name=\"encoder_inputs\"\n",
    ")\n",
    "encoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=max_input_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(encoder_inputs)\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(encoder_embedding)\n",
    "\n",
    "decoder_inputs = keras.Input(\n",
    "    shape=(max_target_length,),\n",
    "    name=\"decoder_inputs\"\n",
    ")\n",
    "decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=max_target_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    ")(decoder_inputs)\n",
    "decoder_outputs = keras_nlp.layers.TransformerDecoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "outputs = keras.layers.Dense(\n",
    "    VOCAB_SIZE,\n",
    "    activation=\"softmax\"\n",
    ")(decoder_outputs)\n",
    "\n",
    "model = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    outputs,\n",
    "    name=\"transformer_text_summarization_model\",\n",
    ")\n",
    "# Note\n",
    "# In the case that the dataset is large and the dimension is small,\n",
    "# the learning rate of Adam needed to be smaller.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\n",
    "        #\"loss\",\n",
    "        # keras_nlp.metrics.RougeL()\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35890/35890 [==============================] - 38130s 1s/step - loss: 0.1890\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = model.fit(\n",
    "    preprocessed_train_dataset,\n",
    "    #validation_data=preprocessed_validation_dataset,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=100,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('text_classification.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1437/1437 [==============================] - 599s 411ms/step - loss: 0.1858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18576039373874664"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss = model.evaluate(\n",
    "    preprocessed_test_dataset,\n",
    ")\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\n",
      "522 287 start end\n",
      "Summary: start the [UNK] horse was rescued from a pool of water and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] was rescued by the [UNK] and [UNK] end\n",
      "Original: There are two chickens in the garden.\n",
      "522 287 start end\n",
      "Summary: start [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] end\n",
      "Original: Two chickens fell into the swimming pool in the garden.\n",
      "522 287 start end\n",
      "Summary: start the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] was swimming with a pool of [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] end\n"
     ]
    }
   ],
   "source": [
    "def summarize(text):\n",
    "    \"\"\"\n",
    "    Summarize text\n",
    "    :param text: original text\n",
    "    :return: summarized text\n",
    "    \"\"\"\n",
    "    table = vectorization_layer.get_vocabulary()\n",
    "    input_sequence = input_vectorization_layer([text])\n",
    "\n",
    "\n",
    "    start_token = vectorization_layer(['[start]'])[0][0].numpy()\n",
    "    end_token = vectorization_layer(['[end]'])[0][0].numpy()\n",
    "    print(start_token, end_token, table[start_token], table[end_token])\n",
    "    decoded_sentence = [start_token]\n",
    "    for i in range(max_target_length):\n",
    "        decoder_inputs = tf.convert_to_tensor(\n",
    "            [decoded_sentence],\n",
    "            dtype=\"int64\",\n",
    "        )\n",
    "        decoder_inputs = tf.concat(\n",
    "            [\n",
    "                decoder_inputs,\n",
    "                tf.zeros(\n",
    "                    [1, max_target_length - i - 1],\n",
    "                    dtype=\"int64\",\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        predictions = model.predict(\n",
    "            [input_sequence, decoder_inputs],\n",
    "            verbose=0\n",
    "        )\n",
    "        predicted_token = np.argmax(predictions[0, i, :])\n",
    "        decoded_sentence.append(predicted_token)\n",
    "        if predicted_token == end_token:\n",
    "            break\n",
    "\n",
    "    detokenized_output = []\n",
    "    for token in decoded_sentence:\n",
    "        detokenized_output.append(table[token])\n",
    "    return \" \".join(detokenized_output)\n",
    "        \n",
    "#     print(start_token)\n",
    "#     decoder_input_sequence = vectorization_layer(['[start]'])\n",
    "#     rows = decoder_input_sequence.row_lengths().shape[0]\n",
    "#     decoder_input_sequence = decoder_input_sequence.to_tensor(shape=(rows, 1438))\n",
    "#     print(decoder_input_sequence)\n",
    "\n",
    "#     # article = input_vectorization_layer(x['article'])\n",
    "#     # highlights = tf.strings.join(['[start] ', x['highlights'], ' [end]'])\n",
    "#     # h = vectorization_layer(highlights)\n",
    "#     # rows = h.row_lengths().shape[0]\n",
    "#     # sequences = h.to_tensor(shape=(rows, 1439))\n",
    "#     # highlights_decoder_input = sequences[:, :-1] # 1438\n",
    "#     # highlights_decoder_output = sequences[:, 1:] # 1438\n",
    "\n",
    "\n",
    "#     summary = []\n",
    "#     for _ in range(max_target_length):\n",
    "#         predictions = model.predict(\n",
    "#             [input_sequence, decoder_input_sequence],\n",
    "#             verbose=0\n",
    "#         )\n",
    "#         #np.argmax(predictions[0, i, :])\n",
    "#         next_token = tf.argmax(predictions[0, -1, :])\n",
    "#         print(predictions[0, -1])\n",
    "#         next_word = tokenizer.index_word.get(next_token.numpy(), '[UNK]')\n",
    "#         if next_word == '[end]':\n",
    "#             break\n",
    "#         summary.append(next_word)\n",
    "#         decoder_input_sequence = tf.concat(\n",
    "#             [decoder_input_sequence, tf.expand_dims([next_token], axis=-1)],\n",
    "#             axis=-1\n",
    "#         )\n",
    "#     return ' '.join(summary)\n",
    "\n",
    "# Sample\n",
    "# sample_text = \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\"\n",
    "# print(\"Original:\", sample_text)\n",
    "# print(\"Summary:\", summarize(sample_text))\n",
    "sample_text = \"\"\"\n",
    "Vice President Dick Cheney will serve as acting president briefly Saturday while President Bush is anesthetized for a routine colonoscopy, White House spokesman Tony Snow said Friday. Bush is scheduled to have the medical procedure, expected to take about 2 1/2 hours, at the presidential retreat at Camp David, Maryland, Snow said. Bush's last colonoscopy was in June 2002, and no abnormalities were found, Snow said. The president's doctor had recommended a repeat procedure in about five years. The procedure will be supervised by Dr. Richard Tubb and conducted by a multidisciplinary team from the National Naval Medical Center in Bethesda, Maryland, Snow said. A colonoscopy is the most sensitive test for colon cancer, rectal cancer and polyps, small clumps of cells that can become cancerous, according to the Mayo Clinic. Small polyps may be removed during the procedure. Snow said that was the case when Bush had colonoscopies before becoming president. Snow himself is undergoing chemotherapy for cancer that began in his colon and spread to his liver. Snow told reporters he had a chemo session scheduled later Friday. Watch Snow talk about Bush's procedure and his own colon cancer » . \"The president wants to encourage everybody to use surveillance,\" Snow said. The American Cancer Society recommends that people without high-risk factors or symptoms begin getting screened for signs of colorectal cancer at age 50. E-mail to a friend .\n",
    "\"\"\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"There are two chickens in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"Two chickens fell into the swimming pool in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [1] Vaswani et al. (2017). Attention Is All You Need.https://doi.org/10.48550/arXiv.1706.03762\n",
    "- [2] tensorflow. Tensor2tensor. Retrieved May 16, 2024, from https://github.com/tensorflow/tensor2tensor\n",
    "- [3] Yutao et al. (2023). Retentive Network: A Successor to Transformer for Large Language Models. https://doi.org/10.48550/arXiv.2307.08621\n",
    "- [4] Wojciech et al. (2019). Neural Text Summarization: A Critical Evaluation. https://doi.org/10.48550/arXiv.1908.08960\n",
    "- [5] Yang et al. (2019). Text Summarization with Pretrained Encoders. https://doi.org/10.48550/arXiv.1908.08345\n",
    "- [6] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 266). Manning.\n",
    "- [7] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 246). Manning.\n",
    "- [8] ROUGE (metric). (2023, November 28). In Wikipedia. https://en.wikipedia.org/wiki/ROUGE_(metric)\n",
    "- [9] Dosovitskiy, A. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. https://doi.org/10.48550/arXiv.2010.11929\n",
    "- [10] BERT (language model). (2024, May 7). In Wikipedia. https://en.wikipedia.org/wiki/BERT_(language_model)\n",
    "- [11] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 250). Manning.\n",
    "- [12] Tensorflow. Retrieved June 6, 2024, from https://www.tensorflow.org\n",
    "- [13] TensorFlow. (2024, May 26). In Wikipedia. https://en.wikipedia.org/wiki/TensorFlow\n",
    "- [14] Keras: Deep Learning for humans. https://keras.io/\n",
    "- [15] Keras. (2024, June 6). In Wikipedia. https://en.wikipedia.org/wiki/Keras\n",
    "- [16] KerasNLP. Retrieved June 6, 2024, from https://keras.io/keras_nlp/\n",
    "- [17] PyTorch. (2024, May 10). In Wikipedia. https://en.wikipedia.org/wiki/PyTorch\n",
    "- [18] Hugging Face, Inc. Transformers. Retrieved June 6, 2024, from https://github.com/huggingface/transformers\n",
    "- [19] (2020, February 22). facebook/bart-large-cnn. Retrieved May 1, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [20] (2020, Aug 9). google/pegasus-cnn_dailymail. Retrieved May 1, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [21] Embedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/layers/core_layers/embedding/\n",
    "- [22] Word2vec. Tensorflow. Retrieved June 4, 2024, from https://www.tensorflow.org/text/tutorials/word2vec\n",
    "- [23] Facebook Inc. FastText. Retrieved June 4, 2024, from https://fasttext.cc/\n",
    "- [24] GloVe: Global Vectors for Word Representation. Retrieved June 4, 2024, from https://nlp.stanford.edu/projects/glove/\n",
    "- [25] Google LLC. Word embeddings. Retrieved June 10, 2024, from https://www.tensorflow.org/text/guide/word_embeddings#word_embeddings_2\n",
    "- [26] PositionEmbedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/position_embedding/\n",
    "- [27] Google LLC. SinePositionEncoding layer. Retrieved June 10, 2024, from https://keras.io/api/keras_nlp/modeling_layers/sine_position_encoding/\n",
    "- [28] TransformerEncoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/\n",
    "- [29] TransformerDecoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_decoder/\n",
    "- [30] facebook (2020, February 22). Facebook/bart-large-cnn. Retrieved June 10, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [31] Google (2020, August 9). Google/pegasus-cnn_dailymail. Retrieved June 10, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [32] Zhang, J. (2020). PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. https://doi.org/10.48550/arXiv.1912.08777\n",
    "- [33] Google (2023, July 8). Abstractive Text Summarization with BART. Retrieved June 10, 2024, from https://keras.io/examples/nlp/abstractive_summarization_with_bart/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
