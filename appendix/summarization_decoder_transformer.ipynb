{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U pip datasets ipywidgets\n",
    "# For mac OS\n",
    "# %pip install -U tensorflow==2.16.2 tensorflow-macos==2.16.2 keras==3.4.1 keras-nlp\n",
    "# 20240803, tensorflow-text can be installed on Apple Silicon mac now!\n",
    "# %pip install tensorflow-text\n",
    "# For Intel mac\n",
    "# %pip install -U tensorflow==2.16.2 tensorflow-text keras==3.4.1 keras-nlp\n",
    "# For AWS SageMaker\n",
    "# %pip install -U tensorflow==2.16.2 tensorflow-datasets tensorflow-text keras==3.4.1 keras-nlp datasets rouge-score py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to determine where this note is running\n",
    "import platform\n",
    "\n",
    "def is_running_on_apple_silicon():\n",
    "    \"\"\"\n",
    "    Determine if this is running on Apple Silicon Mac.\n",
    "    \"\"\"\n",
    "    return platform.system() == \"Darwin\" and platform.processor() == \"arm\"\n",
    "\n",
    "def is_running_on_intel_mac():\n",
    "    \"\"\"\n",
    "    Determine if this is running on Intel Mac.\n",
    "    \"\"\"\n",
    "    return platform.system() == \"Darwin\" and platform.processor() == \"i386\"\n",
    "\n",
    "# This flag is used for tf.debugging.experimental.enable_dump_debug_info.\n",
    "# However, this makes 10 times slower.\n",
    "DEBUGGER_V2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 2.17.0 is expected. The running version is 2.17.0\n",
      "Keras 3.4.1 is expected. The running version is 3.4.1\n",
      "KerasNLP 0.12.1 is expected. The running version is 0.12.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "print(\"Tensorflow 2.17.0 is expected. The running version is\", tf.__version__)\n",
    "print(\"Keras 3.4.1 is expected. The running version is\", keras.__version__)\n",
    "print(\"KerasNLP 0.12.1 is expected. The running version is\", keras_nlp.__version__)\n",
    "\n",
    "if is_running_on_apple_silicon() or is_running_on_intel_mac():\n",
    "    FLOAT_TYPE = tf.float32\n",
    "else:\n",
    "    \"\"\"\n",
    "    # Mixed-precision training\n",
    "    Deep Learning with Python, Second Edition\n",
    "    François Chollet\n",
    "\n",
    "    However, this makes the processing 2.x slower on M2 Apple Silicon.\n",
    "\n",
    "    Machine | 1 step\n",
    "    --- | ---\n",
    "    Intel Mac - fp32 : fp16 | 1 : 1.714\n",
    "    Apple Silicon M2 (Mac Book Pro) - fp32 : fp16 | 1 : 2.813\n",
    "    NVIDIA V100 GPU x 1 (ml.p3.2xlarge) - fp32 : fp16 | 1 : 0.875    Intel Mac - fp32 : fp16 | 1 : 1.714\n",
    "    \"\"\"\n",
    "    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    FLOAT_TYPE = tf.float16\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "# SageMaker cannot use @keras.saving\n",
    "from keras import saving\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(history, title=None, keys=[\"loss\", \"masked_acc\"]):\n",
    "    \"\"\"\n",
    "    Display the plot that indicates the loss and accuracy.\n",
    "    :param history: history object from the tensorflow fit function.\n",
    "    :param title: title text.\n",
    "    :param keys: keys for plotting.\n",
    "    \"\"\"\n",
    "    for key in keys:\n",
    "        if 'loss' in key:\n",
    "            print(\n",
    "                np.min(history.history[f\"val_{key}\"]),\n",
    "                \"The best number of epocs for the validation loss is\",\n",
    "                np.argmin(history.history[f\"val_{key}\"]) + 1,\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                np.max(history.history[f\"val_{key}\"]),\n",
    "                \"The best number of epocs for the validation accuracy is\",\n",
    "                np.argmax(history.history[f\"val_{key}\"]) + 1,\n",
    "            )\n",
    "\n",
    "    flg, axes = plt.subplots(1, 2, tight_layout=True)\n",
    "    if title is not None:\n",
    "        flg.suptitle(t=title, fontsize=14)\n",
    "    for i, key in enumerate(keys):\n",
    "        value = history.history[key]\n",
    "        val_loss = history.history[f\"val_{key}\"]\n",
    "        epochs = range(1, len(value) + 1)\n",
    "        axes[i].plot(epochs, value, label=f\"Training {key}\")\n",
    "        axes[i].plot(epochs, val_loss, label=f\"Validation {key}\")\n",
    "        axes[i].set_title(f\"Training and validation {key}\")\n",
    "        axes[i].set_xlabel(\"epochs\")\n",
    "        axes[i].set_ylabel(key)\n",
    "        axes[i].legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_attention_head(\n",
    "        in_tokens,\n",
    "        translated_tokens,\n",
    "        attention):\n",
    "    # The model didn't generate `[start]` in the output. Skip it.\n",
    "    translated_tokens = translated_tokens[1:]\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.matshow(attention)\n",
    "    ax.set_xticks(range(len(in_tokens)))\n",
    "    ax.set_yticks(range(len(translated_tokens)))\n",
    "\n",
    "    labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "\n",
    "    labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
    "    ax.set_yticklabels(labels)\n",
    "\n",
    "def plot_attention_weights(\n",
    "        vectorization_layer,\n",
    "        sentence,\n",
    "        translated_tokens,\n",
    "        attention_heads):\n",
    "    in_tokens = tf.convert_to_tensor([sentence])\n",
    "    in_tokens = vectorization_layer.tokenize(in_tokens).to_tensor()\n",
    "    in_tokens = vectorization_layer.lookup(in_tokens)[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    for h, head in enumerate(attention_heads):\n",
    "        ax = fig.add_subplot(2, 4, h+1)\n",
    "        plot_attention_head(in_tokens, translated_tokens, head)\n",
    "        ax.set_xlabel(f'Head {h+1}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir(now):\n",
    "    \"\"\"\n",
    "    Specify the log directory for the timestamp\n",
    "    \"\"\"\n",
    "    log_dir = \"logs/fit/\" + now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    return log_dir\n",
    "\n",
    "def get_tensorboard_callback(now):\n",
    "    \"\"\"\n",
    "    Create the TensorBoard callback\n",
    "    \"\"\"\n",
    "    # @see https://www.tensorflow.org/tensorboard/get_started\n",
    "    log_dir = get_log_dir(now=now)\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1\n",
    "    )\n",
    "    return tensorboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @see https://github.com/keras-team/keras-nlp/blob/50e041487b1d8b30b34c5fb738db3ed3406363bc/examples/machine_translation/data.py\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "strip_chars = strip_chars.replace(\"'\", \"\")\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"\n",
    "    Define the custom standardization to remove useless characters.\n",
    "    \"\"\"\n",
    "    input_string = tf.strings.lower(input_string) # This should not be used to generation tasks\n",
    "    input_string = tf.strings.regex_replace(input_string, r'\\s', ' ') # Replace \\n, \\r, \\t, \\f into space.\n",
    "    #input_string = tf.strings.regex_replace(input_string, r\"'\", ' ') # This should not be used to generation tasks\n",
    "    input_string = tf.strings.regex_replace(input_string, r'<\\/?[^>]*>', '') # html tag\n",
    "    #input_string = tf.strings.regex_replace(input_string, r'''<(\"[^\"]*\"|'[^']*'|[^'\">])*>''', '') # html tag\n",
    "    input_string = tf.strings.regex_replace(input_string, r'https?:\\/\\/.*[\\r\\n]*', \" \") # URL\n",
    "    # Some symbols are incorrectly used without space.\n",
    "    input_string = tf.strings.regex_replace(input_string, r\"([%s])\" % re.escape(strip_chars), r\" \\1 \")\n",
    "    input_string = tf.strings.regex_replace(input_string, r'\\s\\s+', ' ') # Finally, spaces to 1 space\n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_nlp.src.backend import ops\n",
    "\n",
    "def visualize_attention_scores(\n",
    "        encoder_tokenizer,\n",
    "        encoder_embedding,\n",
    "        attention_layer,\n",
    "        sentence):\n",
    "    \"\"\"\n",
    "    This method visuzlize MultiHeadAttention of TransformerEncoder in keras_nlp.\n",
    "    attention_scores cannot be accessed in this library.\n",
    "    Therefore, the calculation code is ported. (keras_nlp.layers.modeling.CachedMultiHeadAttention)\n",
    "    :encoder_tokenizer: the tokenizer that was used in encoder\n",
    "    :encoder_embedding: the embedding layer that was used in encoder\n",
    "    :attention_layer: the reference of attention layer, which is ususally CachedMultiHeadAttention\n",
    "    :sentence: the sentence that is analyzed\n",
    "    \"\"\"\n",
    "    sentence = custom_standardization(sentence)\n",
    "    tokenized_sentence = encoder_tokenizer([sentence])\n",
    "    labels = tf.gather(\n",
    "        tf.convert_to_tensor(encoder_tokenizer.get_vocabulary()),\n",
    "        tf.RaggedTensor.from_tensor(tokenized_sentence, padding=0)[0],\n",
    "    ).numpy().tolist()\n",
    "    labels = [label.decode('utf-8') for label in labels]\n",
    "    max_len = tf.RaggedTensor.from_tensor(tokenized_sentence, padding=0)[0].shape[0]\n",
    "\n",
    "    output = encoder_embedding(tokenized_sentence)\n",
    "    # TransformerEncoder uses MultiHeadAttention, internally.\n",
    "    # TransformerDecoder uses CachedMultiHeadAttention, internally.\n",
    "    if type(attention_layer) is keras.layers.MultiHeadAttention:\n",
    "        output, attention_scores = attention_layer(\n",
    "            key=output,\n",
    "            value=output,\n",
    "            query=output,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "    else:\n",
    "        k = attention_layer._key_dense(output)\n",
    "        q = attention_layer._query_dense(output)\n",
    "        q = ops.multiply(\n",
    "            q,\n",
    "            1.0 / ops.sqrt(ops.cast(attention_layer._key_dim, q.dtype)),\n",
    "        )\n",
    "        attention_scores = ops.einsum(\n",
    "            attention_layer._dot_product_equation,\n",
    "            k,\n",
    "            q\n",
    "        )\n",
    "        attention_mask = None\n",
    "        attention_scores = attention_layer._masked_softmax(\n",
    "            attention_scores,\n",
    "            attention_mask\n",
    "        )\n",
    "\n",
    "    # print(attention_scores.shape)\n",
    "    # (1, 6, 128, 128)\n",
    "    # batch_size, num_heads, sequence_length, sequence_length\n",
    "    num_heads = attention_layer.num_heads\n",
    "    columns = 4\n",
    "    rows = math.ceil(num_heads / columns)\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    for i in range(num_heads):\n",
    "        ax = fig.add_subplot(rows, 4, i + 1)\n",
    "        ax = plt.gca()\n",
    "        values = attention_scores[0][i][:max_len, :max_len]\n",
    "        #values = tf.nn.softmax(values)\n",
    "        ax.matshow(values)\n",
    "        ax.set_xticks(range(len(labels)))\n",
    "        ax.set_yticks(range(len(labels)))\n",
    "        ax.set_xticklabels(labels, rotation=90)\n",
    "        ax.set_yticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @see https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "reduce_lr_callbacks = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.2,\n",
    "    patience=4,\n",
    "    verbose=1,\n",
    "    min_delta=0.04,\n",
    "    cooldown=6,\n",
    "    min_lr=2e-6, # 5e-4: 0.0005, 2e-5: 0.00002, 2e-6: 0.000002\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### The reason why the masked loss is required for seq2seq models\n",
    "\n",
    "The prediction y values of seq2seq models contain pad(s), which are used to align the length of every output sequence.\n",
    "In the case that most sequences are much shorter than the longest sentence and pads are not considered,\n",
    "a model that predicts only pads of sentences is highly evaluated.\n",
    "Therefore, excluding pads from the loss calculation improves the model.\n",
    "\n",
    "### The reason why the classification model does not use the masked loss function\n",
    "\n",
    "The prediction y values of classification models do not contain pad(s), which are used to align the length of every output sequence.\n",
    "It just contains the probability of each class.\n",
    "\"\"\"\n",
    "\n",
    "# @see https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "@saving.register_keras_serializable()\n",
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "        # nn.py:609: UserWarning:\n",
    "        # \"`sparse_categorical_crossentropy` received `from_logits=True`,\n",
    "        # but the `output` argument was produced by a Softmax activation and thus does not represent logits.\n",
    "        # Was this intended?\n",
    "        # When logits is True, softmax activation function has not processed the values.\n",
    "        from_logits=True,\n",
    "        reduction='none'\n",
    "    )\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @see https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "# @saving.register_keras_serializable()\n",
    "# def masked_acc(y_true, y_pred):\n",
    "#     # Calculate the loss for each item in the batch.\n",
    "#     y_pred = tf.argmax(y_pred, axis=-1) # last index\n",
    "#     y_pred = tf.cast(y_pred, dtype=y_true.dtype)\n",
    "#     match = tf.cast(y_true == y_pred, dtype=FLOAT_TYPE)\n",
    "#     mask = tf.cast(y_true != 0, dtype=FLOAT_TYPE)\n",
    "#     return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "\n",
    "# @see https://www.tensorflow.org/text/tutorials/transformer\n",
    "@saving.register_keras_serializable()\n",
    "def masked_acc(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=2)\n",
    "    y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "    match = y_true == y_pred\n",
    "    mask = y_true != 0\n",
    "    match = match & mask\n",
    "    match = tf.cast(match, dtype=FLOAT_TYPE)\n",
    "    mask = tf.cast(mask, dtype=FLOAT_TYPE)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(y_true, y_pred, order=2):\n",
    "    rouge_n = keras_nlp.metrics.RougeN(order=order)\n",
    "    return rouge_n(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def prepare_datasets():\n",
    "    \"\"\"\n",
    "    Get training set, validation set, and test set\n",
    "    tensorflow_datasets does not work well with the SSL error.\n",
    "    Therefore, the data is obtained with the Huggingface library and converted to Tensorflow.\n",
    "    :return: train_ds\n",
    "    :return: validation_ds\n",
    "    :return: test_ds\n",
    "    \"\"\"\n",
    "    # How to convert huggingface dataset to tensorflow dataset\n",
    "    # @see https://huggingface.co/docs/datasets/v1.3.0/torch_tensorflow.html#setting-the-format\n",
    "    def convert_hf2tf(\n",
    "            dataset: datasets.DatasetDict,\n",
    "            split: list[str],\n",
    "            columns=['article', 'highlights', 'id',]):\n",
    "        dataset.set_format(\n",
    "            type='tensorflow',\n",
    "            columns=columns\n",
    "        )\n",
    "        l = []\n",
    "        for s in split:\n",
    "            d = dataset[s]\n",
    "            features = {x: d[x] for x in columns}\n",
    "            # .batch(32) is not used to show a simple sampled data below with take(1)\n",
    "            tf_dataset = tf.data.Dataset.from_tensor_slices(features)\n",
    "            l.append(tf_dataset)\n",
    "        return tuple(l)\n",
    "    ds = datasets.load_dataset(\n",
    "        'Samsung/samsum',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    train_ds, validation_ds, test_ds = convert_hf2tf(\n",
    "        dataset=ds,\n",
    "        split=['train', 'validation', 'test'],\n",
    "        columns=['id', 'summary', 'dialogue'],\n",
    "    )\n",
    "    return train_ds, validation_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer(\n",
    "        vectorization_layer,\n",
    "        decoder_sequence_length,\n",
    "        max_tokens=15000):\n",
    "    \"\"\"\n",
    "    Display the plot that indicates the loss and accuracy.\n",
    "    :param vectorization_layer: obtain vocabulary.\n",
    "    :param max_tokens: In other words, this is the vocabulary size.\n",
    "    :param decoder_sequence_length: The sequence length for target.\n",
    "    \"\"\"\n",
    "    vocabulary = vectorization_layer.get_vocabulary()\n",
    "\n",
    "    target_vectorization_layer = keras.layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=decoder_sequence_length,\n",
    "    )\n",
    "    target_vectorization_layer.set_vocabulary(vocabulary)\n",
    "    return target_vectorization_layer\n",
    "\n",
    "def build_datasets(\n",
    "        train_ds, validation_ds, test_ds,\n",
    "        vectorization_layer,\n",
    "        batch_size,\n",
    "        decoder_sequence_length):\n",
    "    start_oov = vectorization_layer(['[start]'])[0][0].numpy().item()\n",
    "    # end_oov = vectorization_layer(['[end]']).to_tensor(shape=(1, 1))\n",
    "    start_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=decoder_sequence_length,\n",
    "        start_value=start_oov,\n",
    "    )\n",
    "    def format_dataset(x):\n",
    "        summary = vectorization_layer(x['summary'])\n",
    "        rows = tf.shape(summary)[0] # rows = batch size\n",
    "        summary = summary.to_tensor(shape=(\n",
    "            rows,\n",
    "            decoder_sequence_length\n",
    "        ))\n",
    "\n",
    "        dialogue = start_packer(vectorization_layer(x['dialogue']))\n",
    "        return (dialogue, ), summary\n",
    "\n",
    "    train_ds = train_ds.batch(batch_size).map(\n",
    "        format_dataset,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).prefetch(tf.data.AUTOTUNE).cache()\n",
    "    if validation_ds is not None:\n",
    "        validation_ds = validation_ds.batch(batch_size).map(\n",
    "            format_dataset,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).prefetch(tf.data.AUTOTUNE).cache()\n",
    "    test_ds = test_ds.batch(batch_size).map(\n",
    "        format_dataset,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).prefetch(tf.data.AUTOTUNE).cache()\n",
    "    return train_ds, validation_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@saving.register_keras_serializable()\n",
    "class TransformerDecoderModel(keras.Model):\n",
    "    \"\"\"\n",
    "    Transformer model class that contains Transformer Decoder inside.\n",
    "    :param decoder_tokenizer: tf.keras.layers.TextVectorization object\n",
    "    :num_decoders: the number of decoders\n",
    "    :num_heads: the number of heads (Attention mechanism)\n",
    "    :dropout: the dropout rate\n",
    "    :transformer_intermediate_dim: the number of dimensions in each decoder\n",
    "    :normalize_first: Pre-Layer Normalization or Post-Layer Normalization\n",
    "    :layer_norm_epsilon: the epsilon value in layer normalization components.\n",
    "    :decoder_vocabulary_size: the number of decoder vocabulary size\n",
    "    :embedding_dim: the dimension for embedding\n",
    "    :mask_zero: whether 0 is used the mask value\n",
    "    :decoder_sequence_length: the length of decoder sequence\n",
    "    :use_residual: whether residual connections are used outside of the Transformer decoder\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        decoder_tokenizer,\n",
    "        num_decoders,\n",
    "        num_heads,\n",
    "        dropout,\n",
    "        transformer_intermediate_dim,\n",
    "        normalize_first,\n",
    "        layer_norm_epsilon,\n",
    "        decoder_vocabulary_size,\n",
    "        embedding_dim,\n",
    "        mask_zero,\n",
    "        decoder_sequence_length,\n",
    "        use_residual,\n",
    "        **kwargs):\n",
    "        super(TransformerDecoderModel, self).__init__(**kwargs)\n",
    "        self.mask_zero = mask_zero\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.use_residual = use_residual\n",
    "        self.decoders = []\n",
    "        if isinstance(layer_norm_epsilon, list):\n",
    "            decoder_layer_norm_epsilons = layer_norm_epsilon\n",
    "        else:\n",
    "            decoder_layer_norm_epsilons = [layer_norm_epsilon for _ in range(num_decoders)]\n",
    "\n",
    "        for i in range(num_decoders):\n",
    "            self.decoders.append(\n",
    "                keras_nlp.layers.TransformerDecoder(\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                    intermediate_dim=transformer_intermediate_dim,\n",
    "                    normalize_first=normalize_first,\n",
    "                    layer_norm_epsilon=decoder_layer_norm_epsilons[i],\n",
    "                    name=f\"decoder_{i}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.decoder_tokenizer = decoder_tokenizer\n",
    "\n",
    "        self.decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "            vocabulary_size=decoder_vocabulary_size,\n",
    "            sequence_length=decoder_sequence_length,\n",
    "            embedding_dim=embedding_dim,\n",
    "            mask_zero=mask_zero,\n",
    "            name=\"decoder_embed\",\n",
    "        )\n",
    "\n",
    "        self.dense = keras.layers.Dense(\n",
    "            decoder_vocabulary_size,\n",
    "            name=\"dense\",\n",
    "        )\n",
    "\n",
    "    # def build(self, input_shape):\n",
    "    #     self.decoder_embedding.build(input_shape[0])\n",
    "    #     decoder_output_shape = self.decoder_embedding.compute_output_shape(input_shape[0])\n",
    "    #     for decoder in self.decoders:\n",
    "    #         decoder.build(\n",
    "    #             decoder_sequence_shape=decoder_output_shape,\n",
    "    #             #encoder_sequence_shape=encoder_output_shape,\n",
    "    #         )\n",
    "    #         decoder_output_shape = decoder.compute_output_shape(\n",
    "    #             decoder_sequence_shape=decoder_output_shape,\n",
    "    #         )\n",
    "    #     self.dense.build(input_shape=decoder_output_shape)\n",
    "    #     self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        This method defines and represents the calculation graph (tensor flow).\n",
    "        :training: True, when calculating val_loss and val_acc. Otherwise, False.\n",
    "        \"\"\"\n",
    "        decoder_input = inputs[0]\n",
    "        decoded = self.decoder_embedding(decoder_input)\n",
    "        for decoder in self.decoders:\n",
    "            if self.use_residual and len(self.decoders) > 1:\n",
    "                residual = decoded\n",
    "            decoded = decoder(\n",
    "                decoder_sequence=decoded,\n",
    "                #encoder_sequence=encoded,\n",
    "                use_causal_mask=True,\n",
    "                training=training,\n",
    "            )\n",
    "            if self.use_residual and len(self.decoders) > 1:\n",
    "                decoded = decoded + residual\n",
    "\n",
    "        output = self.dense(decoded)\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del output._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        This method is used to save a model into a file.\n",
    "        \"\"\"\n",
    "        config = super(TransformerDecoderModel, self).get_config().copy()\n",
    "        config.update({\n",
    "            \"decoder_tokenizer\": self.decoder_tokenizer.get_config(),\n",
    "            \"num_decoders\": len(self.decoders),\n",
    "            \"num_heads\": self.decoders[0].num_heads,\n",
    "            \"dropout\" : self.decoders[0].dropout,\n",
    "            \"transformer_intermediate_dim\": self.decoders[0].intermediate_dim,\n",
    "            \"normalize_first\": self.decoders[0].normalize_first,\n",
    "            \"layer_norm_epsilon\": self.layer_norm_epsilon,\n",
    "            \"decoder_vocabulary_size\": self.decoder_embedding.vocabulary_size,\n",
    "            \"embedding_dim\": self.decoder_embedding.embedding_dim,\n",
    "            \"mask_zero\": self.mask_zero,\n",
    "            \"decoder_sequence_length\": self.decoder_embedding.sequence_length,\n",
    "            \"use_residual\": self.use_residual,\n",
    "        })\n",
    "        return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"\n",
    "        This method is used to build a model from a saved file.\n",
    "        \"\"\"\n",
    "        decoder_tokenizer_config = config.pop(\"decoder_tokenizer\")\n",
    "        decoder_tokenizer = keras.layers.TextVectorization.from_config(decoder_tokenizer_config)\n",
    "        return cls(\n",
    "            decoder_tokenizer=decoder_tokenizer,\n",
    "            **config\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "EXAMPLES = [\n",
    "    \"\"\"(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men\\'s 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio Carl Lewis, Michael Johnson and Allyson Felix, not to mention the small matter of six Olympic titles. The relay triumph followed individual successes in the 100 and 200 meters in the Russian capital. \"I\\'m proud of myself and I\\'ll continue to work to dominate for as long as possible,\" Bolt said, having previously expressed his intention to carry on until the 2016 Rio Olympics. Victory was never seriously in doubt once he got the baton safely in hand from Ashmeade, while Gatlin and the United States third leg runner Rakieem Salaam had problems. Gatlin strayed out of his lane as he struggled to get full control of their baton and was never able to get on terms with Bolt. Earlier, Jamaica\\'s women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple. Their quartet recorded a championship record of 41.29 seconds, well clear of France, who crossed the line in second place in 42.73 seconds. Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover. The British quartet, who were initially fourth, were promoted to the bronze which eluded their men\\'s team. Fraser-Pryce, like Bolt aged 26, became the first woman to achieve three golds in the 100-200 and the relay. In other final action on the last day of the championships, France\\'s Teddy Tamgho became the third man to leap over 18m in the triple jump, exceeding the mark by four centimeters to take gold. Germany\\'s Christina Obergfoll finally took gold at global level in the women\\'s javelin after five previous silvers, while Kenya\\'s Asbel Kiprop easily won a tactical men\\'s 1500m final. Kiprop\\'s compatriot Eunice Jepkoech Sum was a surprise winner of the women\\'s 800m. Bolt\\'s final dash for golden glory brought the eight-day championship to a rousing finale, but while the hosts topped the medal table from the United States there was criticism of the poor attendances in the Luzhniki Stadium. There was further concern when their pole vault gold medalist Yelena Isinbayeva made controversial remarks in support of Russia\\'s new laws, which make \"the propagandizing of non-traditional sexual relations among minors\" a criminal offense. She later attempted to clarify her comments, but there were renewed calls by gay rights groups for a boycott of the 2014 Winter Games in Sochi, the next major sports event in Russia.\"\"\",\n",
    "    \"\"\"Vice President Dick Cheney will serve as acting president briefly Saturday while President Bush is anesthetized for a routine colonoscopy, White House spokesman Tony Snow said Friday. Bush is scheduled to have the medical procedure, expected to take about 2 1/2 hours, at the presidential retreat at Camp David, Maryland, Snow said. Bush's last colonoscopy was in June 2002, and no abnormalities were found, Snow said. The president's doctor had recommended a repeat procedure in about five years. The procedure will be supervised by Dr. Richard Tubb and conducted by a multidisciplinary team from the National Naval Medical Center in Bethesda, Maryland, Snow said. A colonoscopy is the most sensitive test for colon cancer, rectal cancer and polyps, small clumps of cells that can become cancerous, according to the Mayo Clinic. Small polyps may be removed during the procedure. Snow said that was the case when Bush had colonoscopies before becoming president. Snow himself is undergoing chemotherapy for cancer that began in his colon and spread to his liver. Snow told reporters he had a chemo session scheduled later Friday. Watch Snow talk about Bush's procedure and his own colon cancer » . \"The president wants to encourage everybody to use surveillance,\" Snow said. The American Cancer Society recommends that people without high-risk factors or symptoms begin getting screened for signs of colorectal cancer at age 50. E-mail to a friend .\"\"\",\n",
    "    \"There are two chickens in the garden.\",\n",
    "    \"Two chickens fell into the swimming pool in the garden.\",\n",
    "]\n",
    "\n",
    "def post_process(text):\n",
    "    text = text.replace(\"[start] \", \"\")\n",
    "    text = text.replace(\" [end]\", \"\")\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    ss = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.capitalize()\n",
    "        tokens = nltk.tokenize.word_tokenize(text=sentence)\n",
    "        tagged_tokens = nltk.tag.pos_tag(tokens=tokens)\n",
    "        result = []\n",
    "        for i, (token, tag) in enumerate(tagged_tokens):\n",
    "            if i == 0:  # The first word of the sentence\n",
    "                result.append(token.capitalize())\n",
    "            elif tag.startswith('NNP'):  # proper noun\n",
    "                result.append(token.capitalize())\n",
    "            else:\n",
    "                result.append(token.lower())\n",
    "        s = \" \".join(result)\n",
    "        ss.append(s)\n",
    "    text = \" \".join(ss)\n",
    "    text = text.replace(\" ,\", \",\")\n",
    "    text = text.replace(\" .\", \".\")\n",
    "    # Revert OOV.\n",
    "    # text = '[start] ' + text + ' [end]'\n",
    "    return text\n",
    "\n",
    "def decode_sequence(\n",
    "        input_sentence,\n",
    "        model,\n",
    "        max_sequence_length,\n",
    "        lookup_table,\n",
    "        use_post_processing=False):\n",
    "    \"\"\"\n",
    "    Generate summarized text from the input sentence.\n",
    "    :input_sentence: the original text that is summarized\n",
    "    :model: the TransformerDecoderModel class model\n",
    "    :max_sequence_length: the maximum length of the summarized text\n",
    "    :lookup_table: the table holds token IDs and their actual words\n",
    "    \"\"\"\n",
    "    decoder_tokenizer = model.decoder_tokenizer\n",
    "    tokenized_input = decoder_tokenizer([input_sentence])\n",
    "\n",
    "    start_token = decoder_tokenizer(\"[start]\")[0].numpy()\n",
    "    end_token = decoder_tokenizer(\"[end]\")[0].numpy()\n",
    "\n",
    "    decoded_sentence = [start_token]\n",
    "    for i in range(max_sequence_length):\n",
    "        decoder_inputs = tf.convert_to_tensor(\n",
    "            [decoded_sentence],\n",
    "            dtype=\"int64\",\n",
    "        )\n",
    "        decoder_inputs = tf.concat(\n",
    "            [\n",
    "                decoder_inputs,\n",
    "                tf.zeros(\n",
    "                    [1, max_sequence_length - i - 1],\n",
    "                    dtype=\"int64\",\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        input = (\n",
    "            tokenized_input,\n",
    "            decoder_inputs,\n",
    "        )\n",
    "        predictions = model(input)\n",
    "        predicted_token = np.argmax(predictions[0, i, :])\n",
    "        decoded_sentence.append(predicted_token)\n",
    "        if predicted_token == end_token:\n",
    "            break\n",
    "\n",
    "    detokenized_output = []\n",
    "    for token in decoded_sentence:\n",
    "        detokenized_output.append(lookup_table[token])\n",
    "    text = \" \".join(detokenized_output)\n",
    "    if use_post_processing:\n",
    "        text = post_process(text=text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def predict_main(\n",
    "        filepath,\n",
    "        examples,\n",
    "        decoder_sequence_length,\n",
    "        use_post_processing=False):\n",
    "    \"\"\"\n",
    "    Generate summarized text with the model file.\n",
    "    :filepath: the file path specifies the model\n",
    "    :examples: the list of text that is summarized.\n",
    "    :decoder_sequence_length: the maximum length of the summarized text.\n",
    "    \"\"\"\n",
    "    loaded_model = keras.models.load_model(\n",
    "        filepath,\n",
    "        # Just in case, TransformerDecoderModel is specified.\n",
    "        # However, it does not seem necessary.\n",
    "        custom_objects={\n",
    "            \"TransformerDecoderModel\": TransformerDecoderModel,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    decoder_tokenizer = loaded_model.decoder_tokenizer\n",
    "    vocab = decoder_tokenizer.get_vocabulary()\n",
    "    index_lookup_table = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "    summarized = []\n",
    "    for example in examples:\n",
    "        summarized.append(\n",
    "            decode_sequence(\n",
    "                example,\n",
    "                loaded_model,\n",
    "                decoder_sequence_length,\n",
    "                index_lookup_table,\n",
    "                use_post_processing=use_post_processing,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        print(\"ORIGINAL SENTENCE: \", examples[i])\n",
    "        print(\"SUMMARIZED RESULT: \", summarized[i])\n",
    "\n",
    "def predict_model(\n",
    "        model,\n",
    "        examples,\n",
    "        decoder_sequence_length,\n",
    "        use_post_processing=False):\n",
    "    \"\"\"\n",
    "    Generate summarized text with the model.\n",
    "    :model: the file path specifies the model\n",
    "    :examples: the list of text that is summarized.\n",
    "    :decoder_sequence_length: the maximum length of the summarized text.\n",
    "    \"\"\"\n",
    "    decoder_tokenizer = model.decoder_tokenizer\n",
    "    vocab = decoder_tokenizer.get_vocabulary()\n",
    "    index_lookup_table = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "    summarized = []\n",
    "    for example in examples:\n",
    "        summarized.append(\n",
    "            decode_sequence(\n",
    "                example,\n",
    "                model,\n",
    "                decoder_sequence_length,\n",
    "                index_lookup_table,\n",
    "                use_post_processing=use_post_processing,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        print(\"ORIGINAL SENTENCE: \", examples[i])\n",
    "        print(\"SUMMARIZED RESULT: \", summarized[i])\n",
    "\n",
    "def predict_dataset(\n",
    "        model,\n",
    "        iterable_dataset,\n",
    "        decoder_sequence_length,\n",
    "        use_post_processing=False):\n",
    "    \"\"\"\n",
    "    Generate summarized text with the model.\n",
    "    :model: the file path specifies the model\n",
    "    :iterable_dataset: the dataset, which is mainly test set, used to generate summarized text.\n",
    "    :decoder_sequence_length: the maximum length of the summarized text.\n",
    "    \"\"\"\n",
    "    decoder_tokenizer = model.decoder_tokenizer\n",
    "    vocab = decoder_tokenizer.get_vocabulary()\n",
    "    index_lookup_table = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    for entry in iterable_dataset:\n",
    "        text = entry[\"dialogue\"]\n",
    "        result = decode_sequence(\n",
    "            text,\n",
    "            model,\n",
    "            decoder_sequence_length,\n",
    "            index_lookup_table,\n",
    "            use_post_processing=use_post_processing,\n",
    "        )\n",
    "        y_true = entry[\"summary\"]\n",
    "        y_true = y_true.decode('utf-8')\n",
    "        y_pred = result.replace('[start]', '').replace('[end]', '').strip()\n",
    "        # print(y_true, '\\n\\t' , y_pred)\n",
    "        y_trues.append(y_true)\n",
    "        y_preds.append(y_pred)\n",
    "    return y_trues, y_preds\n",
    "\n",
    "def calculate_rouge_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    # RougeN metric\n",
    "    # @see https://keras.io/api/keras_nlp/metrics/rouge_n/\n",
    "    \"\"\"\n",
    "    rouge_n = keras_nlp.metrics.RougeN(order=2)\n",
    "    rouge_2_score = rouge_n(y_true, y_pred)\n",
    "    rouge_n = keras_nlp.metrics.RougeN(order=1)\n",
    "    rouge_1_score = rouge_n(y_true, y_pred)\n",
    "    rouge_l = keras_nlp.metrics.RougeL()\n",
    "    rouge_l_score = rouge_l(y_true, y_pred)\n",
    "    return rouge_1_score, rouge_2_score, rouge_l_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "        model,\n",
    "        train_ds,\n",
    "        validation_ds,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        steps_per_epoch,\n",
    "        now,\n",
    "        verbose,\n",
    "        callbacks=[]):\n",
    "    \"\"\"\n",
    "    Run training.\n",
    "    :train_ds: training set\n",
    "    :validation_ds: validation set\n",
    "    :optimizer: optimizer.Optimizer\n",
    "    :epochs: the number of epochs\n",
    "    :steps_per_epoch: the number of weight updates in a epoch\n",
    "    :now: timestamp\n",
    "    \"\"\"\n",
    "    metrics = [\n",
    "        masked_acc,\n",
    "        #keras.metrics.SparseCategoricalAccuracy(),\n",
    "        #keras_nlp.metrics.RougeN(), #  This cannot be used here\n",
    "    ]\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics,\n",
    "        loss=masked_loss,\n",
    "        weighted_metrics=[],\n",
    "    )\n",
    "    callbacks.append(get_tensorboard_callback(now=now))\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_ds,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RougeEvaluator(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to calculate ROUGE scores for every epoch\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset,\n",
    "            max_length):\n",
    "        \"\"\"\n",
    "        :dataset: dataset object that is used for ROUGE evaluation\n",
    "        :max_length: the max length of the summarized text\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.max_length = max_length\n",
    "  \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_true, y_pred = predict_dataset(\n",
    "            model=self.model,\n",
    "            iterable_dataset=tfds.as_numpy(self.dataset),\n",
    "            decoder_sequence_length=self.max_length\n",
    "        )\n",
    "        rouge_1_score, rouge_2_score, rouge_l_score = calculate_rouge_score(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred\n",
    "        )\n",
    "        print(rouge_1_score)\n",
    "        print(rouge_2_score)\n",
    "        print(rouge_l_score)\n",
    "\n",
    "def build_model(\n",
    "        train_ds, validation_ds, test_ds,\n",
    "        vectorization_layer,\n",
    "        num_decoders,\n",
    "        num_heads,\n",
    "        dropout,\n",
    "        transformer_intermediate_dim,\n",
    "        normalize_first,\n",
    "        layer_norm_epsilon,\n",
    "        decoder_sequence_length,\n",
    "        use_residual,\n",
    "        vocab_size,\n",
    "        batch_size,\n",
    "        embedding_dim,\n",
    "        mask_zero,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        steps_per_epoch,\n",
    "        verbose=1,\n",
    "        callbacks=[]):\n",
    "    \"\"\"\n",
    "    Build the model with specified parameters.\n",
    "    :return: model: trained model\n",
    "    :return: filepath: model file path if it is saved\n",
    "    :return: history: history object to plot\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    if DEBUGGER_V2:\n",
    "        tf.debugging.experimental.enable_dump_debug_info(\n",
    "            get_log_dir(now=now),\n",
    "            tensor_debug_mode=\"FULL_HEALTH\",\n",
    "            circular_buffer_size=-1\n",
    "        )\n",
    "    # before validation_ds is transformed\n",
    "    # if verbose > 0 and validation_ds is not None:\n",
    "    #     callbacks.append(RougeEvaluator(\n",
    "    #         dataset=validation_ds,\n",
    "    #         max_length=decoder_sequence_length,\n",
    "    #     ))\n",
    "    target_vectorization_layer = prepare_tokenizer(\n",
    "        vectorization_layer=vectorization_layer,\n",
    "        decoder_sequence_length=decoder_sequence_length,\n",
    "        max_tokens=vocab_size,\n",
    "    )\n",
    "    train_ds, validation_ds, test_ds = build_datasets(\n",
    "        train_ds=train_ds,\n",
    "        validation_ds=validation_ds,\n",
    "        test_ds=test_ds,\n",
    "        vectorization_layer=vectorization_layer,\n",
    "        batch_size=batch_size,\n",
    "        decoder_sequence_length=decoder_sequence_length,\n",
    "    )\n",
    "\n",
    "    target_vocab_size = target_vectorization_layer.vocabulary_size()\n",
    "    model = TransformerDecoderModel(\n",
    "        decoder_tokenizer=target_vectorization_layer,\n",
    "        num_decoders=num_decoders,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        transformer_intermediate_dim=transformer_intermediate_dim,\n",
    "        normalize_first=normalize_first,\n",
    "        layer_norm_epsilon=layer_norm_epsilon,\n",
    "        decoder_vocabulary_size=target_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        mask_zero=mask_zero,\n",
    "        decoder_sequence_length=decoder_sequence_length,\n",
    "        use_residual=use_residual,\n",
    "    )\n",
    "    # This should be called seemingly.\n",
    "    # model.build(\n",
    "    #     input_shape=(\n",
    "    #         (None, decoder_sequence_length)\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    history = run_training(\n",
    "        model,\n",
    "        train_ds=train_ds,\n",
    "        validation_ds=validation_ds,\n",
    "        optimizer=optimizer,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        now=now,\n",
    "        verbose=verbose,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    timestamp = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    if is_running_on_apple_silicon() or is_running_on_intel_mac():\n",
    "        filepath = f'model/summarization_model_{timestamp}.keras'\n",
    "    else:\n",
    "        filepath = f'summarization_model_{timestamp}.keras'\n",
    "    print(f\"Saving to {filepath}\")\n",
    "    model.save(filepath=filepath)\n",
    "\n",
    "    print(f\"Successfully saved model to {filepath}\")\n",
    "    return model, filepath, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size | CPU | GPU\n",
    "--- | --- | ---\n",
    "--- | 7m52s | > 25m\n",
    "32 | 1m41s | 2m26s\n",
    "64 | 1m37s | 2m5s\n",
    "128 | 1m37s | 1m49s\n",
    "256 | 1m31s | 1m40s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 01:05:44.383292: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-08-24 01:05:44.383313: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-08-24 01:05:44.383316: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-08-24 01:05:44.383330: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-08-24 01:05:44.383340: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-08-24 01:05:45.532174: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "ADAPT_BATCH_SIZE = 256\n",
    "\n",
    "train_ds, validation_ds, test_ds = prepare_datasets()\n",
    "vectorization_layer = keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    # max_tokens=VOCABULARY_SIZE,\n",
    "    output_mode='int',\n",
    "    ragged=True,\n",
    ")\n",
    "vectorization_layer.adapt(\n",
    "    train_ds.batch(ADAPT_BATCH_SIZE).map(\n",
    "        lambda row: '[start] ' + row['summary'] + ' ' + row['dialogue'] + ' [end]',\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    ),\n",
    "    batch_size=ADAPT_BATCH_SIZE,\n",
    ")\n",
    "# Use the maximum size of the dataset. 34219\n",
    "VOCABULARY_SIZE = len(vectorization_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', ':', '.', ',', 'i', '?', 'the', 'to', 'you', '!', 'a']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorization_layer.get_vocabulary()[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'transformer_decoder_model', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "STEPS_PER_EPOCH = None\n",
    "# ENCODER_SEQUENCE_LENGTH = 128 # 128: 75% covered. max is 803.\n",
    "# DECODER_SEQUENCE_LENGTH = 64 # 32: 75% covered. max is 64\n",
    "DECODER_SEQUENCE_LENGTH = 128\n",
    "MASK_ZERO = False\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "model, filepath, history = build_model(\n",
    "    train_ds, validation_ds, test_ds,\n",
    "    vectorization_layer=vectorization_layer,\n",
    "    num_decoders=2,\n",
    "    num_heads=6,\n",
    "    dropout=0.35,\n",
    "    transformer_intermediate_dim=512,\n",
    "    normalize_first=False,\n",
    "    layer_norm_epsilon=1e-5,\n",
    "    decoder_sequence_length=DECODER_SEQUENCE_LENGTH,\n",
    "    use_residual=True,\n",
    "    vocab_size=VOCABULARY_SIZE,\n",
    "    batch_size=128,\n",
    "    embedding_dim=128,\n",
    "    mask_zero=MASK_ZERO,\n",
    "    optimizer=optimizer,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=[reduce_lr_callbacks],\n",
    ")\n",
    "plot(history=history)\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "best_epoch = max(np.argmin(history.history[f\"val_loss\"]), np.argmax(history.history[f\"val_masked_acc\"]))\n",
    "best_epoch = best_epoch + 1\n",
    "print(\"best_epoch is\", best_epoch)\n",
    "model, filepath, history = build_model(\n",
    "    train_ds.concatenate(validation_ds), None, test_ds,\n",
    "    vectorization_layer=vectorization_layer,\n",
    "    num_decoders=2,\n",
    "    num_heads=6,\n",
    "    dropout=0.35,\n",
    "    transformer_intermediate_dim=512,\n",
    "    normalize_first=False,\n",
    "    layer_norm_epsilon=1e-5,\n",
    "    decoder_sequence_length=DECODER_SEQUENCE_LENGTH,\n",
    "    use_residual=True,\n",
    "    vocab_size=VOCABULARY_SIZE,\n",
    "    batch_size=128,\n",
    "    embedding_dim=128,\n",
    "    mask_zero=MASK_ZERO,\n",
    "    optimizer=optimizer,\n",
    "    epochs=best_epoch,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=[reduce_lr_callbacks],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = predict_dataset(\n",
    "    model=model,\n",
    "    iterable_dataset=tfds.as_numpy(test_ds), # The length of the test dataset is 819.\n",
    "    decoder_sequence_length=DECODER_SEQUENCE_LENGTH,\n",
    "    use_post_processing=True,\n",
    ")\n",
    "rouge_1_score, rouge_2_score, rouge_l_score = calculate_rouge_score(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred,\n",
    ")\n",
    "print(rouge_1_score)\n",
    "print(rouge_2_score)\n",
    "print(rouge_l_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\",\n",
       "  'Eric and Rob are going to watch a stand-up on youtube.',\n",
       "  \"Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\",\n",
       "  'Emma will be home soon and she will let Will know.',\n",
       "  \"Jane is in Warsaw. Ollie and Jane has a party. Jane lost her calendar. They will get a lunch this week on Friday. Ollie accidentally called Jane and talked about whisky. Jane cancels lunch. They'll meet for a tea at 6 pm.\"],\n",
       " ['Hannah is hannah to the for the to...',\n",
       "  \"Eric is a with [ unk ].. Cooler 's......... They and......... Snowfalls...... And\",\n",
       "  'Lenny and a to a the.............. Drafted...',\n",
       "  \"The will jonah 's to going to for for for dinner tonight.. Will in... Thai...\",\n",
       "  'Ollie and to are going to to............ 21. There. Muffins. Fan..... Come. Forest...............'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:5], y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
