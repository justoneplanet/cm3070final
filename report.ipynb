{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review\n",
    "\n",
    "## Introduction\n",
    "\n",
    "I aim to explore the current understanding of machine learning model architectures and their performances, focusing on the key technology \"Transformer\". Moreover, in this project, I investigate the effectiveness of the Transformer and finally build a text summarization model.\n",
    "\n",
    "### Transformer\n",
    "\n",
    "Vaswani et al. (2017) proposed a new simple architecture, named Transformer, based on attention mechanisms without recurrence and convolutions. [1]\n",
    "\n",
    "The architecture of the Transformer, which consists of both encoders and decoders, is introduced with a simple figure.\n",
    "\n",
    "The paper indicates the algorithms and architecture of the Transformer with formulas, comparing the former architectures such as the recurrent neural network model and the gate recurrent unit model. How to reduce the amount of calculation for longer input sequences, which is one of the defects, is also introduced kindly. However, because of the nature of the paper, the concrete implementation is not shown on it. Instead, it provides a link to a GitHub repository, \"tensor2tensor,\" where some implementations with Python code are introduced.\n",
    "\n",
    "Though these codes might still be beneficial, They have not been maintained for years. It is pretty uncertain whether it can run on the current Python environment.\n",
    "\n",
    "Because the Transformer was initially developed for the machine translation task, various tasks except translation are not mentioned in this paper written in 2017. Thereby, the effectiveness of the Transformer is not investigated for a wide range of NLP tasks.\n",
    "\n",
    "As mentioned at the beginning, hyperparameter tunings are obviously required to investigate effectiveness. The section \"6.2 Model Variations\" describes the parameters such as the number of heads, the parameters of the optimizer, and so on that exist in the Transformer model, and the same indicators will be used to compare the performance in this project. And the experiment environment, which the paper disclosed, is useful to the experiments in this project.\n",
    "\n",
    "There are not following items that are essential for this project in the paper.\n",
    "\n",
    "- Sufficient benchmarks for various NLP tasks/applications\n",
    "- Minimum viable and runnable code in the modern environment\n",
    "\n",
    "However,\n",
    "\n",
    "\n",
    "項目は使える\n",
    "有用な指標を提供している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "#### Abstract\n",
    "\n",
    ">  We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\n",
    "The proposal of a new simple network architecture.\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "The recurrent models have disadvantages: They are difficult to compute in parallel, and memory constraints make processing longer sequences difficult.\n",
    "\n",
    "The attention mechanism makes it possible to model the dependencies without regard to their distance in the input or output sequences.\n",
    "\n",
    "The Transformer can compute parallel and improve the performance.\n",
    "\n",
    "#### Background\n",
    "\n",
    "The background of necessity for the Transformer\n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "Simply introduced the Transformer model that consists of encoder and decoder.\n",
    "\n",
    "\n",
    "\n",
    "> self-attention could yield more interpretable models\n",
    "付加的advantage\n",
    "\n",
    "> While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
    "\n",
    "headsは多くても少なくても良く無い。hyperparameterの一つである\n",
    "\n",
    "> Constituency parsing\n",
    "句解析で翻訳以外でどうなのかbenchmarkしている（RNNと比べて）\n",
    "\n",
    "heading書いてそれぞれ1行でsummaryとcritisizeしても良いかも\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "It improves the calculation of \n",
    "\n",
    "\n",
    "\n",
    "飛躍的に向上した\n",
    "\n",
    "一方で実際にどういうケースで\n",
    "\n",
    "それは元々翻訳タスクを目的にされたもので\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Vaswani et al. (2017). Attention Is All You Need.https://doi.org/10.48550/arXiv.1706.03762\n",
    "2. tensorflow (n.d.). Tensor2tensor. Retrieved May 16, 2024, from https://github.com/tensorflow/tensor2tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
