{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "1. **[Introduction](#Introduction)**: this will explain the project concept and motivation for the project (this can be based on your proposal). This must also state which project template you are using.  \n",
    "2. **[Literature Review](#Literature-Review)**: this is a revised version of the document that you submit for your second peer review.\n",
    "3. **[Design](#Design)**: this is a revised version of the document that you submit for your third peer review.\n",
    "4. **[Implementation](#Implementation)**: this should describe the implementation of the project. This should follow the style of the topic 6 peer review (but greatly expanded to cover the entire implementation), describing the major algorithms/techniques used, explanation of the most important parts of the code and a visual representation of the results (e.g. screenshots or graphs). \n",
    "5. **[Evaluation](#Evaluation)**: describe the evaluation carried out (e.g. user studies or testing on data) and give the results. This should give a critical evaluation of the project as a whole making clear successes, failures, limitations and possible extensions.\n",
    "6. **[Conclusion](#Conclusion)**: This can be a short summary of the project as a whole but it can also bring out any broader themes you would like to discuss or suggest further work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Introduction\n",
    "\n",
    "In this project, I use public datasets, such as SAMSum and CNN daily mail datasets, to build a text summarization model. Text summarization task means generating a short organized summary from a long article, such as news, through extracting and abstracting.\n",
    "\n",
    "This will be a research project in which I investigate recent trends and deepen my knowledge of Deep Learning, which goes beyond the contents of CM3015, and a text summarization model is eventually built.\n",
    "\n",
    "In addition, this project supposes an organization or a group is the user, such as a (web) system development company with little machine-learning knowledge. Therefore, the deliverable report must be scientific and convincing to explain why each parameter is adopted and be knowledgeable for its developers. The deliverable code and model must run easily on common devices, such as a web server. Mobile devices, on the other hand, might not be able to run the large model because they have a smaller amount of memory.\n",
    "\n",
    "## What is interesting and motivative (Rapid Evolution) \n",
    "\n",
    "In recent years, various types of text representations and neural network models have been developed, especially since the Transformer model was invented.\n",
    "\n",
    "### Text representation\n",
    "\n",
    "In addition to the One-hot-encoding introduced in CM3015, I am going to use other techniques, such as word2vec and GloVe, that can appropriately handle the meanings of each word.\n",
    "\n",
    "The neural network is a vector transformation. It is worth investigating how well each representation of embedding layers can capture the characteristics of text data.\n",
    "\n",
    "### Transformer\n",
    "\n",
    "The transformer is one of the most common and game-changing models for sequential data, such as text. In addition, it is also used in text-to-image generative artificial intelligence (AI), large language models (LLM), and so on. There are a lot of derived models based on the Transformer architecture, such as BERT, which outperforms others in natural language processing. [10] Whereas another model based on the Transformer, named Vision Transformer, has been used for image recognition tasks to overcome CNN's weaknesses recently. [9] Moreover, there is a more efficient model based on the Transformer model, which is called the Retentive Network. Because all these architectures are based on the Transformer model, deepening its knowledge leads to the understanding of whole machine learning. Additionally, even in the future, a number of new architectures based on the Transformer models will be released.\n",
    "\n",
    "## What is interesting and motivative (from the aspect of efficiency and ecology)\n",
    "\n",
    "Measuring the performances of various models is one of the exciting and essential points.\n",
    "\n",
    "Advanced models do not always achieve the best performance. For example, when the Recurrent Neural Network model, which is one of the advanced models, is used for a text classification task, even though it does not always work better than a dense model, it mostly consumes greater computing resources. [11] That is, it completely wastes resources. I am going to analyze the strengths and weaknesses of each model in terms of practical uses.\n",
    "\n",
    "As recent investigations indicate, building LLM models indirectly emits a significant amount of CO2. That is, the investigation of efficiency that leads to the ecology is socially meaningful, even in the environmental aspect. However, since there are not enough computing resources to build an LLM model, this project does not target building an LLM itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Literature Review\n",
    "\n",
    "I aim to explore the current understanding of machine learning model architectures and their performances, focusing on the key technology \"Transformer\". Moreover, in this project, I investigate the effectiveness of the Transformer and finally build a text summarization model. Specifically, the following items were reviewed. The bold items of them did not exist in the preliminary report and were added in the final report.\n",
    "\n",
    "- Transformer\n",
    "- **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**\n",
    "- **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension**\n",
    "- RetNet\n",
    "- Neural Text Summarization: A Critical Evaluation\n",
    "    - **ROUGE: A Package for Automatic Evaluation of Summaries**\n",
    "- **SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization**\n",
    "- Text Summarization with Pretrained Encoders\n",
    "- facebook/bart-large-cnn\n",
    "- google/pegasus-cnn_dailymail\n",
    "\n",
    "## Transformer\n",
    "\n",
    "Vaswani et al. (2017) proposed a new simple architecture, named Transformer, based on attention mechanisms without recurrence and convolutions. [1]\n",
    "\n",
    "The architecture of the Transformer, which consists of both encoders and decoders, is introduced with a simple figure.\n",
    "\n",
    "The paper indicates the algorithms and architecture of the Transformer with formulas, comparing the former architectures, such as the recurrent neural network model and the gate recurrent unit model. How to reduce the amount of calculation for longer input sequences, which is one of the defects, is also introduced kindly. However, because of the nature of the paper, the concrete implementation is not shown on it. Instead, it provides a link to a GitHub repository, \"tensor2tensor,\" [2] where some implementations with Python code are introduced. However, since the repository has not been updated by the owner, it is pretty uncertain whether they can run on the current Python environment or not.\n",
    "\n",
    "In addition, because the Transformer was initially developed for the machine translation task, various tasks except translation are not mentioned in this paper, written in 2017. Thereby, the effectiveness of the Transformer is not investigated for a wide range of NLP tasks here.\n",
    "\n",
    "As mentioned at the beginning, hyperparameter tunings are obviously required to investigate effectiveness. The section \"6.2 Model Variations\" describes the parameters such as the number of heads, the parameters of the optimizer, and so on that exist in the Transformer model, and the same indicators will be used to compare the performance in this project. And the experiment environment, which the paper disclosed, is useful to the experiments in this project.\n",
    "\n",
    "I have obtained the following beneficial knowledge, even though it is not directly related to my project.\n",
    "\n",
    "- The number of heads is neither too many nor too few\n",
    "- Interpretable models\n",
    "\n",
    "The paper does not include the following items, which are required for this project.\n",
    "\n",
    "- Sufficient benchmarks for various NLP tasks/applications\n",
    "- Minimum viable and runnable code in the modern environment\n",
    "\n",
    "## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "\n",
    "Jacob D. et al. introduced a new language representation model Bidirectional Encoder Representations from Transformers (BERT) that can be fine-tuned with one additional output layer for various tasks. [34]\n",
    "\n",
    "- BERT uses masked language models to enable pretrained deep bidirectional representations, not unidirectional.\n",
    "- A distinctive feature of BERT is its unified architecture across different tasks.\n",
    "\n",
    "The following new features are used in BERT.\n",
    "\n",
    "- Masked language model\n",
    "- Next sentence prediction\n",
    "\n",
    "And we have to care the following point.\n",
    "\n",
    "> For $BERT_{LARGE}$ we found that finetuning was sometimes unstable on small datasets.\n",
    "\n",
    "Finally, the effectiveness and necessity of pretrainning and transfer learning have been shown in this paper. It is indicated that these techniques are essential for computationally or quantitatively less resources and applicable for a wide range of NLP tasks.\n",
    "\n",
    "## BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
    "\n",
    "Mike L. et al. presented BART, a denoising autoencoder for pretraining sequence-to-sequence models. [35] BART is simply expressed of a combined model with the following parts;\n",
    "\n",
    "- BEAT\n",
    "- Auto regressive decoder\n",
    "\n",
    "And the following diagram, which is described in the paper, depicts primary types of architectures of BART.\n",
    "\n",
    "<img src=\"./img/Screenshot 2024-08-07 at 23.54.42.png\" width=\"75%\" />\n",
    "\n",
    "Re-implemented pre-training approaches are introduced as follows.\n",
    "\n",
    "- Language Model: training next words left-to-right.\n",
    "- Permuted Language Model: generating sampled 1/6 of the tokens in a random order.\n",
    "- Masked Language Model: as same as the BERT model, 15% of tokens are masked and predicted.\n",
    "- Multitask Masked Language Model: in addition to GPT left-to-right and BERT random masks, added various masks, includes right-to-left. \n",
    "- Masked Seq-to-Seq: masked 50% of tokens of a span, trained a Seq-to-Seq model, and predicted them.\n",
    "\n",
    "The following tasks are used to compare the performances of each pre-training.\n",
    "\n",
    "- SQuAD: extractive question answering task on Wikipedia paragraphs.\n",
    "- MNLI: a bitext classification task to predict whether one sentence entails another.\n",
    "- ELI5: answering task for a long-form abstractive question.\n",
    "- XSum: news text summarization task\n",
    "- ConvAI2: dialogue text generation task\n",
    "- CNN/DM: text summarization task\n",
    "\n",
    "## RetNet\n",
    "\n",
    "Yutao et al. (2023) proposed the Retentive Network (RetNet), which is a strong successor to Transformer for large language models.[3]\n",
    "\n",
    "One of the drawbacks of the Transformer, which is introduced in this paper, is the slow inference and inefficiency due to multi-head attention.\n",
    "The performance comparison between the RetNet and the Transformer has been executed for large language models and represents the improvement. They are executed and compared for large language models and that difference might not be remarkable on smaller models such as text summarization tasks.\n",
    "\n",
    "The section \"3 Experiments\" mentions the following:\n",
    "\n",
    "> Perplexity decreases along with scaling up the model size. We empirically observe that RetNet tends to outperform Transformer when the model size is larger than 2B. [3]\n",
    "\n",
    "When the model size is larger than 2.7B, with more than 2.7 billion parameters, the advantages of the RetNet model might finally be detected.\n",
    "\n",
    "The section \"3.3 Training Cost\" shows the specific environment where the evaluation of this paper was executed as follows.\n",
    "\n",
    "> We evaluate the results with eight Nvidia A100-80GB GPUs, because FlashAttention is highly optimized for A100. Tensor parallelism is enabled for 6.7B and 13B models. [3]\n",
    "\n",
    "Building a text summarization model in this project is mainly executed on the local macOS machine, which has an M2 CPU, which has a significantly different architecture from Nvidia GPUs.\n",
    "\n",
    "> Moreover, without relying on specific kernels, it is easy to train RetNet on other platforms efficiently. For example, we train the RetNet models on an AMD MI200 cluster with decent throughput. It is notable that RetNet has the potential to further reduce cost via advanced implementation, such as kernel fusion. [3]\n",
    "\n",
    "Even though the above quote mentioned the performance on the other device, the environment is highly parallelized, and it is uncertain if this project can derive beneficial knowledge through experimental comparisons.\n",
    "\n",
    "Though this does not matter directly to this project, as the sections \"2.3 Overall Architecture of Retention Networks\" and \"3.4 Inference Cost\" mention, the inference cost of the RetNet model is remarkably $O(1)$. [3] In the case where the quick response for longer sequences/tokens is required, the RetNet model can be one of the realistic choices.\n",
    "\n",
    "## Neural Text Summarization: A Critical Evaluation\n",
    "\n",
    "Wojciech et al. (2019) introduced and evaluated the current shortcomings of text summarization.[4] The methodology of the evaluation should be notable in this paper. The previously introduced 2 papers are not for text summarization tasks, and the BLEU metric is used, which is for the evaluation of translation task performance. This is not for summarization tasks. Though loss values probably evaluate the performances, they cannot clearly consider sentences' structures for evaluations. Therefore, following this paper, the ROUGE metric is used in this project. Fortunately, KerasNLP has the following 2 metrics as standard.\n",
    "\n",
    "- [ROUGE-N](https://keras.io/api/keras_nlp/metrics/rouge_n/)\n",
    "- [ROUGE-L](https://keras.io/api/keras_nlp/metrics/rouge_l/)\n",
    "\n",
    "Note that the ROUGE-N metric uses n-gram to refer to the overlap, and the ROUGE-L metric uses the longest common subsequence that can detect the sentence level structure. [8]\n",
    "\n",
    "Though the following does not matter directly, some shortcomings of the current summarization model are introduced here.\n",
    "\n",
    "- Even though a summarization task depends on the reader's expectations and prior knowledge, the current models are not provided as additional information.\n",
    "- Even though the ROUGE metric is widely used, it does not factually and consistently examine summarized text, but just lexically.\n",
    "- The bias and diversity of the dataset.\n",
    "\n",
    "### ROUGE: A Package for Automatic Evaluation of Summaries\n",
    "\n",
    "Chin-Yew Lin suggested the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) to measure the quality of a summary text. [36] Tough, human judgments were traditionally used for the evaluation of the text summarization quality, it was time-consuming and costly even for simple manual evaluation, and it is impossible to be frequently executed. ROUGE is one of the methods, which correlates to human judgments, to programmatically measure the quality of summarized text. The quation is introduced as follows;\n",
    "\n",
    "> $ROUGE-N = \\frac{\\sum_{S∈ReferenceSummaries}\\sum_{gram_n∈S}Count_{match}(gram_n)}{\\sum_{S∈(ReferenceSummaries)\\sum_{gram_n∈S}Count(gram_n)}}$\\\n",
    "> [36]\n",
    "\n",
    "That is, matching of n-gram is used to evaluate the quality.\n",
    "\n",
    "## SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\n",
    "\n",
    "Bogdan Gliwa et al. introduced the SAMSum Corpus, which is an abstractive dialogue summaries. [37] The paper shows that dedicated models and non-standard evaluations are required for an abstractive dialogue summarization.\n",
    "\n",
    "## Text Summarization with Pretrained Encoders\n",
    "\n",
    "Yang et al. (2019) indicated the effectiveness of the BERT, which is an abbreviation of Bidirectional Encoder Representations from Transformers, for text summarization, which is one of the NLP tasks. [5]\n",
    "\n",
    "In this paper, the Bidirectional Encoder Representations from Transformers(BERT) model, where a pre-trained model and a scratch model are combined, is compared with other typical models for text summarization tasks, and it is concluded that the BERT model outperformed others. However, it seems slightly unclear whether the pre-trained model actually works better because there is not a comparison of the same architecture model between pre-trained and scratch.\n",
    "\n",
    "As the section \"Introduction\" mentions, the BERT model, one of the Transformer models, has affected word and sentence representation. Thus, this model may become a research target in this project.\n",
    "\n",
    "> When fine-tuning for a specific task, unlike ELMo whose parameters are usually fixed, parameters in BERT are jointly fine-tuned with additional task- specific parameters. [5]\n",
    "\n",
    "Whereas, as the above is written, since the pretrained BERT model goes through the fine-tuning phase, it might take longer time for training than fixed parameters' models such as word2vec and GloVe.\n",
    "\n",
    "The table of the section \"3.3 Abstractive Summarization\" holds the various types of datasets.\n",
    "Though this might not matter to the paper, apart from CNN DailyMail, NYT, and XSum are used as the datasets. In this project, the CNN dailymail will be utilized as a dataset. However, they are kept as sub plans, preparing for unexpected situations.\n",
    "\n",
    "> Each token $w_i$ is assigned three kinds of embeddings: token embeddings indicate the meaning of each token, segmentation embeddings are used to discriminate between two sentences (e.g., during a sentence-pair classification task) and position embeddings indicate the position of each token within the text sequence. [5]\n",
    "\n",
    "If the BERT model is used, differences in embeddings must be addressed.\n",
    "\n",
    "## facebook/bart-large-cnn\n",
    "\n",
    "This is a text summarization model developed by Facebook that is available on Hugging Face. [30]\n",
    "\n",
    "Particularly noteworthy is the model size and its performance. The model size, the number of parameters, is 406M, and the results of ROUGE-1, ROUGE-2, and ROUGE-L are 42.949, 20.815, and 30.619 respectively.\n",
    "\n",
    "In addition, pre-trained BART is used in this text summarization model.\n",
    "\n",
    "> BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. [30]\n",
    "\n",
    "> BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs. [30]\n",
    "\n",
    "The concrete specifications are as follows. [30]\n",
    "\n",
    "Parameter | Value\n",
    "--- | ---\n",
    "Self-attention heads | 16\n",
    "Encoder layers | 12\n",
    "Feed forward network dim | 4096\n",
    "Dropout | 0.1\n",
    "Max length | 142\n",
    "Max position emneddings | 1024\n",
    "Min length | 56\n",
    "\n",
    "## google/pegasus-cnn_dailymail\n",
    "\n",
    "This is another text summarization model developed by Google that is available on Hugging Face. [31]\n",
    "\n",
    "This model has been trained not only with the CNN dailymail dataset but also with many others. It has 568M parameters and consists of 12 Transformer encoder layers and 12 Transformer decoder layers. The sinusoidal method is used for the positional embedding. The number of vocabulary is 50265.\n",
    "\n",
    "In addition, the paper \"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\" shows that the results of ROUGE-1, ROUGE-2, and ROUGE-L are 44.16, 21.56, and 41.30 respectively. [32]\n",
    "\n",
    "The concrete specifications of the Pegasus base model are as follows. [32]\n",
    "\n",
    "Parameter | Value\n",
    "--- | ---\n",
    "Self-attention heads | 12\n",
    "Encoder layers | 12\n",
    "Feed forward network dim | 3072\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, the superiority and effectiveness of the Transformer and its self-attention have been shown, compared with the other architectures such as the recurrent neural network and convolutional neural network. Whereas performances, tunings, other NLP tasks, and architectures, which determine if encode or/and decode is used, have not been shown. And that is what I aim to do.\n",
    "\n",
    "I have slightly changed my mind while reading papers. Specifically, instead of regarding the RetNet model as the main topic, hyperparameter tunings of the Transformer model are going to be mainly researched. The RetNet model will be researched optionally. The reason is as follows.\n",
    "\n",
    "- The RetNet paper shows that it is effective only for large language models.\n",
    "- The experiments on the ResNet paper were executed on the highly parallelized environment, which is different from the local machine that is used in this project.\n",
    "- The training to build a text summarization model takes a considerable amount of time.\n",
    "\n",
    "Therefore, because a beneficial report may not be provided if most resources are invested in experiments with little chance of seeing differences, I am going to make the experiment about the RetNet model optional. Instead, clearly beneficial hyperparameter tunings are executed in the first half to ensure users' benefits.\n",
    "\n",
    "Moreover, the comparison between the pre-trained models and non-pre-trained models will also be one of the interesting topics.\n",
    "\n",
    "Finally, it might be very difficult to self-build a text summarization model and mark a better score of ROUGE metrics on the current local machine because the models of Google and Facebook are trained with a number of layers and parameters using high computational GPUs. However, it might be possible to build a smaller model that is beneficial for a specific environment. Since KerasNLP provides an easy way to use Bidirectional Autoregressive Transformer (BART), the performance might be able to get close to the Google or Facebook results if it is used. [33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Design\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "I have chosen the template \"Deep Learning on a public dataset\" of CM3015 \"Machine Learning and Neural Networks\". In this project, I build a text summarization model with public datasets, such as the SAMSum corpus and the Cable News Network(CNN) dailymail datasets.\n",
    "\n",
    "Unlike other templates of other courses, the deliverables are the machine learning model and its documents, and this project does not have user interfaces for end users. However, I assume to deliver the machine learning model to an organization that has a plan where the text summarization model is integrated into a system, which has actual end users.\n",
    "\n",
    "There is also the aspect of a research project where I investigate recent trends and deepen my knowledge of Deep Learning that was not sufficiently covered in the CM3015 course. Day by day and month by month, the field and market of deep learning technology for natural language processing is rapidly growing. The deliverable document is going to be beneficial for developers.\n",
    "\n",
    "## Domain and Users\n",
    "\n",
    "As I mentioned above, unlike other project templates, the project template on CM3015 does not have a specific user. However, since it is difficult to determine what is beneficial and how beneficial a feature is without specific users, I assume that users are developers outside the organization who want to install a machine learning feature and model into a currently running system. Specifically, that is as follows.\n",
    "\n",
    "They are not familiar with deep learning but with server-side applications of Python, and they want to adopt a text summarization feature into a current running system. My job is solving the problem and providing text summarization models and their knowledgeable and beneficial documents to them.\n",
    "\n",
    "## Justify features based on domain and users\n",
    "\n",
    "Generally, end users do not mind if a model is highly advanced or not. Instead, they mind the time and performance of response.\n",
    "\n",
    "### Why is the Transformer model used?\n",
    "\n",
    "In this project, text summarization models are built. That is, the neural network learns sequences as the input to output another sequences as the output. The Transformer model is superior to the Dense model to process sequences data. That is the reason why the Transformer model is used for the text summarization model.\n",
    "\n",
    "On text classification tasks, well-tuned dense layer models overcome advanced models, such as a Transformer model, an Recurrent Neural Network(RNN) model and so on, under certain conditions.\n",
    "\n",
    "However, text generation tasks, such as text summarization, could not achieve satisfactory outcomes for a long time. Encoder-decoder models, text embeddings, and Transformer, which can handle sequences well have made them possible. That is why the Transformer model is used.\n",
    "\n",
    "### Why is hyperparameter tuning necessary?\n",
    "\n",
    "There are a number of hypterparameters in a neural network model, and there does not exist a formula that can derive optimized their values. They depend on each dataset and architecture. The best parameter set must be found with hyperparameter tunings.\n",
    "\n",
    "Prior to testing advanced models, the hyperparameter tunings should be executed sufficiently. Even if an advanced model such as RetNet outperforms a traditional model, it is impossible to compare the advanced model and the traditional model fairly without hyperparameter tunings.\n",
    "\n",
    "Fairly comparisons derive better models that are beneficial for users. That is the reason why hypter parameter tunings are necessary.\n",
    "\n",
    "### Why is the RetNet model reseached optionally?\n",
    "\n",
    "Optionally, the other cutting-edge models, such as the RetNet model, are examined. Generally, end users do not mind if a model is highly advanced model or not. Instead, they mind the time and performance of response. Whereas, it might be able to efficiently use resources, and that might lead to the reduction of the cost for users. This is one of the reason why the research of the RetNet model is made optional.\n",
    "\n",
    "### Which dataset is mainly used?\n",
    "\n",
    "In this reseach, the following datasets are compared to choose one of them to build a model. These datasets have been mainly from the Tensorflow datasets catalog since this project adopted Tensorflow. [38](Some of the following datasets was not able to be accessed with the Tensorflow datasets library with DownloadError.)\n",
    "\n",
    "__ | dataset/download size | max sequence length (75%-covered)\n",
    "--- | --- | ---\n",
    "cnn_dailymail | 1.29 GiB | 2347/1296 (877/60)\n",
    "big_patent | 9.45 GiB | -\n",
    "newsroom | 5.13 GiB | -\n",
    "reddit | 18.09 GiB | -\n",
    "scientific_papers | 4.20 GiB | -\n",
    "aeslc | 14.96 MiB | 3136/15 (131/5)\n",
    "billsum | 260.84 MiB | 3055/808 (1644/240)\n",
    "booksum | - | -\n",
    "databricks_dolly | 12.69 MiB | -\n",
    "samsum | 10.71 MiB | 803/64 (128/27)\n",
    "\n",
    "Initially, I chose the cnn_dailymail dataset according to the 2 reasons of the sufficient size for training and widely known news media and I have implemented the prototype text summarization model. It was supposed that experiments go well.\n",
    "\n",
    "However, when I tried to increase the encoder/decoder sequence size or the vocabulary size, the training phase exhausted the RAM and the out of memory error finally caused, even though the local environment has 32GB RAM. That is, the local experiment environment did not satisfy the minimum requirement with this dataset size.\n",
    "\n",
    "As a result, I must have changed the dataset into smaller one and the samsum dataset, which was the smallest, was the best choice. This dataset published by Samsung Research and Development is one of the reliable datasets.\n",
    "\n",
    "## Structure of the project\n",
    "\n",
    "The main deliverable is a model for the text summarization task and its Jupiter notebook document. The model is provided with minimum viable code to run it. Multiple models are possibly provided because the difference in the size of each model affects the machine's requirements where it runs. The notebook's headings will be as follows.\n",
    "\n",
    "- Abstract\n",
    "- Introduction\n",
    "- Background\n",
    "    - Mathematic explanation\n",
    "    - Scientific description\n",
    "    - Technical description\n",
    "- Experiments/Methodology\n",
    "- Conclusion\n",
    "    - Best model\n",
    "    - Verification of hypotheses\n",
    "    - Future work\n",
    "- Citation/Reference\n",
    "\n",
    "## Key technologies and methods\n",
    "\n",
    "I introduce libraries and technologies here. To be correct and objective, I explain them in my own words as little as possible, citing public documents, and websites. Instead, I explain how each technology and library is used in my own words for the project.\n",
    "\n",
    "#### TensorFlow\n",
    "\n",
    "This is a machine learning library, which is mainly used in this project.\n",
    "\n",
    "> An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources. [12]\n",
    "\n",
    "In this project, the TensorFlow library is used not only for building text summarization models but also for most other tasks from the beginning to the end.\n",
    "\n",
    "#### TensorBoard\n",
    "\n",
    "This is a visualization tool for machine learning program such as Tensorflow and PyTorch. This is used to visualize Transformer models and debug them.\n",
    "\n",
    "#### Keras\n",
    "\n",
    "This is an OSS neural network library. It depends on the library version, it can switch Tensorflow to other machine learning library.\n",
    "\n",
    "> Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. Keras also gives the highest priority to crafting great documentation and developer guides. [14]\n",
    "\n",
    "This library is used throughout the whole project to manipulate the TensorFlow library.\n",
    "\n",
    "#### KerasNLP\n",
    "\n",
    "> KerasNLP is a natural language processing library that works natively with TensorFlow, JAX, or PyTorch. Built on Keras 3, these models, layers, metrics, and tokenizers can be trained and serialized in any framework and re-used in another without costly migrations. [16]\n",
    "\n",
    "In this project, the Transformer model, which consists of the Transformer encoder layer and the Transformer decoder layer, is used to build text summarization models. It is possible to self-implement them by following the paper and books. However, it might unintentionally include bugs. Then, the model and its documents lose the reliability for readers. Therefore, the KerasNLP, which is a part of the widely known library \"Keras\" and contains the Transformer layer classes, is utilized here to secure the reliability.\n",
    "\n",
    "Moreover, a lot of hyperparameter tunings are executed in this project. Their classes in KerasNLP is well-architected so that hyperparameters are easily injected to each layer. This is another reason why the KerasNLP is used here.\n",
    "\n",
    "#### PyTorch\n",
    "\n",
    "This is another OSS machine learning library. PyTorch is featured with the following characteristics according to the official website.\n",
    "\n",
    "> **Production Ready**\\\n",
    "> Transition seamlessly between eager and graph modes with TorchScript, and accelerate the path to production with TorchServe.\\\n",
    "> **Distributed Training**\\\n",
    "> Scalable distributed training and performance optimization in research and production is enabled by the torch.distributed backend.\\\n",
    "> **Robust Ecosystem**\\\n",
    "> A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more.\\\n",
    "> **Cloud Support**\\\n",
    "> PyTorch is well supported on major cloud platforms, providing frictionless development and easy scaling.\\\n",
    "> [17]\n",
    "\n",
    "It is not directly used for the project development. However, some text summarization models that I have introduced has been developed with this PyTorch library.\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "This is a software library of Hugging Face, Inc. The official repository of GitHub introduces as follows.\n",
    "\n",
    "> Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. [18]\n",
    "\n",
    "> Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. [18]\n",
    "\n",
    "> Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. [18]\n",
    "\n",
    "I have investigated the following 2 text summarization models as a part of competitive research.\n",
    "\n",
    "- facebook/bart-large-cnn [19]\n",
    "- google/pegasus-cnn_dailymail [20]\n",
    "\n",
    "They are distributed through huggingface and the Transformers library is necessary to use these models.\n",
    "\n",
    "## Work Plan (Gantt Chart)\n",
    "\n",
    "The first 2 to 3 weeks are used to develop the basic Transformer model. Its part of the document is written at the same time. After that, hyperparameter tunings are repeatedly executed with small cycles so that they are efficient. The uncertainty of this phase is the length of training time. The optional implementation for the RetNet model keeps aside the time of 3 weeks. At last, there is a time for the documentation and the buffer.\n",
    "\n",
    "<img src=\"img/gantt_chart.png\">\n",
    "\n",
    "## Evaluation Plan\n",
    "\n",
    "Normal testing, which is for machine learning models, and hypothesis verification are executed to evaluate the project. However, the human tester is not used to check performance because it is difficult to organize the tester team for this project free of charge.\n",
    "\n",
    "### Testing\n",
    "\n",
    "For the test dataset, the following metrics are used to measure and test model performance. This is executed to numerically prove how better a generated model is.\n",
    "\n",
    "- loss\n",
    "- ROUGE-N\n",
    "- ROUGE-L\n",
    "\n",
    "### Verification of hypotheses\n",
    "\n",
    "The following hypotheses are verified.\n",
    "\n",
    "- Hyperparameter tunings are significantly effective even on a Transformer model.\n",
    "- Transformer models can generate text more sophisticatedly than dense layer models.\n",
    "- The encoder-decoder model of the Transformer works better for text summarization tasks than the only-decoder model.\n",
    "- Pre-trained models improve the text summarization performance.\n",
    "\n",
    "The following hypothesis is verified as a result of an optional implementation, if possible.\n",
    "\n",
    "- The RetNet architecture for a small model does not make a difference.\n",
    "\n",
    "Whereas, it is seemingly difficult to verify the following hypotheses in this project, even though they are interesting topics. The first one is a thought during the prototype implementation. A huge Transformer model is essential to verify the first hypothesis, and it is impossible for the current local environment to run. And no one has solved the second one as far as I have read a number of papers. That is, the task of text summarization is strongly related to human prior knowledge and understanding of the things that are summarized. Thus, I am going to exclude them from the verification of the project temporarily.\n",
    "\n",
    "- An excessive number of units and layers for the number of dataset entries overfits.\n",
    "- Prior knowledge and bias cannot be solved on the current architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U pip datasets ipywidgets\n",
    "# For mac OS\n",
    "# %pip install -U tensorflow==2.16.2 tensorflow-macos==2.16.2 keras==3.4.1 keras-nlp\n",
    "# 20240803, tensorflow-text can be installed on Apple Silicon mac now!\n",
    "# %pip install tensorflow-text\n",
    "# For Intel mac\n",
    "# %pip install -U tensorflow==2.16.2 tensorflow-text keras==3.4.1 keras-nlp\n",
    "# For AWS SageMaker\n",
    "# %pip install -U tensorflow==2.16.2 tensorflow-datasets tensorflow-text keras==3.4.1 keras-nlp datasets rouge-score py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to determine where this note is running\n",
    "import platform\n",
    "\n",
    "def is_running_on_apple_silicon():\n",
    "    \"\"\"\n",
    "    Determine if this is running on Apple Silicon Mac.\n",
    "    \"\"\"\n",
    "    return platform.system() == \"Darwin\" and platform.processor() == \"arm\"\n",
    "\n",
    "def is_running_on_intel_mac():\n",
    "    \"\"\"\n",
    "    Determine if this is running on Intel Mac.\n",
    "    \"\"\"\n",
    "    return platform.system() == \"Darwin\" and platform.processor() == \"i386\"\n",
    "\n",
    "# This flag is used for tf.debugging.experimental.enable_dump_debug_info.\n",
    "# However, this makes 10 times slower.\n",
    "DEBUGGER_V2 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed-precision Training\n",
    "\n",
    "This technique is introduced in the Deep Learning with Python book. [41] However, this makes the processing 2.x slower on Mac Book Pro as follows. That is why it is not used for the local Mac environments.\n",
    "\n",
    "Machine | 1 step\n",
    "--- | ---\n",
    "Intel (Mac Book Pro) - fp32 : fp16 | 1 : 1.714\n",
    "Apple Silicon M2 (Mac Book Pro) - fp32 : fp16 | 1 : 2.813\n",
    "NVIDIA V100 GPU (ml.p3.2xlarge) - fp32 : fp16 | 1 : 0.875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 2.17.0 is expected. The running version is 2.17.0\n",
      "Keras 3.4.1 is expected. The running version is 3.4.1\n",
      "KerasNLP 0.12.1 is expected. The running version is 0.12.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "print(\"Tensorflow 2.17.0 is expected. The running version is\", tf.__version__)\n",
    "print(\"Keras 3.4.1 is expected. The running version is\", keras.__version__)\n",
    "print(\"KerasNLP 0.12.1 is expected. The running version is\", keras_nlp.__version__)\n",
    "\n",
    "if is_running_on_apple_silicon() or is_running_on_intel_mac():\n",
    "    FLOAT_TYPE = tf.float32\n",
    "else:\n",
    "    keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    FLOAT_TYPE = tf.float16\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "# SageMaker cannot use @keras.saving\n",
    "from keras import saving\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(history, title=None, keys=[\"loss\", \"masked_acc\"]):\n",
    "    \"\"\"\n",
    "    Display the plot that indicates the loss and accuracy.\n",
    "    :param history: history object from the tensorflow fit function.\n",
    "    :param title: title text.\n",
    "    :param keys: keys for plotting.\n",
    "    \"\"\"\n",
    "    for key in keys:\n",
    "        if 'loss' in key:\n",
    "            print(\n",
    "                np.min(history.history[f\"val_{key}\"]),\n",
    "                \"The best number of epocs for the validation loss is\",\n",
    "                np.argmin(history.history[f\"val_{key}\"]) + 1,\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                np.max(history.history[f\"val_{key}\"]),\n",
    "                \"The best number of epocs for the validation accuracy is\",\n",
    "                np.argmax(history.history[f\"val_{key}\"]) + 1,\n",
    "            )\n",
    "\n",
    "    flg, axes = plt.subplots(1, 2, tight_layout=True)\n",
    "    if title is not None:\n",
    "        flg.suptitle(t=title, fontsize=14)\n",
    "    for i, key in enumerate(keys):\n",
    "        value = history.history[key]\n",
    "        val_loss = history.history[f\"val_{key}\"]\n",
    "        epochs = range(1, len(value) + 1)\n",
    "        axes[i].plot(epochs, value, label=f\"Training {key}\")\n",
    "        axes[i].plot(epochs, val_loss, label=f\"Validation {key}\")\n",
    "        axes[i].set_title(f\"Training and validation {key}\")\n",
    "        axes[i].set_xlabel(\"epochs\")\n",
    "        axes[i].set_ylabel(key)\n",
    "        axes[i].legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "The TensorBoard tool is enabled with its callback that is set to the model fit function. And the log directory must be specified in its callback argument. This log directory is also used in the experimental debugger `tf.debugging.experimental.enable_dump_debug_info` Therefore, the function that returns the log directory path is methodized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dir(now):\n",
    "    log_dir = \"logs/fit/\" + now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    return log_dir\n",
    "\n",
    "def get_tensorboard_callback(now):\n",
    "    \"\"\"\n",
    "    Create the TensorBoard callback\n",
    "    \"\"\"\n",
    "    # @see https://www.tensorflow.org/tensorboard/get_started\n",
    "    log_dir = get_log_dir(now=now)\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1\n",
    "    )\n",
    "    return tensorboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @see https://github.com/keras-team/keras-nlp/blob/50e041487b1d8b30b34c5fb738db3ed3406363bc/examples/machine_translation/data.py\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"\n",
    "    Define the custom standardization to remove useless characters.\n",
    "    \"\"\"\n",
    "    s = tf.strings.lower(input_string)\n",
    "    s = tf.strings.regex_replace(s, r'\\n', ' ')\n",
    "    s = tf.strings.regex_replace(s, r'\\\\n', ' ')\n",
    "    s = tf.strings.regex_replace(s, r'<br />', ' ')\n",
    "    s = tf.strings.regex_replace(s, r\"'\", ' ')\n",
    "    s = tf.strings.regex_replace(s, r'&amp;', '')\n",
    "    s = tf.strings.regex_replace(s, r'[_\"\\-;%()|+&=*%.,!?:#$@/]', ' ')\n",
    "    s = tf.strings.regex_replace(s, r'<\\/?[^>]*>', '') # html tag\n",
    "    #s = tf.strings.regex_replace(s, r'''<(\"[^\"]*\"|'[^']*'|[^'\">])*>''', '') # html tag\n",
    "    s = tf.strings.regex_replace(s, r'https?:\\/\\/.*[\\r\\n]*', \" \") # URL\n",
    "    s = tf.strings.regex_replace(s, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate control\n",
    "\n",
    "The `ReduceLROnPlateau` makes the learning rate smaller when the monitoring value gets stuck. This worked well for the CNN Dailymail dataset, where the model did not overtit, to the training dataset to improve the performance somewhat after getting stuck with the initial learning rate. However, the same effect did not happened for the SAMSum dataset, where the model overfitted. Even though the learning rate got smaller by monitoring the loss when it gets stuck, that affected the training curve and did not affect the validation curve. Even by monitoring the validation loss, the learning rate reduction affected only the training curve. On the other hand, because this control did not affect negatively, it was remained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @see https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "reduce_lr_callbacks = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_delta=0.04,\n",
    "    cooldown=8,\n",
    "    min_lr=2e-5, # 5e-4: 0.0005, 2e-5: 0.00002\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loss function definition\n",
    "\n",
    "The prediction y values of seq2seq models contain pad(s), which are used to align the length of every output sequence. In the case that most sequences are much shorter than the longest sentence and pads are not considered, a model that predicts only pads of sentences is unintentionally highly evaluated.\n",
    "Therefore, the custom loss function excluding pads from the loss calculation is defined as follows. As same as the loss function, the influence of the pads is excluded from the accuracy metric.\n",
    "\n",
    "**The reason why the classification model does not use the masked loss function**\n",
    "\n",
    "The prediction y values of classification models do not contain pad(s), which are used to align the length of every output sequence. It just contains the probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @see https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "@saving.register_keras_serializable()\n",
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "        # nn.py:609: UserWarning:\n",
    "        # \"`sparse_categorical_crossentropy` received `from_logits=True`,\n",
    "        # but the `output` argument was produced by a Softmax activation and thus does not represent logits.\n",
    "        # Was this intended?\n",
    "        # When logits is True, softmax activation function has not processed the values.\n",
    "        from_logits=True,\n",
    "        reduction='none'\n",
    "    )\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "# @see https://www.tensorflow.org/text/tutorials/transformer\n",
    "@saving.register_keras_serializable()\n",
    "def masked_acc(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=2)\n",
    "    y_true = tf.cast(y_true, dtype=y_pred.dtype)\n",
    "    match = y_true == y_pred\n",
    "    mask = y_true != 0\n",
    "    match = match & mask\n",
    "    match = tf.cast(match, dtype=FLOAT_TYPE)\n",
    "    mask = tf.cast(mask, dtype=FLOAT_TYPE)\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As commented in the code below, the tensorflow_datasets library did not work with the SSL error to obtain the SAMSum dataset. Instead, the Huggingface library is used here and converted to the Tensorflow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def prepare_datasets():\n",
    "    \"\"\"\n",
    "    Get training set, validation set, and test set\n",
    "    tensorflow_datasets does not work well with the SSL error.\n",
    "    Therefore, the data is obtained with the Huggingface library and converted to Tensorflow.\n",
    "    :return: train_ds\n",
    "    :return: validation_ds\n",
    "    :return: test_ds\n",
    "    \"\"\"\n",
    "    # How to convert huggingface dataset to tensorflow dataset\n",
    "    # @see https://huggingface.co/docs/datasets/v1.3.0/torch_tensorflow.html#setting-the-format\n",
    "    def convert_hf2tf(\n",
    "            dataset: datasets.DatasetDict,\n",
    "            split: list[str],\n",
    "            columns=['article', 'highlights', 'id',]):\n",
    "        dataset.set_format(\n",
    "            type='tensorflow',\n",
    "            columns=columns\n",
    "        )\n",
    "        l = []\n",
    "        for s in split:\n",
    "            d = dataset[s]\n",
    "            features = {x: d[x] for x in columns}\n",
    "            # .batch(32) is not used to show a simple sampled data below with take(1)\n",
    "            tf_dataset = tf.data.Dataset.from_tensor_slices(features)\n",
    "            l.append(tf_dataset)\n",
    "        return tuple(l)\n",
    "    ds = datasets.load_dataset(\n",
    "        'Samsung/samsum',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    train_ds, validation_ds, test_ds = convert_hf2tf(\n",
    "        dataset=ds,\n",
    "        split=['train', 'validation', 'test'],\n",
    "        columns=['id', 'summary', 'dialogue'],\n",
    "    )\n",
    "    return train_ds, validation_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer Preparation\n",
    "\n",
    "2 `TextVectorization` layers of `tf.keras.layers` instantiated here because this Transformer model has 2 inputs. One of them is used for the encoder input that is original text input and the other is used for the decoder input that is the ground truth summarized text on training. Only the `output_sequence_length` value is different from each other. If a translation model is build with the Transformer model, then the vocabulary is also different from each other.\n",
    "\n",
    "### Dataset Preparation\n",
    "\n",
    "Since the raw text cannot be processed directly, the text data, which is the sequence of words, is encoded into the sequence of numbers. Moreover, the sequence for decoder input is added the special out-of-vocabulary (OOV) token \"[start]\" and the sequence for decoder output is added the special OOV token \"[end]\". In this process, because most of the text length is not the same as the maximum length of the encoder input, the decoder input, or the decoder output, the pad is used to fill the length. Specifically, each input sequence in training is encoded as follows;\n",
    "\n",
    "```\n",
    "**Encoder input**\n",
    "Lee: what was the name of the song again Roy: I'll send you the link once i get home Lee: okay..... waiting [pad] [pad] [pad] ...\n",
    "\n",
    "**Decoder input**\n",
    "[start] Roy will send Lee a link to the song when he gets home. [pad] [pad] [pad] ...\n",
    "\n",
    "**Decoder output**\n",
    "Roy will send Lee a link to the song when he gets home. [end] [pad] [pad] [pad] ...\n",
    "```\n",
    "\n",
    "And, on the prediction, each input sequence is as follows to output a summarized text that is generated at the decoder output;\n",
    "\n",
    "```\n",
    "**Encoder input**\n",
    "Lee: what was the name of the song again Roy: I'll send you the link once i get home Lee: okay..... waiting [pad] [pad] [pad] ...\n",
    "\n",
    "**Decoder input**\n",
    "[start]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer(\n",
    "        vectorization_layer,\n",
    "        encoder_sequence_length,\n",
    "        decoder_sequence_length,\n",
    "        max_tokens=15000):\n",
    "    \"\"\"\n",
    "    Display the plot that indicates the loss and accuracy.\n",
    "    :param vectorization_layer: TextVectorization layer that has been already adapted all words of dataset\n",
    "    :param max_tokens: In other words, this is the vocabulary size\n",
    "    :param encoder_sequence_length: The sequence length for input\n",
    "    :param input_output_sequence_length: The sequence length for target\n",
    "    \"\"\"\n",
    "    vocabulary = vectorization_layer.get_vocabulary()\n",
    "\n",
    "    input_vectorization_layer = keras.layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=encoder_sequence_length,\n",
    "    )\n",
    "    target_vectorization_layer = keras.layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=decoder_sequence_length,\n",
    "    )\n",
    "    input_vectorization_layer.set_vocabulary(vocabulary)\n",
    "    target_vectorization_layer.set_vocabulary(vocabulary)\n",
    "    return input_vectorization_layer, target_vectorization_layer\n",
    "\n",
    "def build_datasets(\n",
    "        train_ds, validation_ds, test_ds,\n",
    "        vectorization_layer,\n",
    "        batch_size,\n",
    "        encoder_sequence_length,\n",
    "        decoder_sequence_length):\n",
    "    \"\"\"\n",
    "    vectorization_layer(['This is a pen', 'I am a software engineer'])\n",
    "    #vectorization_layer(['This is a pen', 'I am a software engineer']).row_lengths().shape[0]\n",
    "    # 2\n",
    "    rows = vectorization_layer(['This is a pen', 'I am a software engineer']).row_lengths().shape[0]\n",
    "    vectorization_layer(['This is a pen', 'I am a software engineer']).to_tensor(shape=(rows, 10))\n",
    "    # .to_tensor()\n",
    "\n",
    "    RaggedTensor.to_tensor can make 0-filled Tensor\n",
    "    \"\"\"\n",
    "    def format_dataset(x):\n",
    "        # decoder_sequence_length: either the following 2.\n",
    "        # - decoder input: [start] + sentence\n",
    "        # - decoder output: sentence + [end]\n",
    "        # That is, decoder_sequence_length = sentence length + 1\n",
    "        summarized_text_length = decoder_sequence_length - 1\n",
    "\n",
    "        d = vectorization_layer(x['dialogue'])\n",
    "        r = d.row_lengths().shape[0]\n",
    "        dialogue = d.to_tensor(shape=(r, encoder_sequence_length))\n",
    "\n",
    "        start_oov = vectorization_layer(['[start]']).to_tensor(shape=(1, 1))\n",
    "        end_oov = vectorization_layer(['[end]']).to_tensor(shape=(1, 1))\n",
    "        summary = vectorization_layer(x['summary'])\n",
    "        \"\"\"\n",
    "        print(h.row_lengths().shape[0]) None\n",
    "        print(tf.shape(h)[0]) Tensor(\"RaggedShape/Cast_3:0\", shape=(), dtype=int32)\n",
    "        At the last step, the number of rows is not equal to the batch size.\n",
    "        \"\"\"\n",
    "        rows = tf.shape(summary)[0]\n",
    "        summary = summary[:rows, :summarized_text_length]\n",
    "        start_oov = tf.repeat(start_oov, repeats=rows , axis=0)\n",
    "        end_oov = tf.repeat(end_oov, repeats=rows , axis=0)\n",
    "        summary = tf.concat([start_oov, summary, end_oov], axis=1)\n",
    "\n",
    "        sequences = summary.to_tensor(shape=(\n",
    "            rows,\n",
    "            summarized_text_length + 1 + 1 # start + sentence + end\n",
    "        ))\n",
    "\n",
    "        summary_decoder_input = sequences[:, :-1] # start + sentence\n",
    "        summary_decoder_output = sequences[:, 1:] # sentence + end\n",
    "        return (\n",
    "            (\n",
    "                dialogue, # encoder input\n",
    "                summary_decoder_input, # decoder input\n",
    "            ),\n",
    "            summary_decoder_output, # decoder output\n",
    "            tf.cast((summary_decoder_output != 0), dtype=FLOAT_TYPE)\n",
    "        )\n",
    "    train_ds = train_ds.batch(batch_size).map(\n",
    "        format_dataset,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).prefetch(tf.data.AUTOTUNE).cache()\n",
    "    if validation_ds is not None:\n",
    "        validation_ds = validation_ds.batch(batch_size).map(\n",
    "            format_dataset,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).prefetch(tf.data.AUTOTUNE).cache()\n",
    "    test_ds = test_ds.batch(batch_size).map(\n",
    "        format_dataset,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).prefetch(tf.data.AUTOTUNE).cache()\n",
    "    return train_ds, validation_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Transformer Model\n",
    "\n",
    "The `TransformerEncoderDecoderModel` class is defined as a sub-class of `keras.Model` here. The advantage of using the subclassing API of Tensorflow is capability to save the vocabulary data together into a model file easily, aside from the customization of the model architecture and the training loop. That is, the vocabulary data, which is held in the 2 TextVectorization layers, is saved the table of the actual words and their internal IDs with trained parameters in this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@saving.register_keras_serializable()\n",
    "class TransformerEncoderDecoderModel(keras.Model):\n",
    "    \"\"\"\n",
    "    Transformer model class that contains Transformer Encoder and Decoder inside.\n",
    "    :param encoder_tokenizer: tf.keras.layers.TextVectorization object\n",
    "    :param decoder_tokenizer: tf.keras.layers.TextVectorization object\n",
    "    :num_encoders: the number of encoders\n",
    "    :num_decoders: the number of decoders\n",
    "    :num_heads: the number of heads (Attention mechanism)\n",
    "    :dropout: the dropout rate\n",
    "    :transformer_intermediate_dim: the number of dimensions in each encoder and decoder\n",
    "    :normalize_first: Pre-Layer Normalization or Post-Layer Normalization\n",
    "    :layer_norm_epsilon: the epsilon value in layer normalization components.\n",
    "    :encoder_vocabulary_size: the number of encoder vocabulary size\n",
    "    :decoder_vocabulary_size: the number of decoder vocabulary size\n",
    "    :embedding_dim: the dimension for embedding\n",
    "    :mask_zero: whether 0 is used the mask value\n",
    "    :encoder_sequence_length: the length of encoder sequence\n",
    "    :decoder_sequence_length: the length of decoder sequence\n",
    "    :use_residual: whether residual connections are used outside of the Transformer encoder and decoder\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_tokenizer,\n",
    "        decoder_tokenizer,\n",
    "        num_encoders,\n",
    "        num_decoders,\n",
    "        num_heads,\n",
    "        dropout,\n",
    "        transformer_intermediate_dim,\n",
    "        normalize_first,\n",
    "        layer_norm_epsilon,\n",
    "        encoder_vocabulary_size,\n",
    "        decoder_vocabulary_size,\n",
    "        embedding_dim,\n",
    "        mask_zero,\n",
    "        encoder_sequence_length,\n",
    "        decoder_sequence_length,\n",
    "        use_residual,\n",
    "        **kwargs):\n",
    "        super(TransformerEncoderDecoderModel, self).__init__(**kwargs)\n",
    "        self.mask_zero = mask_zero\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.use_residual = use_residual\n",
    "        self.encoders = []\n",
    "        self.decoders = []\n",
    "        if isinstance(layer_norm_epsilon, list):\n",
    "            encoder_layer_norm_epsilons = layer_norm_epsilon\n",
    "            decoder_layer_norm_epsilons = layer_norm_epsilon\n",
    "        else:\n",
    "            encoder_layer_norm_epsilons = [layer_norm_epsilon for _ in range(num_encoders)]\n",
    "            decoder_layer_norm_epsilons = [layer_norm_epsilon for _ in range(num_decoders)]\n",
    "\n",
    "        for i in range(num_encoders):\n",
    "            self.encoders.append(\n",
    "                keras_nlp.layers.TransformerEncoder(\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                    intermediate_dim=transformer_intermediate_dim,\n",
    "                    normalize_first=normalize_first,\n",
    "                    layer_norm_epsilon=encoder_layer_norm_epsilons[i],\n",
    "                    name=f\"encoder_{i}\",\n",
    "                )\n",
    "            )\n",
    "        for i in range(num_decoders):\n",
    "            self.decoders.append(\n",
    "                keras_nlp.layers.TransformerDecoder(\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                    intermediate_dim=transformer_intermediate_dim,\n",
    "                    normalize_first=normalize_first,\n",
    "                    layer_norm_epsilon=decoder_layer_norm_epsilons[i],\n",
    "                    name=f\"decoder_{i}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.encoder_tokenizer = encoder_tokenizer\n",
    "        self.decoder_tokenizer = decoder_tokenizer\n",
    "\n",
    "        self.encoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "            vocabulary_size=encoder_vocabulary_size,\n",
    "            sequence_length=encoder_sequence_length,\n",
    "            embedding_dim=embedding_dim,\n",
    "            mask_zero=mask_zero,\n",
    "            name=\"encoder_embed\",\n",
    "        )\n",
    "\n",
    "        self.decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "            vocabulary_size=decoder_vocabulary_size,\n",
    "            sequence_length=decoder_sequence_length,\n",
    "            embedding_dim=embedding_dim,\n",
    "            mask_zero=mask_zero,\n",
    "            name=\"decoder_embed\",\n",
    "        )\n",
    "\n",
    "        self.dense = keras.layers.Dense(\n",
    "            decoder_vocabulary_size,\n",
    "            name=\"dense\",\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        This method must set the shapes of tensors into every layer.\n",
    "        @see https://github.com/keras-team/keras/issues/19535#issuecomment-2060299275\n",
    "        fcollet has answered and explained how to implement for the following warning.\n",
    "\n",
    "        ```\n",
    "        UserWarning: `build()` was called on layer 'transformer_encoder_decoder_model_*',\n",
    "        however the layer does not have a `build()` method implemented and it looks like it has unbuilt state.\n",
    "        This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line.\n",
    "        Make sure to implement a proper `build()` method\n",
    "        ```\n",
    "\n",
    "        The incorrect implementation of this method causes the following problem:\n",
    "        - When the model is generated from .keras file, the loaded model does not work correctly.\n",
    "        \"\"\"\n",
    "        self.encoder_embedding.build(input_shape[0])\n",
    "        self.decoder_embedding.build(input_shape[1])\n",
    "        encoder_output_shape = self.encoder_embedding.compute_output_shape(input_shape[0])\n",
    "        decoder_output_shape = self.decoder_embedding.compute_output_shape(input_shape[1])\n",
    "\n",
    "        for encoder in self.encoders:\n",
    "            encoder.build(inputs_shape=encoder_output_shape)\n",
    "            encoder_output_shape = encoder.compute_output_shape(\n",
    "                inputs_shape=encoder_output_shape\n",
    "            )\n",
    "        for decoder in self.decoders:\n",
    "            decoder.build(\n",
    "                decoder_sequence_shape=decoder_output_shape,\n",
    "                encoder_sequence_shape=encoder_output_shape,\n",
    "            )\n",
    "            decoder_output_shape = decoder.compute_output_shape(\n",
    "                decoder_sequence_shape=decoder_output_shape,\n",
    "            )\n",
    "\n",
    "        self.dense.build(input_shape=decoder_output_shape)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        This method defines and represents the calculation graph (tensor flow).\n",
    "        :training: True, when calculating val_loss and val_acc. Otherwise, False.\n",
    "        \"\"\"\n",
    "        encoder_input, decoder_input = (\n",
    "            inputs[0],\n",
    "            inputs[1],\n",
    "        )\n",
    "        encoded = self.encoder_embedding(encoder_input)\n",
    "        for encoder in self.encoders:\n",
    "            if self.use_residual and len(self.encoders) > 1:\n",
    "                residual = encoded\n",
    "            encoded = encoder(\n",
    "                inputs=encoded,\n",
    "                training=training,\n",
    "            )\n",
    "            if self.use_residual and len(self.encoders) > 1:\n",
    "                encoded = encoded + residual\n",
    "\n",
    "        decoded = self.decoder_embedding(decoder_input)\n",
    "        for decoder in self.decoders:\n",
    "            if self.use_residual and len(self.decoders) > 1:\n",
    "                residual = decoded\n",
    "            decoded = decoder(\n",
    "                decoder_sequence=decoded,\n",
    "                encoder_sequence=encoded,\n",
    "                use_causal_mask=True,\n",
    "                training=training,\n",
    "            )\n",
    "            if self.use_residual and len(self.decoders) > 1:\n",
    "                decoded = decoded + residual\n",
    "\n",
    "        output = self.dense(decoded)\n",
    "\n",
    "        try:\n",
    "            # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "            # b/250038731\n",
    "            del output._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        This method is used to save a model into a file.\n",
    "        \"\"\"\n",
    "        config = super(TransformerEncoderDecoderModel, self).get_config().copy()\n",
    "        config.update({\n",
    "            \"encoder_tokenizer\": self.encoder_tokenizer.get_config(),\n",
    "            \"decoder_tokenizer\": self.decoder_tokenizer.get_config(),\n",
    "            \"num_encoders\": len(self.encoders),\n",
    "            \"num_decoders\": len(self.decoders),\n",
    "            \"num_heads\": self.encoders[0].num_heads,\n",
    "            \"dropout\" : self.encoders[0].dropout,\n",
    "            \"transformer_intermediate_dim\": self.encoders[0].intermediate_dim,\n",
    "            \"normalize_first\": self.encoders[0].normalize_first,\n",
    "            \"layer_norm_epsilon\": self.layer_norm_epsilon,\n",
    "            \"encoder_vocabulary_size\": self.encoder_embedding.vocabulary_size,\n",
    "            \"decoder_vocabulary_size\": self.decoder_embedding.vocabulary_size,\n",
    "            \"embedding_dim\": self.encoder_embedding.embedding_dim,\n",
    "            \"mask_zero\": self.mask_zero,\n",
    "            \"encoder_sequence_length\": self.encoder_embedding.sequence_length,\n",
    "            \"decoder_sequence_length\": self.decoder_embedding.sequence_length,\n",
    "            \"use_residual\": self.use_residual,\n",
    "        })\n",
    "        return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"\n",
    "        This method is used to build a model from a saved file.\n",
    "        \"\"\"\n",
    "        encoder_tokenizer_config = config.pop(\"encoder_tokenizer\")\n",
    "        decoder_tokenizer_config = config.pop(\"decoder_tokenizer\")\n",
    "        encoder_tokenizer = keras.layers.TextVectorization.from_config(encoder_tokenizer_config)\n",
    "        decoder_tokenizer = keras.layers.TextVectorization.from_config(decoder_tokenizer_config)\n",
    "        return cls(\n",
    "            encoder_tokenizer=encoder_tokenizer,\n",
    "            decoder_tokenizer=decoder_tokenizer,\n",
    "            **config\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Firstly, the text that is summarized and the start OOV token \"[start]\" are given into the encoder input and the decoder input, respectively. Then, the first word that follows \"[start]\" is derived from the decoder output. Next, the outputted word is combined with the previous input sequence and it is used the next input sequence of the decoder input. At this time, the encoder input is not changed. This process is repeatedly executed until the decoder output contains the end OOV token \"[end]\" or reaches to the maximum length. Finally, the summarized text is completed.\n",
    "\n",
    "Unlike the classification task, it computationally takes O(n) in proportion to the length of the summarized text. This might be one of the disadvantages if the task summarizes much longer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(\n",
    "        input_sentence,\n",
    "        model,\n",
    "        max_sequence_length,\n",
    "        lookup_table):\n",
    "    \"\"\"\n",
    "    Generate summarized text from the input sentence.\n",
    "    :input_sentence: the original text that is summarized\n",
    "    :model: the TransformerEncoderDecoderModel class model\n",
    "    :max_sequence_length: the maximum length of the summarized text\n",
    "    :lookup_table: the table holds token IDs and their actual words\n",
    "    \"\"\"\n",
    "    encoder_tokenizer = model.encoder_tokenizer\n",
    "    decoder_tokenizer = model.decoder_tokenizer\n",
    "    tokenized_input = encoder_tokenizer([input_sentence])\n",
    "\n",
    "    start_token = decoder_tokenizer(\"[start]\")[0].numpy()\n",
    "    end_token = decoder_tokenizer(\"[end]\")[0].numpy()\n",
    "\n",
    "    decoded_sentence = [start_token]\n",
    "    for i in range(max_sequence_length):\n",
    "        decoder_inputs = tf.convert_to_tensor(\n",
    "            [decoded_sentence],\n",
    "            dtype=\"int64\",\n",
    "        )\n",
    "        decoder_inputs = tf.concat(\n",
    "            [\n",
    "                decoder_inputs,\n",
    "                tf.zeros(\n",
    "                    [1, max_sequence_length - i - 1],\n",
    "                    dtype=\"int64\",\n",
    "                ),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        input = (\n",
    "            tokenized_input,\n",
    "            decoder_inputs,\n",
    "        )\n",
    "        predictions = model(input)\n",
    "        predicted_token = np.argmax(predictions[0, i, :])\n",
    "        decoded_sentence.append(predicted_token)\n",
    "        if predicted_token == end_token:\n",
    "            break\n",
    "\n",
    "    detokenized_output = []\n",
    "    for token in decoded_sentence:\n",
    "        detokenized_output.append(lookup_table[token])\n",
    "    return \" \".join(detokenized_output)\n",
    "\n",
    "\n",
    "def predict_main(\n",
    "        filepath,\n",
    "        examples,\n",
    "        decoder_sequence_length):\n",
    "    \"\"\"\n",
    "    Generate summarized text with the model file.\n",
    "    :filepath: the file path specifies the model\n",
    "    :examples: the list of text that is summarized.\n",
    "    :decoder_sequence_length: the maximum length of the summarized text.\n",
    "    \"\"\"\n",
    "    loaded_model = keras.models.load_model(\n",
    "        filepath,\n",
    "        # Just in case, TransformerEncoderDecoderModel is specified.\n",
    "        # However, it does not seem necessary.\n",
    "        custom_objects={\n",
    "            \"TransformerEncoderDecoderModel\": TransformerEncoderDecoderModel,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    decoder_tokenizer = loaded_model.decoder_tokenizer\n",
    "    vocab = decoder_tokenizer.get_vocabulary()\n",
    "    index_lookup_table = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "    summarized = []\n",
    "    for example in examples:\n",
    "        summarized.append(\n",
    "            decode_sequence(\n",
    "                example,\n",
    "                loaded_model,\n",
    "                decoder_sequence_length,\n",
    "                index_lookup_table,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        print(\"ORIGINAL SENTENCE: \", examples[i])\n",
    "        print(\"SUMMARIZED RESULT: \", summarized[i])\n",
    "\n",
    "def predict_model(\n",
    "        model,\n",
    "        examples,\n",
    "        decoder_sequence_length):\n",
    "    \"\"\"\n",
    "    Generate summarized text with the model.\n",
    "    :model: the file path specifies the model\n",
    "    :examples: the list of text that is summarized.\n",
    "    :decoder_sequence_length: the maximum length of the summarized text.\n",
    "    \"\"\"\n",
    "    decoder_tokenizer = model.decoder_tokenizer\n",
    "    vocab = decoder_tokenizer.get_vocabulary()\n",
    "    index_lookup_table = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "    summarized = []\n",
    "    for example in examples:\n",
    "        summarized.append(\n",
    "            decode_sequence(\n",
    "                example,\n",
    "                model,\n",
    "                decoder_sequence_length,\n",
    "                index_lookup_table,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for i in range(len(examples)):\n",
    "        print(\"ORIGINAL SENTENCE: \", examples[i])\n",
    "        print(\"SUMMARIZED RESULT: \", summarized[i])\n",
    "\n",
    "def predict_dataset(\n",
    "        model,\n",
    "        iterable_dataset,\n",
    "        decoder_sequence_length):\n",
    "    \"\"\"\n",
    "    Generate summarized text with the model.\n",
    "    :model: the file path specifies the model\n",
    "    :iterable_dataset: the dataset, which is mainly test set, used to generate summarized text.\n",
    "    :decoder_sequence_length: the maximum length of the summarized text.\n",
    "    \"\"\"\n",
    "    decoder_tokenizer = model.decoder_tokenizer\n",
    "    vocab = decoder_tokenizer.get_vocabulary()\n",
    "    index_lookup_table = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    for entry in iterable_dataset:\n",
    "        text = entry[\"dialogue\"]\n",
    "        result = decode_sequence(\n",
    "            text,\n",
    "            model,\n",
    "            decoder_sequence_length,\n",
    "            index_lookup_table,\n",
    "        )\n",
    "        y_true = entry[\"summary\"]\n",
    "        y_true = y_true.decode('utf-8').lower()\n",
    "        y_pred = result.replace('[start]', '').replace('[end]', '').strip()\n",
    "        # print(y_true, '\\n\\t' , y_pred)\n",
    "        y_trues.append(y_true)\n",
    "        y_preds.append(y_pred)\n",
    "    return y_trues, y_preds\n",
    "\n",
    "def rouge_n(y_true, y_pred, order=2):\n",
    "    rouge_n = keras_nlp.metrics.RougeN(order=order)\n",
    "    return rouge_n(y_true, y_pred)\n",
    "\n",
    "def calculate_rouge_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    # RougeN metric\n",
    "    # @see https://keras.io/api/keras_nlp/metrics/rouge_n/\n",
    "    \"\"\"\n",
    "    rouge_n = keras_nlp.metrics.RougeN(order=2)\n",
    "    rouge_2_score = rouge_n(y_true, y_pred)\n",
    "    rouge_n = keras_nlp.metrics.RougeN(order=1)\n",
    "    rouge_1_score = rouge_n(y_true, y_pred)\n",
    "    rouge_l = keras_nlp.metrics.RougeL()\n",
    "    rouge_l_score = rouge_l(y_true, y_pred)\n",
    "    return rouge_1_score, rouge_2_score, rouge_l_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "        model,\n",
    "        train_ds,\n",
    "        validation_ds,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        steps_per_epoch,\n",
    "        now,\n",
    "        verbose,\n",
    "        callbacks=[]):\n",
    "    \"\"\"\n",
    "    Run training.\n",
    "    :train_ds: training set\n",
    "    :validation_ds: validation set\n",
    "    :optimizer: optimizer.Optimizer\n",
    "    :epochs: the number of epochs\n",
    "    :steps_per_epoch: the number of weight updates in a epoch\n",
    "    :now: timestamp\n",
    "    \"\"\"\n",
    "    metrics = [\n",
    "        masked_acc,\n",
    "        #keras.metrics.SparseCategoricalAccuracy(),\n",
    "        #keras_nlp.metrics.RougeN(), #  This cannot be used here\n",
    "    ]\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics,\n",
    "        loss=masked_loss,\n",
    "        weighted_metrics=[],\n",
    "    )\n",
    "    callbacks.append(get_tensorboard_callback(now=now))\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_ds,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RougeEvaluator(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to calculate ROUGE scores for every epoch\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset,\n",
    "            max_length):\n",
    "        \"\"\"\n",
    "        :dataset: dataset object that is used for ROUGE evaluation\n",
    "        :max_length: the max length of the summarized text\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.max_length = max_length\n",
    "  \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_true, y_pred = predict_dataset(\n",
    "            model=self.model,\n",
    "            iterable_dataset=tfds.as_numpy(self.dataset),\n",
    "            decoder_sequence_length=self.max_length\n",
    "        )\n",
    "        rouge_1_score, rouge_2_score, rouge_l_score = calculate_rouge_score(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred\n",
    "        )\n",
    "        print(rouge_1_score)\n",
    "        print(rouge_2_score)\n",
    "        print(rouge_l_score)\n",
    "\n",
    "def build_model(\n",
    "        train_ds, validation_ds, test_ds,\n",
    "        vectorization_layer,\n",
    "        num_encoders,\n",
    "        num_decoders,\n",
    "        num_heads,\n",
    "        dropout,\n",
    "        transformer_intermediate_dim,\n",
    "        normalize_first,\n",
    "        layer_norm_epsilon,\n",
    "        encoder_sequence_length,\n",
    "        decoder_sequence_length,\n",
    "        use_residual,\n",
    "        vocab_size,\n",
    "        batch_size,\n",
    "        embedding_dim,\n",
    "        mask_zero,\n",
    "        optimizer,\n",
    "        epochs,\n",
    "        steps_per_epoch,\n",
    "        verbose=1,\n",
    "        callbacks=[]):\n",
    "    \"\"\"\n",
    "    Build the model with specified parameters.\n",
    "    :return: model: trained model\n",
    "    :return: filepath: model file path if it is saved\n",
    "    :return: history: history object to plot\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now()\n",
    "    if DEBUGGER_V2:\n",
    "        tf.debugging.experimental.enable_dump_debug_info(\n",
    "            get_log_dir(now=now),\n",
    "            tensor_debug_mode=\"FULL_HEALTH\",\n",
    "            circular_buffer_size=-1\n",
    "        )\n",
    "    # before validation_ds is transformed\n",
    "    # if verbose > 0 and validation_ds is not None:\n",
    "    #     callbacks.append(RougeEvaluator(\n",
    "    #         dataset=validation_ds,\n",
    "    #         max_length=decoder_sequence_length,\n",
    "    #     ))\n",
    "    input_vectorization_layer, target_vectorization_layer = prepare_tokenizer(\n",
    "        vectorization_layer=vectorization_layer,\n",
    "        encoder_sequence_length=encoder_sequence_length,\n",
    "        decoder_sequence_length=decoder_sequence_length,\n",
    "        max_tokens=vocab_size,\n",
    "    )\n",
    "    train_ds, validation_ds, test_ds = build_datasets(\n",
    "        train_ds=train_ds,\n",
    "        validation_ds=validation_ds,\n",
    "        test_ds=test_ds,\n",
    "        vectorization_layer=vectorization_layer,\n",
    "        batch_size=batch_size,\n",
    "        encoder_sequence_length=encoder_sequence_length,\n",
    "        decoder_sequence_length=decoder_sequence_length,\n",
    "    )\n",
    "\n",
    "    input_vocab_size = input_vectorization_layer.vocabulary_size()\n",
    "    target_vocab_size = target_vectorization_layer.vocabulary_size()\n",
    "    model = TransformerEncoderDecoderModel(\n",
    "        encoder_tokenizer=input_vectorization_layer,\n",
    "        decoder_tokenizer=target_vectorization_layer,\n",
    "        num_encoders=num_encoders,\n",
    "        num_decoders=num_decoders,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        transformer_intermediate_dim=transformer_intermediate_dim,\n",
    "        normalize_first=normalize_first,\n",
    "        layer_norm_epsilon=layer_norm_epsilon,\n",
    "        encoder_vocabulary_size=input_vocab_size,\n",
    "        decoder_vocabulary_size=target_vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        mask_zero=mask_zero,\n",
    "        encoder_sequence_length=encoder_sequence_length,\n",
    "        decoder_sequence_length=decoder_sequence_length,\n",
    "        use_residual=use_residual,\n",
    "    )\n",
    "    # This should be called seemingly.\n",
    "    model.build(\n",
    "        input_shape=(\n",
    "            (None, encoder_sequence_length),\n",
    "            (None, decoder_sequence_length)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    history = run_training(\n",
    "        model,\n",
    "        train_ds=train_ds,\n",
    "        validation_ds=validation_ds,\n",
    "        optimizer=optimizer,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        now=now,\n",
    "        verbose=verbose,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    timestamp = now.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    if is_running_on_apple_silicon() or is_running_on_intel_mac():\n",
    "        filepath = f'model/summarization_model_{timestamp}.keras'\n",
    "    else:\n",
    "        filepath = f'summarization_model_{timestamp}.keras'\n",
    "    print(f\"Saving to {filepath}\")\n",
    "    model.save(filepath=filepath)\n",
    "\n",
    "    print(f\"Successfully saved model to {filepath}\")\n",
    "    return model, filepath, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 11:49:55.603544: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-08-13 11:49:55.603570: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-08-13 11:49:55.603573: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-08-13 11:49:55.603585: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-08-13 11:49:55.603597: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-08-13 11:49:56.566355: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "ADAPT_BATCH_SIZE = 256\n",
    "\n",
    "train_ds, validation_ds, test_ds = prepare_datasets()\n",
    "vectorization_layer = keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    # max_tokens=VOCABULARY_SIZE,\n",
    "    output_mode='int',\n",
    "    ragged=True,\n",
    ")\n",
    "vectorization_layer.adapt(\n",
    "    train_ds.batch(ADAPT_BATCH_SIZE).map(\n",
    "        lambda row: '[start] ' + row['summary'] + ' ' + row['dialogue'] + ' [end]',\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    ),\n",
    "    batch_size=ADAPT_BATCH_SIZE,\n",
    ")\n",
    "# Use the maximum size of the dataset. 31907\n",
    "VOCABULARY_SIZE = len(vectorization_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "STEPS_PER_EPOCH = None\n",
    "ENCODER_SEQUENCE_LENGTH = 128 # 128: 75% covered. max is 803.\n",
    "DECODER_SEQUENCE_LENGTH = 64 # 32: 75% covered. max is 64\n",
    "MASK_ZERO = False\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "model, filepath, history = build_model(\n",
    "    train_ds, validation_ds, test_ds,\n",
    "    vectorization_layer=vectorization_layer,\n",
    "    num_encoders=2,\n",
    "    num_decoders=2,\n",
    "    num_heads=6,\n",
    "    dropout=0.35,\n",
    "    transformer_intermediate_dim=512,\n",
    "    normalize_first=False,\n",
    "    layer_norm_epsilon=1e-5,\n",
    "    encoder_sequence_length=ENCODER_SEQUENCE_LENGTH,\n",
    "    decoder_sequence_length=DECODER_SEQUENCE_LENGTH,\n",
    "    use_residual=True,\n",
    "    vocab_size=VOCABULARY_SIZE,\n",
    "    batch_size=128,\n",
    "    embedding_dim=128,\n",
    "    mask_zero=MASK_ZERO,\n",
    "    optimizer=optimizer,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=[reduce_lr_callbacks],\n",
    ")\n",
    "plot(history=history)\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "best_epoch = max(np.argmin(history.history[f\"val_loss\"]), np.argmax(history.history[f\"val_masked_acc\"]))\n",
    "best_epoch = best_epoch + 1\n",
    "print(\"best_epoch is\", best_epoch)\n",
    "model, filepath, history = build_model(\n",
    "    train_ds.concatenate(validation_ds), None, test_ds,\n",
    "    vectorization_layer=vectorization_layer,\n",
    "    num_encoders=2,\n",
    "    num_decoders=2,\n",
    "    num_heads=6,\n",
    "    dropout=0.35,\n",
    "    transformer_intermediate_dim=512,\n",
    "    normalize_first=False,\n",
    "    layer_norm_epsilon=1e-5,\n",
    "    encoder_sequence_length=ENCODER_SEQUENCE_LENGTH,\n",
    "    decoder_sequence_length=DECODER_SEQUENCE_LENGTH,\n",
    "    use_residual=True,\n",
    "    vocab_size=VOCABULARY_SIZE,\n",
    "    batch_size=128,\n",
    "    embedding_dim=128,\n",
    "    mask_zero=MASK_ZERO,\n",
    "    optimizer=optimizer,\n",
    "    epochs=best_epoch,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    callbacks=[reduce_lr_callbacks],\n",
    ")\n",
    "\n",
    "y_true, y_pred = predict_dataset(\n",
    "    model=model,\n",
    "    iterable_dataset=tfds.as_numpy(test_ds), # The length of the test dataset is 819.\n",
    "    decoder_sequence_length=DECODER_SEQUENCE_LENGTH\n",
    ")\n",
    "rouge_1_score, rouge_2_score, rouge_l_score = calculate_rouge_score(\n",
    "    y_true=y_true,\n",
    "    y_pred=y_pred\n",
    ")\n",
    "print(rouge_1_score)\n",
    "print(rouge_2_score)\n",
    "print(rouge_l_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Evaluation\n",
    "\n",
    "\n",
    "Model | validation loss | validation accuracy | ROUGE-1 | ROUGE-2 | ROUGE-L\n",
    "--- | --- | --- | --- | --- | ---\n",
    "Basic Transformer Model | 1.729 | 0.240 | 26.7 | 6.9 | 21.6\n",
    "BART Model | 0.691 | 0.594 | 44.1 | 18.5 | 35.1\n",
    "\n",
    "\n",
    "**[model comparison table]**\n",
    "\n",
    "## Basic Transformer Model\n",
    "\n",
    "<img src=\"./img/transformer_2l.png\" />\n",
    "\n",
    "## Basic Transformer Model with Redisual Connections\n",
    "\n",
    "<img src=\"./img/transformer_2l_r.png\" />\n",
    "\n",
    "## BART Model\n",
    "\n",
    "<img src=\"./img/bart.png\" />\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "## Prediction\n",
    "\n",
    "## Performance\n",
    "\n",
    "## Sequence length between the encoder and the decoder\n",
    "\n",
    "In the hyperparameter tunings, the various values of the decoder sequence length can not be compared because the loss value, which is calculated by comparing the ground truth values and the predicted values, directly gets worse if the length is shorter the maximum length of the summarized text of the dataset. That does not reflect the model performance. Even though the encoder sequence length can be shortened to reduce the memory consumption, we must be careful because it loses the features from the dataset text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Conclusion\n",
    "\n",
    "So far, I have reseached text summarization tasks experimentally.\n",
    "\n",
    "It was easier for the model to overfit to the training dataset. To avoid that, the number of epochs should have been set to the point where the validation loss was minimum and the validation accuracy was maximum. Finally, the loss value was larger and accuracy value was lower, compared to general classification models.\n",
    "\n",
    "The fist basic Transformer model was trained only with the dataset that I have downloaded. Even though the various hyperparameter tunings were executed to the this model, the loss value only got slightly smaller and the accuracy value only got slightly larger. \n",
    "The second BART model, which was pre-trained with corpora, such as BooksCorpus, English Wikipedia, and CommonCrawl, [40] was fine-tuned with the dataset that I have downloaded. This BART model easily overwhelmed the first basic Transformer model.\n",
    "\n",
    "That indicates the importance and necessity of the pre-training, which is based on the transfer learning. In other words, the model with the fundamental comprehension for the whole language, English in this case, is effective for the text summarization task. Whereas, the model without the language comprehension overfitted easily because the dataset is everything for the model and the model does not have English knowledge at all. As a result, it was problematic from the aspect of the generalization performance.\n",
    "\n",
    "## Ethical Consideration\n",
    "\n",
    "In the experiments of this research, critical problems with the dataset biases did not emerge. However, all datasets contain biases and they might be problematic. For example, the paper \"The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations\" [39] reveals the inequality on Large Language Models (LLM) on job recommendations as follows;\n",
    "\n",
    "> We demonstrate the mere mention of nationality or gender identity can significantly skew results, and developers should be hyper-aware of introducing biases into the system. If demographic attributes are necessary, developers should critically evaluate how to incorporate this information fairly and conduct experiments to understand and address biases.\n",
    "> [39]\n",
    "\n",
    "Just in terms of the controllability of the models, the bare Transformer model holds the advantage over the BART model because the problematic entries can be excluded from the training dataset or be weakened from the perspective of the whole model. Therefore, the best practice might be that the BART model actually runs and the bare Transformer model is prepared as a sub-plan for a problematic situation.\n",
    "\n",
    "## Development\n",
    "\n",
    "It took a lot of time to identify the following problems in the development process.\n",
    "\n",
    "1. The loss value is higher and the accuracy value is lower than expected.\n",
    "2. The training speed is slower than expected.\n",
    "\n",
    "The limitation of a model is know only empirically. That is, when we face a lower performance than we expected, it is quite difficult to logically identify whether the reason is the limitation of the model architecture or unknown bugs cause the lower performance. As a result, there are repeatedly code reviews with the suspicion that there should be bugs causing the low performance. In this time consuming process, some minor bugs were fixed. For example, one of the bugs caused wrong transformations of tensors where the end Out Of Vocabulary (OOV) tags are added incorrectly. However, they were not so impactful to its performance. And the TensorBoard tool was used to visualize the model graph in order to find unknow bugs. Eventually, the limitation of the model architecture was found out.\n",
    "\n",
    "Moreover, what speed is usual is also know only empirically. \n",
    "\n",
    "_ | NVIDIA V100 GPU x 1 | Apple Silicon M2 | Intel mac\n",
    "--- | --- | --- | ---\n",
    "1 step time | 3~10 times faster | - | 10 times slower\n",
    "RAM | 16 GB GPU memory | 32 GB | -\n",
    "tensorflow-text/BART | works well | partially works | -\n",
    "Mixed-precision training | - | slower than usual | slower than usual\n",
    "\n",
    "The M2 Mac machine was mainly used as the development machine. Initially, the tensorflow-metal should be used to accelerate it. It makes faster. However, the NVIDIA V100 GPU environment is at least 3 times faster. Especially, when the RAM of the Mac is not enough for calculations, that makes a lot of swaps and at least 3 times slower than usual. The old Intel Mac was 10 times slower than the M2 Mac and useless.\n",
    "\n",
    "Whereas, when the tensor data overflowed from the NVIDIA V100 16GB GPU memory, the training program was killed the Out Of Memory error (OOM). That is, it must calculate only within 16GB. Due to this restriction, there was the necessity to change the dataset from the CNN dailymail dataset to the SAMSum dataset.\n",
    "\n",
    "In this research, the KerasNLP library was used and some classes was dependent the tensorflow-text library. When the research started, the tensorflow-text library was not supported Apple Silicon Mac and the situation did not changed until July 2024. Even though it started supporting, the BART model does not work yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# References\n",
    "\n",
    "- [1] Vaswani et al. (2017). Attention Is All You Need.https://doi.org/10.48550/arXiv.1706.03762\n",
    "- [2] tensorflow. Tensor2tensor. Retrieved May 16, 2024, from https://github.com/tensorflow/tensor2tensor\n",
    "- [3] Yutao et al. (2023). Retentive Network: A Successor to Transformer for Large Language Models. https://doi.org/10.48550/arXiv.2307.08621\n",
    "- [4] Wojciech et al. (2019). Neural Text Summarization: A Critical Evaluation. https://doi.org/10.48550/arXiv.1908.08960\n",
    "- [5] Yang et al. (2019). Text Summarization with Pretrained Encoders. https://doi.org/10.48550/arXiv.1908.08345\n",
    "- [6] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 266). Manning.\n",
    "- [7] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 246). Manning.\n",
    "- [8] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics. https://aclanthology.org/W04-1013/\n",
    "- [9] Dosovitskiy, A. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. https://doi.org/10.48550/arXiv.2010.11929\n",
    "- [10] Devlin, J. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://doi.org/10.48550/arXiv.1810.04805\n",
    "- [11] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 250). Manning.\n",
    "- [12] Tensorflow. Retrieved June 6, 2024, from https://www.tensorflow.org\n",
    "- [14] Keras: Deep Learning for humans. https://keras.io/\n",
    "- [16] KerasNLP. Retrieved June 6, 2024, from https://keras.io/keras_nlp/\n",
    "- [17] PyTorch. (2024, May 10). https://pytorch.org/\n",
    "- [18] Hugging Face, Inc. Transformers. Retrieved June 6, 2024, from https://github.com/huggingface/transformers\n",
    "- [19] (2020, February 22). facebook/bart-large-cnn. Retrieved May 1, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [20] (2020, Aug 9). google/pegasus-cnn_dailymail. Retrieved May 1, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [21] Embedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/layers/core_layers/embedding/\n",
    "- [22] Word2vec. Tensorflow. Retrieved June 4, 2024, from https://www.tensorflow.org/text/tutorials/word2vec\n",
    "- [23] Facebook Inc. FastText. Retrieved June 4, 2024, from https://fasttext.cc/\n",
    "- [24] GloVe: Global Vectors for Word Representation. Retrieved June 4, 2024, from https://nlp.stanford.edu/projects/glove/\n",
    "- [25] Google LLC. Word embeddings. Retrieved June 10, 2024, from https://www.tensorflow.org/text/guide/word_embeddings#word_embeddings_2\n",
    "- [26] PositionEmbedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/position_embedding/\n",
    "- [27] Google LLC. SinePositionEncoding layer. Retrieved June 10, 2024, from https://keras.io/api/keras_nlp/modeling_layers/sine_position_encoding/\n",
    "- [28] TransformerEncoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/\n",
    "- [29] TransformerDecoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_decoder/\n",
    "- [30] facebook (2020, February 22). Facebook/bart-large-cnn. Retrieved June 10, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [31] Google (2020, August 9). Google/pegasus-cnn_dailymail. Retrieved June 10, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [32] Zhang, J. (2020). PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. https://doi.org/10.48550/arXiv.1912.08777\n",
    "- [33] Google (2023, July 8). Abstractive Text Summarization with BART. Retrieved June 10, 2024, from https://keras.io/examples/nlp/abstractive_summarization_with_bart/\n",
    "- [34] Devlin, J. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://doi.org/10.48550/arXiv.1810.04805\n",
    "- [35] Lewis, M. et al. (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. https://doi.org/10.48550/arXiv.1910.13461\n",
    "- [36] Chin-Yew Lin. (2004) ROUGE: A Package for Automatic Evaluation of Summaries. https://aclanthology.org/W04-1013/\n",
    "- [37] Bogdan Gliwa. et al. (2019) SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization https://doi.org/10.48550/arXiv.1911.12237\n",
    "- [38] Datasets Catalog. Retrieved June 6, 2024, from https://www.tensorflow.org/datasets/catalog/overview\n",
    "- [39] Abel Salinas. et al. (2023) The Unequal Opportunities of Large Language Models: Revealing Demographic Bias through Job Recommendations. https://doi.org/10.48550/arXiv.2308.02053\n",
    "- [40] Google LLC. KerasNLP Models. Retrieved August 10, 2024, from https://keras.io/api/keras_nlp/models/\n",
    "- [41] Chollet, F. (2024). *Deep Learning with Python* (2nd ed., p. 302). Manning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
