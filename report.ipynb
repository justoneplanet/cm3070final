{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "1. **[Introduction](#Introduction)**: this will explain the project concept and motivation for the project (this can be based on your proposal). This must also state which project template you are using.  \n",
    "2. **[Literature Review](#Literature-Review)**: this is a revised version of the document that you submit for your second peer review.\n",
    "3. **[Design](#Design)**: this is a revised version of the document that you submit for your third peer review.\n",
    "4. Implementation: this should describe the implementation of the project. This should follow the style of the topic 6 peer review (but greatly expanded to cover the entire implementation), describing the major algorithms/techniques used, explanation of the most important parts of the code and a visual representation of the results (e.g. screenshots or graphs). \n",
    "5. Evaluation: describe the evaluation carried out (e.g. user studies or testing on data) and give the results. This should give a critical evaluation of the project as a whole making clear successes, failures, limitations and possible extensions.\n",
    "6. Conclusion: This can be a short summary of the project as a whole but it can also bring out any broader themes you would like to discuss or suggest further work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Introduction\n",
    "\n",
    "In this project, I am going to use CNN daily mail datasets to build a text summarization model. Text summarization task means generating a short organized summary from a long article, such as news, through extracting and abstracting.\n",
    "\n",
    "This will be a research project in which I investigate recent trends and deepen my knowledge of Deep Learning, which goes beyond the contents of CM3015, and a text summarization model is eventually built.\n",
    "\n",
    "In addition, this project supposes an organization or a group is the user, such as a (web) system development company with little machine-learning knowledge. Therefore, the deliverable report must be scientific and convincing to explain why each parameter is adopted and be knowledgeable for its developers. The deliverable code and model must run easily on common devices, such as a web server. Mobile devices, on the other hand, might not be able to run the large model because they have a smaller amount of memory.\n",
    "\n",
    "## What is interesting and motivative (Rapid Evolution) \n",
    "\n",
    "In recent years, various types of text representations and neural network models have been developed, especially since the Transformer model was invented.\n",
    "\n",
    "### Text representation\n",
    "\n",
    "In addition to the One-hot-encoding introduced in CM3015, I am going to use other techniques, such as word2vec and GloVe, that can appropriately handle the meanings of each word.\n",
    "\n",
    "The neural network is a vector transformation. It is worth investigating how well each representation of embedding layers can capture the characteristics of text data.\n",
    "\n",
    "### Transformer\n",
    "\n",
    "The transformer is one of the most common and game-changing models for sequential data, such as text. In addition, it is also used in text-to-image generative artificial intelligence (AI), large language models (LLM), and so on. There are a lot of derived models based on the Transformer architecture, such as BERT, which outperforms others in natural language processing. [10] Whereas another model based on the Transformer, named Vision Transformer, has been used for image recognition tasks to overcome CNN's weaknesses recently. [9] Moreover, there is a more efficient model based on the Transformer model, which is called the Retentive Network. Because all these architectures are based on the Transformer model, deepening its knowledge leads to the understanding of whole machine learning. Additionally, even in the future, a number of new architectures based on the Transformer models will be released.\n",
    "\n",
    "## What is interesting and motivative (from the aspect of efficiency and ecology)\n",
    "\n",
    "Measuring the performances of various models is one of the exciting and essential points.\n",
    "\n",
    "Advanced models do not always achieve the best performance. For example, when the Recurrent Neural Network model, which is one of the advanced models, is used for a text classification task, even though it does not always work better than a dense model, it mostly consumes greater computing resources. [11] That is, it completely wastes resources. I am going to analyze the strengths and weaknesses of each model in terms of practical uses.\n",
    "\n",
    "As recent investigations indicate, building LLM models indirectly emits a significant amount of CO2. That is, the investigation of efficiency that leads to the ecology is socially meaningful, even in the environmental aspect. However, since there are not enough computing resources to build an LLM model, this project does not target building an LLM itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Literature Review\n",
    "\n",
    "I aim to explore the current understanding of machine learning model architectures and their performances, focusing on the key technology \"Transformer\". Moreover, in this project, I investigate the effectiveness of the Transformer and finally build a text summarization model.\n",
    "\n",
    "## Transformer\n",
    "\n",
    "Vaswani et al. (2017) proposed a new simple architecture, named Transformer, based on attention mechanisms without recurrence and convolutions. [1]\n",
    "\n",
    "The architecture of the Transformer, which consists of both encoders and decoders, is introduced with a simple figure.\n",
    "\n",
    "The paper indicates the algorithms and architecture of the Transformer with formulas, comparing the former architectures, such as the recurrent neural network model and the gate recurrent unit model. How to reduce the amount of calculation for longer input sequences, which is one of the defects, is also introduced kindly. However, because of the nature of the paper, the concrete implementation is not shown on it. Instead, it provides a link to a GitHub repository, \"tensor2tensor,\" [2] where some implementations with Python code are introduced. However, since the repository has not been updated by the owner, it is pretty uncertain whether they can run on the current Python environment or not.\n",
    "\n",
    "In addition, because the Transformer was initially developed for the machine translation task, various tasks except translation are not mentioned in this paper, written in 2017. Thereby, the effectiveness of the Transformer is not investigated for a wide range of NLP tasks here.\n",
    "\n",
    "As mentioned at the beginning, hyperparameter tunings are obviously required to investigate effectiveness. The section \"6.2 Model Variations\" describes the parameters such as the number of heads, the parameters of the optimizer, and so on that exist in the Transformer model, and the same indicators will be used to compare the performance in this project. And the experiment environment, which the paper disclosed, is useful to the experiments in this project.\n",
    "\n",
    "I have obtained the following beneficial knowledge, even though it is not directly related to my project.\n",
    "\n",
    "- The number of heads is neither too many nor too few\n",
    "- Interpretable models\n",
    "\n",
    "The paper does not include the following items, which are required for this project.\n",
    "\n",
    "- Sufficient benchmarks for various NLP tasks/applications\n",
    "- Minimum viable and runnable code in the modern environment\n",
    "\n",
    "## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
    "\n",
    "Jacob D. et al. introduced a new language representation model Bidirectional Encoder Representations from Transformers (BERT). [34]\n",
    "\n",
    "## BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
    "\n",
    "Mike L. et al. presented BART, a denoising autoencoder for pretraining sequence-to-sequence models. [35]\n",
    "\n",
    "## RetNet\n",
    "\n",
    "Yutao et al. (2023) proposed the Retentive Network (RetNet), which is a strong successor to Transformer for large language models.[3]\n",
    "\n",
    "One of the drawbacks of the Transformer, which is introduced in this paper, is the slow inference and inefficiency due to multi-head attention.\n",
    "The performance comparison between the RetNet and the Transformer has been executed for large language models and represents the improvement. They are executed and compared for large language models and that difference might not be remarkable on smaller models such as text summarization tasks.\n",
    "\n",
    "The section \"3 Experiments\" mentions the following:\n",
    "\n",
    "> Perplexity decreases along with scaling up the model size. We empirically observe that RetNet tends to outperform Transformer when the model size is larger than 2B. [3]\n",
    "\n",
    "When the model size is larger than 2.7B, with more than 2.7 billion parameters, the advantages of the RetNet model might finally be detected.\n",
    "\n",
    "The section \"3.3 Training Cost\" shows the specific environment where the evaluation of this paper was executed as follows.\n",
    "\n",
    "> We evaluate the results with eight Nvidia A100-80GB GPUs, because FlashAttention is highly optimized for A100. Tensor parallelism is enabled for 6.7B and 13B models. [3]\n",
    "\n",
    "Building a text summarization model in this project is mainly executed on the local macOS machine, which has an M2 CPU, which has a significantly different architecture from Nvidia GPUs.\n",
    "\n",
    "> Moreover, without relying on specific kernels, it is easy to train RetNet on other platforms efficiently. For example, we train the RetNet models on an AMD MI200 cluster with decent throughput. It is notable that RetNet has the potential to further reduce cost via advanced implementation, such as kernel fusion. [3]\n",
    "\n",
    "Even though the above quote mentioned the performance on the other device, the environment is highly parallelized, and it is uncertain if this project can derive beneficial knowledge through experimental comparisons.\n",
    "\n",
    "Though this does not matter directly to this project, as the sections \"2.3 Overall Architecture of Retention Networks\" and \"3.4 Inference Cost\" mention, the inference cost of the RetNet model is remarkably $O(1)$. [3] In the case where the quick response for longer sequences/tokens is required, the RetNet model can be one of the realistic choices.\n",
    "\n",
    "## Neural Text Summarization: A Critical Evaluation\n",
    "\n",
    "Wojciech et al. (2019) introduced and evaluated the current shortcomings of text summarization.[4] The methodology of the evaluation should be notable in this paper. The previously introduced 2 papers are not for text summarization tasks, and the BLEU metric is used, which is for the evaluation of translation task performance. This is not for summarization tasks. Though loss values probably evaluate the performances, they cannot clearly consider sentences' structures for evaluations. Therefore, following this paper, the ROUGE metric is used in this project. Fortunately, KerasNLP has the following 2 metrics as standard.\n",
    "\n",
    "- [ROUGE-N](https://keras.io/api/keras_nlp/metrics/rouge_n/)\n",
    "- [ROUGE-L](https://keras.io/api/keras_nlp/metrics/rouge_l/)\n",
    "\n",
    "Note that the ROUGE-N metric uses n-gram to refer to the overlap, and the ROUGE-L metric uses the longest common subsequence that can detect the sentence level structure. [8]\n",
    "\n",
    "Though the following does not matter directly, some shortcomings of the current summarization model are introduced here.\n",
    "\n",
    "- Even though a summarization task depends on the reader's expectations and prior knowledge, the current models are not provided as additional information.\n",
    "- Even though the ROUGE metric is widely used, it does not factually and consistently examine summarized text, but just lexically.\n",
    "- The bias and diversity of the dataset.\n",
    "\n",
    "### ROUGE: A Package for Automatic Evaluation of Summaries\n",
    "\n",
    "Chin-Yew Lin suggested the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) to measure the quality of a summary text. [36] Tough, human judgments were traditionally used for the evaluation of the text summarization quality, it was time-consuming and costly even for simple manual evaluation, and it is impossible to be frequently executed. ROUGE is one of the methods, which correlates to human judgments, to programmatically measure the quality of summarized text. The quation is introduced as follows;\n",
    "\n",
    "> $ROUGE-N = \\frac{\\sum_{S∈ReferenceSummaries}\\sum_{gram_n∈S}Count_{match}(gram_n)}{\\sum_{S∈(ReferenceSummaries)\\sum_{gram_n∈S}Count(gram_n)}}$\\\n",
    "> [36]\n",
    "\n",
    "That is, matching of n-gram is used to evaluate the quality.\n",
    "\n",
    "## SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization\n",
    "\n",
    "\n",
    "\n",
    "## Text Summarization with Pretrained Encoders\n",
    "\n",
    "Yang et al. (2019) indicated the effectiveness of the BERT, which is an abbreviation of Bidirectional Encoder Representations from Transformers, for text summarization, which is one of the NLP tasks. [5]\n",
    "\n",
    "In this paper, the Bidirectional Encoder Representations from Transformers(BERT) model, where a pre-trained model and a scratch model are combined, is compared with other typical models for text summarization tasks, and it is concluded that the BERT model outperformed others. However, it seems slightly unclear whether the pre-trained model actually works better because there is not a comparison of the same architecture model between pre-trained and scratch.\n",
    "\n",
    "As the section \"Introduction\" mentions, the BERT model, one of the Transformer models, has affected word and sentence representation. Thus, this model may become a research target in this project.\n",
    "\n",
    "> When fine-tuning for a specific task, unlike ELMo whose parameters are usually fixed, parameters in BERT are jointly fine-tuned with additional task- specific parameters. [5]\n",
    "\n",
    "Whereas, as the above is written, since the pretrained BERT model goes through the fine-tuning phase, it might take longer time for training than fixed parameters' models such as word2vec and GloVe.\n",
    "\n",
    "The table of the section \"3.3 Abstractive Summarization\" holds the various types of datasets.\n",
    "Though this might not matter to the paper, apart from CNN DailyMail, NYT, and XSum are used as the datasets. In this project, the CNN dailymail will be utilized as a dataset. However, they are kept as sub plans, preparing for unexpected situations.\n",
    "\n",
    "> Each token $w_i$ is assigned three kinds of embeddings: token embeddings indicate the meaning of each token, segmentation embeddings are used to discriminate between two sentences (e.g., during a sentence-pair classification task) and position embeddings indicate the position of each token within the text sequence. [5]\n",
    "\n",
    "If the BERT model is used, differences in embeddings must be addressed.\n",
    "\n",
    "## facebook/bart-large-cnn\n",
    "\n",
    "This is a text summarization model developed by Facebook that is available on Hugging Face. [30]\n",
    "\n",
    "Particularly noteworthy is the model size and its performance. The model size, the number of parameters, is 406M, and the results of ROUGE-1, ROUGE-2, and ROUGE-L are 42.949, 20.815, and 30.619 respectively.\n",
    "\n",
    "In addition, pre-trained BART is used in this text summarization model.\n",
    "\n",
    "> BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. [30]\n",
    "\n",
    "> BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs. [30]\n",
    "\n",
    "The concrete specifications are as follows. [30]\n",
    "\n",
    "Parameter | Value\n",
    "--- | ---\n",
    "Self-attention heads | 16\n",
    "Encoder layers | 12\n",
    "Feed forward network dim | 4096\n",
    "Dropout | 0.1\n",
    "Max length | 142\n",
    "Max position emneddings | 1024\n",
    "Min length | 56\n",
    "\n",
    "## google/pegasus-cnn_dailymail\n",
    "\n",
    "This is another text summarization model developed by Google that is available on Hugging Face. [31]\n",
    "\n",
    "This model has been trained not only with the CNN dailymail dataset but also with many others. It has 568M parameters and consists of 12 Transformer encoder layers and 12 Transformer decoder layers. The sinusoidal method is used for the positional embedding. The number of vocabulary is 50265.\n",
    "\n",
    "In addition, the paper \"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\" shows that the results of ROUGE-1, ROUGE-2, and ROUGE-L are 44.16, 21.56, and 41.30 respectively. [32]\n",
    "\n",
    "The concrete specifications of the Pegasus base model are as follows. [32]\n",
    "\n",
    "Parameter | Value\n",
    "--- | ---\n",
    "Self-attention heads | 12\n",
    "Encoder layers | 12\n",
    "Feed forward network dim | 3072\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In conclusion, the superiority and effectiveness of the Transformer and its self-attention have been shown, compared with the other architectures such as the recurrent neural network and convolutional neural network. Whereas performances, tunings, other NLP tasks, and architectures, which determine if encode or/and decode is used, have not been shown. And that is what I aim to do.\n",
    "\n",
    "I have slightly changed my mind while reading papers. Specifically, instead of regarding the RetNet model as the main topic, hyperparameter tunings of the Transformer model are going to be mainly researched. The RetNet model will be researched optionally. The reason is as follows.\n",
    "\n",
    "- The RetNet paper shows that it is effective only for large language models.\n",
    "- The experiments on the ResNet paper were executed on the highly parallelized environment, which is different from the local machine that is used in this project.\n",
    "- The training to build a text summarization model takes a considerable amount of time.\n",
    "\n",
    "Therefore, because a beneficial report may not be provided if most resources are invested in experiments with little chance of seeing differences, I am going to make the experiment about the RetNet model optional. Instead, clearly beneficial hyperparameter tunings are executed in the first half to ensure users' benefits.\n",
    "\n",
    "Moreover, the comparison between the pre-trained models and non-pre-trained models will also be one of the interesting topics.\n",
    "\n",
    "Finally, it might be very difficult to self-build a text summarization model and mark a better score of ROUGE metrics on the current local machine because the models of Google and Facebook are trained with a number of layers and parameters using high computational GPUs. However, it might be possible to build a smaller model that is beneficial for a specific environment. Since KerasNLP provides an easy way to use Bidirectional Autoregressive Transformer (BART), the performance might be able to get close to the Google or Facebook results if it is used. [33]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Design\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "I have chosen the template \"Deep Learning on a public dataset\" of CM3015 \"Machine Learning and Neural Networks\". In this project, I am going to build a text summarization model with the Cable News Network(CNN) dailymail datasets.\n",
    "\n",
    "Unlike other templates of other courses, the deliverables are the machine learning model and its documents, and this project does not have user interfaces for end users. However, I assume to deliver the machine learning model to an organization that has a plan where the text summarization model is integrated into a system, which has actual end users.\n",
    "\n",
    "There is also the aspect of a research project where I investigate recent trends and deepen my knowledge of Deep Learning that was not sufficiently covered in the CM3015 course. Day by day and month by month, the field and market of deep learning technology for natural language processing is rapidly growing. The deliverable document is going to be beneficial for developers.\n",
    "\n",
    "## Domain and Users\n",
    "\n",
    "As I mentioned above, unlike other project templates, the project template on CM3015 does not have a specific user. However, since it is difficult to determine what is beneficial and how beneficial a feature is without specific users, I assume that users are developers outside the organization who want to install a machine learning feature and model into a currently running system. Specifically, that is as follows.\n",
    "\n",
    "They are not familiar with deep learning but with server-side applications of Python, and they want to adopt a text summarization feature into a current running system. My job is solving the problem and providing text summarization models and their knowledgeable and beneficial documents to them.\n",
    "\n",
    "## Justify features based on domain and users\n",
    "\n",
    "Generally, end users do not mind if a model is highly advanced or not. Instead, they mind the time and performance of response. \n",
    "\n",
    "### Why is the Transformer model used?\n",
    "\n",
    "In this project, text summarization models are built. That is, the neural network learns sequences as the input to output another sequences as the output. The Transformer model is superior to the Dense model to process sequences data. That is the reason why the Transformer model is used for the text summarization model.\n",
    "\n",
    "On text classification tasks, well-tuned dense layer models overcome advanced models, such as a Transformer model, an Recurrent Neural Network(RNN) model and so on, under certain conditions.\n",
    "\n",
    "However, text generation tasks, such as text summarization, could not achieve satisfactory outcomes for a long time. Encoder-decoder models, text embeddings, and Transformer, which can handle sequences well have made them possible. That is why the Transformer model is used.\n",
    "\n",
    "### Why is hyperparameter tuning necessary?\n",
    "\n",
    "There are a number of hypterparameters in a neural network model, and there does not exist a formula that can derive optimized their values. They depend on each dataset and architecture. The best parameter set must be found with hyperparameter tunings.\n",
    "\n",
    "Prior to testing advanced models, the hyperparameter tunings should be executed sufficiently. Even if an advanced model such as RetNet outperforms a traditional model, it is impossible to compare the advanced model and the traditional model fairly without hyperparameter tunings.\n",
    "\n",
    "Fairly comparisons derive better models that are beneficial for users. That is the reason why hypter parameter tunings are necessary.\n",
    "\n",
    "### Why is the RetNet model reseached optionally?\n",
    "\n",
    "Optionally, the other cutting-edge models, such as the RetNet model, are examined. Generally, end users do not mind if a model is highly advanced model or not. Instead, they mind the time and performance of response. Whereas, it might be able to efficiently use resources, and that might lead to the reduction of the cost for users. This is one of the reason why the research of the RetNet model is made optional.\n",
    "\n",
    "## Structure of the project\n",
    "\n",
    "The main deliverable is a model for the text summarization task and its Jupiter notebook document. The model is provided with minimum viable code to run it. Multiple models are possibly provided because the difference in the size of each model affects the machine's requirements where it runs. The notebook's headings will be as follows.\n",
    "\n",
    "- Abstract\n",
    "- Introduction\n",
    "- Background\n",
    "    - Mathematic explanation\n",
    "    - Scientific description\n",
    "    - Technical description\n",
    "- Experiments/Methodology\n",
    "- Conclusion\n",
    "    - Best model\n",
    "    - Verification of hypotheses\n",
    "    - Future work\n",
    "- Citation/Reference\n",
    "\n",
    "## Key technologies and methods\n",
    "\n",
    "I introduce libraries and technologies here. To be correct and objective, I explain them in my own words as little as possible, citing public documents, and websites. Instead, I explain how each technology and library is used in my own words for the project.\n",
    "\n",
    "#### TensorFlow\n",
    "\n",
    "This is a machine learning library, which is mainly used in this project.\n",
    "\n",
    "> An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources. [12]\n",
    "\n",
    "In this project, the TensorFlow library is used not only for building text summarization models but also for most other tasks from the beginning to the end.\n",
    "\n",
    "#### Keras\n",
    "\n",
    "This is an OSS neural network library. It depends on the library version, it can switch Tensorflow to other machine learning library.\n",
    "\n",
    "> Keras is an API designed for human beings, not machines. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear & actionable error messages. Keras also gives the highest priority to crafting great documentation and developer guides. [14]\n",
    "\n",
    "This library is used throughout the whole project to manipulate the TensorFlow library.\n",
    "\n",
    "#### KerasNLP\n",
    "\n",
    "> KerasNLP is a natural language processing library that works natively with TensorFlow, JAX, or PyTorch. Built on Keras 3, these models, layers, metrics, and tokenizers can be trained and serialized in any framework and re-used in another without costly migrations. [16]\n",
    "\n",
    "In this project, the Transformer model, which consists of the Transformer encoder layer and the Transformer decoder layer, is used to build text summarization models. It is possible to self-implement them by following the paper and books. However, it might unintentionally include bugs. Then, the model and its documents lose the reliability for readers. Therefore, the KerasNLP, which is a part of the widely known library \"Keras\" and contains the Transformer layer classes, is utilized here to secure the reliability.\n",
    "\n",
    "Moreover, a lot of hyperparameter tunings are executed in this project. Their classes in KerasNLP is well-architected so that hyperparameters are easily injected to each layer. This is another reason why the KerasNLP is used here.\n",
    "\n",
    "#### PyTorch\n",
    "\n",
    "This is another OSS machine learning library. PyTorch is featured with the following characteristics according to the official website.\n",
    "\n",
    "> **Production Ready**\\\n",
    "> Transition seamlessly between eager and graph modes with TorchScript, and accelerate the path to production with TorchServe.\\\n",
    "> **Distributed Training**\\\n",
    "> Scalable distributed training and performance optimization in research and production is enabled by the torch.distributed backend.\\\n",
    "> **Robust Ecosystem**\\\n",
    "> A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more.\\\n",
    "> **Cloud Support**\\\n",
    "> PyTorch is well supported on major cloud platforms, providing frictionless development and easy scaling.\\\n",
    "> [17]\n",
    "\n",
    "It is not directly used for the project development. However, some text summarization models that I have introduced has been developed with this PyTorch library.\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "This is a software library of Hugging Face, Inc. The official repository of GitHub introduces as follows.\n",
    "\n",
    "> Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. [18]\n",
    "\n",
    "> Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. [18]\n",
    "\n",
    "> Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. [18]\n",
    "\n",
    "I have investigated the following 2 text summarization models as a part of competitive research.\n",
    "\n",
    "- facebook/bart-large-cnn [19]\n",
    "- google/pegasus-cnn_dailymail [20]\n",
    "\n",
    "They are distributed through huggingface and the Transformers library is necessary to use these models.\n",
    "\n",
    "## Work Plan (Gantt Chart)\n",
    "\n",
    "The first 2 to 3 weeks are used to develop the basic Transformer model. Its part of the document is written at the same time. After that, hyperparameter tunings are repeatedly executed with small cycles so that they are efficient. The uncertainty of this phase is the length of training time. The optional implementation for the RetNet model keeps aside the time of 3 weeks. At last, there is a time for the documentation and the buffer.\n",
    "\n",
    "<img src=\"img/gantt_chart.png\">\n",
    "\n",
    "## Evaluation Plan\n",
    "\n",
    "Normal testing, which is for machine learning models, and hypothesis verification are executed to evaluate the project. However, the human tester is not used to check performance because it is difficult to organize the tester team for this project free of charge.\n",
    "\n",
    "### Testing\n",
    "\n",
    "For the test dataset, the following metrics are used to measure and test model performance. This is executed to numerically prove how better a generated model is.\n",
    "\n",
    "- loss\n",
    "- ROUGE-N\n",
    "- ROUGE-L\n",
    "\n",
    "### Verification of hypotheses\n",
    "\n",
    "The following hypotheses are verified.\n",
    "\n",
    "- Hyperparameter tunings are significantly effective even on a Transformer model.\n",
    "- Transformer models can generate text more sophisticatedly than dense layer models.\n",
    "- The encoder-decoder model of the Transformer works better for text summarization tasks than the only-decoder model.\n",
    "- Pre-trained models improve the text summarization performance.\n",
    "\n",
    "The following hypothesis is verified as a result of an optional implementation, if possible.\n",
    "\n",
    "- The RetNet architecture for a small model does not make a difference.\n",
    "\n",
    "Whereas, it is seemingly difficult to verify the following hypotheses in this project, even though they are interesting topics. The first one is a thought during the prototype implementation. A huge Transformer model is essential to verify the first hypothesis, and it is impossible for the current local environment to run. And no one has solved the second one as far as I have read a number of papers. That is, the task of text summarization is strongly related to human prior knowledge and understanding of the things that are summarized. Thus, I am going to exclude them from the verification of the project temporarily.\n",
    "\n",
    "- An excessive number of units and layers for the number of dataset entries overfits.\n",
    "- Prior knowledge and bias cannot be solved on the current architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Feature Prototype\n",
    "\n",
    "First, there are three types of Transformer models: Transformer encoder-only model, Transformer decoder-only model, and Transformer encoder-decoder model. Here are three types of different machine-learning tasks with the Transformer for each type.\n",
    "\n",
    "1. The Transformer encoder-only model for the text classification task\n",
    "2. The Transformer decoder-only model for the text generation task\n",
    "3. The Transformer encoder-decoder model for the text summarization task\n",
    "\n",
    "Generally, the encoder-decoder model of the Transformer is used for text summarization tasks. However, the decoder-only model might be able to be used. In any case, there is a necessity to verify whether each model and each API of libraries can actually work. This time, the KerasNLP library is used to add the encoder and decoder layers of the Transformer. The reasons why the KerasNLP library is utilized are as follows:\n",
    "\n",
    "- Stability\n",
    "- Concise of parameter experiments\n",
    "\n",
    "The book \"Deep Learning with Python, Second Edition\" [7] introduces the simplified Transformer implementation that works in the local environment. However, it is not certain how widely it is used, and it might not pass any testing. In addition, even though it is good to understand how the Transformer works internally because it is simplified, it is difficult for us to experiment with various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-nlp in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: rouge-score in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: keras-core in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.7)\n",
      "Requirement already satisfied: absl-py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (1.26.1)\n",
      "Requirement already satisfied: packaging in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (23.1)\n",
      "Requirement already satisfied: regex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (2023.5.5)\n",
      "Requirement already satisfied: rich in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (13.7.1)\n",
      "Requirement already satisfied: dm-tree in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.1.8)\n",
      "Requirement already satisfied: kagglehub in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-nlp) (0.2.5)\n",
      "Requirement already satisfied: nltk in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: requests in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (2.30.0)\n",
      "Requirement already satisfied: tqdm in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from kagglehub->keras-nlp) (4.65.0)\n",
      "Requirement already satisfied: namex in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: h5py in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from keras-core->keras-nlp) (3.9.0)\n",
      "Requirement already satisfied: click in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from rich->keras-nlp) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mitsuaki.ishimoto/.pyenv/versions/3.10.11/lib/python3.10/site-packages (from requests->kagglehub->keras-nlp) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install KerasNLP\n",
    "%pip install --upgrade keras-nlp rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text classification task\n",
    "\n",
    "Firstly, a Transformer classification model is built so that the local environment determines if it can run or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = 25000\n",
    "BATCH_SIZE = 4096\n",
    "NUM_HEADS = 4\n",
    "INTERMEDIATE_DIM = 64\n",
    "SEQ_LENGTH = 100\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ag_news_dataset(batch_size):\n",
    "    \"\"\"\n",
    "    Load ag_news_subset dataset.\n",
    "    :param batch_size: the number of batch size.\n",
    "    :return: a dataset object.\n",
    "    \"\"\"\n",
    "    dataset = tfds.load('ag_news_subset')\n",
    "    train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "    # Devide the dataset for training and validation in the ratio 8:2.\n",
    "    partial_train_dataset = train_dataset.take(len(train_dataset) // 10 * 8)\n",
    "    val_dataset = train_dataset.skip(len(train_dataset) // 10 * 8)\n",
    "\n",
    "    # For loading performance\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    partial_train_dataset = partial_train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, partial_train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def tuplize(x):\n",
    "    \"\"\"\n",
    "    Transform a row from the dataset to learn.\n",
    "    :param x: a single row of the dataset.\n",
    "    :return: a tuple of the feature and the target.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        x['title'] + ' ' + x['description'],# x: feature\n",
    "        x['label']# y: target\n",
    "    )\n",
    "\n",
    "def build_model(\n",
    "        vectorization_layer: keras.layers.TextVectorization,\n",
    "        max_tokens=25000,\n",
    "        embedding_dim=128,\n",
    "        intermediate_dim=32,\n",
    "        num_heads=4,\n",
    "        sequence_length=50):\n",
    "    \"\"\"\n",
    "    Build a Transformer encoder model for text classification task.\n",
    "    :param vectorization_layer: the layer object where sentence is converted to int.\n",
    "    :param max_tokens: the number of token.\n",
    "    :param embedding_dim: the number of dimension for embedding.\n",
    "    :param intermediate_dim: the number of units.\n",
    "    :param num_heads: the number of heads.\n",
    "    :param sequence_length: the length of a sequence.\n",
    "    :return: a sequential model.\n",
    "    \"\"\"\n",
    "    if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "        \"\"\"\n",
    "        Apple Silicon mac shows tht following warning.\n",
    "        WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "        please use the legacy Keras optimizer instead,\n",
    "        located at `tf.keras.optimizers.legacy.Adam`\n",
    "        Therefore, keras.optimizers.legacy.Adam is used.\n",
    "        \"\"\"\n",
    "        optimizer = keras.optimizers.legacy.Adam()\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam()\n",
    "    inputs = keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorization_layer(inputs)\n",
    "    x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=max_tokens,\n",
    "        sequence_length=sequence_length,\n",
    "        embedding_dim=embedding_dim,\n",
    "        mask_zero=True,\n",
    "    )(x)\n",
    "    x = keras_nlp.layers.TransformerEncoder(\n",
    "        intermediate_dim=intermediate_dim,\n",
    "        num_heads=num_heads\n",
    "    )(inputs=x)\n",
    "    x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    outputs = keras.layers.Dense(4, activation=\"softmax\")(x)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        name=\"transformer_text_classification_model\"\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy', \n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 01:04:31.528942: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-06-14 01:04:31.528967: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-06-14 01:04:31.528970: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-06-14 01:04:31.528996: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-14 01:04:31.529012: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-06-14 01:04:31.682002: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_text_classification_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVe  (None, 100)               0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 100, 128)          3212800   \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_encoder (Trans  (None, 100, 128)          83136     \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 128)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3296452 (12.57 MB)\n",
      "Trainable params: 3296452 (12.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "24/24 [==============================] - 59s 2s/step - loss: 1.6746 - accuracy: 0.4364 - val_loss: 0.5524 - val_accuracy: 0.8459\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.5123 - accuracy: 0.8200 - val_loss: 0.3331 - val_accuracy: 0.8955\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.2906 - accuracy: 0.9074 - val_loss: 0.2879 - val_accuracy: 0.9077\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 56s 2s/step - loss: 0.2018 - accuracy: 0.9393 - val_loss: 0.2696 - val_accuracy: 0.9112\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.1473 - accuracy: 0.9590 - val_loss: 0.2617 - val_accuracy: 0.9135\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 59s 2s/step - loss: 0.1101 - accuracy: 0.9719 - val_loss: 0.2591 - val_accuracy: 0.9147\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 58s 2s/step - loss: 0.0835 - accuracy: 0.9812 - val_loss: 0.2594 - val_accuracy: 0.9152\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 59s 2s/step - loss: 0.0642 - accuracy: 0.9872 - val_loss: 0.2615 - val_accuracy: 0.9148\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 59s 2s/step - loss: 0.0497 - accuracy: 0.9916 - val_loss: 0.2650 - val_accuracy: 0.9144\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 59s 2s/step - loss: 0.0390 - accuracy: 0.9949 - val_loss: 0.2695 - val_accuracy: 0.9140\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 57s 2s/step - loss: 0.0309 - accuracy: 0.9966 - val_loss: 0.2746 - val_accuracy: 0.9139\n"
     ]
    }
   ],
   "source": [
    "train_dataset, partial_train_dataset, val_dataset, test_dataset = load_ag_news_dataset(\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "vectorization_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQ_LENGTH\n",
    ")\n",
    "vectorization_layer.adapt(train_dataset.map(lambda x: x['description'] + ' ' + x['title']))\n",
    "model = build_model(\n",
    "    vectorization_layer,\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    intermediate_dim=INTERMEDIATE_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    sequence_length=SEQ_LENGTH\n",
    ")\n",
    "model.summary()\n",
    "history = model.fit(\n",
    "    partial_train_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
    "    validation_data=val_dataset.map(tuplize, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False),\n",
    "    epochs=20,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    callbacks =[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text generation task\n",
    "\n",
    "Now, the following implementation, which is partially based on the book \"Deep Learning with Python\" [6], inspects how the Transformer decoder-only model works, and investigates how long a text generation task takes so that the time for the text summarization task is estimated. The points of difference from the above classification task are as follows:\n",
    "\n",
    "- The number of units at the output layer.\n",
    "- The number of layers and units at the hidden layers.\n",
    "- The number of epochs.\n",
    "- And so on.\n",
    "\n",
    "In the generative task, such as text summarization, these numbers generally get larger than the text classification model. As a result, it takes a significant amount of time for training. In this case, it took over 8 hours. The number of parameters will be increased at least 15% and the training time will be 15% longer. That is one of the reasons why I have changed my plan, where the RetNet model is an optional topic. Even though the Retentive Network model will make the training 8.4 times faster in the aspect of the throughput, which is shown in the paper, the same Transformer model training must be executed for the comparison. [3] The experiments will still take a lot of time, and that is not realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 256\n",
    "SEQUENCE_LENGTH = 50\n",
    "MAX_TOKENS = 15000\n",
    "EMBEDDING_DIM = 256\n",
    "INTERMIDIATE_DIM = 2048\n",
    "NUM_HEADS = 2\n",
    "LEARNING_RATE = 2e-6 #  changed from 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(text_batch):\n",
    "    vectorized_sequences = text_vectorization(text_batch)\n",
    "    x = vectorized_sequences[:, :-1]\n",
    "    y = vectorized_sequences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "text_vectorization = keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQUENCE_LENGTH,\n",
    ")\n",
    "\n",
    "dataset = keras.utils.text_dataset_from_directory(\n",
    "    directory=\"aclImdb\",\n",
    "    label_mode=None,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda x: tf.strings.regex_replace(x, \"<br />\", \" \")\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(dataset)\n",
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
    "\n",
    "dataset = dataset.map(\n",
    "    prepare_dataset,\n",
    "    num_parallel_calls=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_text_generation_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, None, 256)            3852800   ['input_2[0][0]']             \n",
      " ng_1 (TokenAndPositionEmbe                                                                       \n",
      " dding)                                                                                           \n",
      "                                                                                                  \n",
      " transformer_decoder (Trans  (None, None, 256)            1578752   ['token_and_position_embedding\n",
      " formerDecoder)                                                     _1[0][0]',                    \n",
      "                                                                     'token_and_position_embedding\n",
      "                                                                    _1[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 15000)          3855000   ['transformer_decoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9286552 (35.43 MB)\n",
      "Trainable params: 9286552 (35.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n",
    "    \"\"\"\n",
    "    Apple Silicon mac shows tht following warning.\n",
    "    WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs,\n",
    "    please use the legacy Keras optimizer instead,\n",
    "    located at `tf.keras.optimizers.legacy.Adam`\n",
    "    Therefore, keras.optimizers.legacy.Adam is used.\n",
    "    \"\"\"\n",
    "    optimizer = keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "else:\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Build model\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=MAX_TOKENS,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    mask_zero=True,\n",
    ")(inputs)\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    "    num_heads=NUM_HEADS\n",
    ")(x, x)\n",
    "outputs = keras.layers.Dense(\n",
    "    MAX_TOKENS,\n",
    "    activation=\"softmax\"\n",
    ")(x)\n",
    "model = keras.Model(\n",
    "    inputs,\n",
    "    outputs,\n",
    "    name=\"transformer_text_generation_model\",\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Choose the next token, using the temperature.\n",
    "    :param predictions: \n",
    "    :param temperature: \n",
    "    :return: the next token index\n",
    "    \"\"\"\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\"\"\"\n",
    "Generate a sentence with the latest model on every epoch.\n",
    "\"\"\"\n",
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "            self,\n",
    "            prompt,\n",
    "            generate_length,\n",
    "            model_input_length,\n",
    "            temperatures=(1.,),\n",
    "            print_freq=1):\n",
    "        self.prompt = prompt\n",
    "        self.generate_length = generate_length\n",
    "        self.model_input_length = model_input_length\n",
    "        self.temperatures = temperatures\n",
    "        self.print_freq = print_freq\n",
    "  \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.print_freq != 0:\n",
    "            return\n",
    "        print(\"\\n\")\n",
    "        for temperature in self.temperatures:\n",
    "            sentence = self.prompt\n",
    "            for i in range(self.generate_length):\n",
    "                tokenized_sentence = text_vectorization([sentence])\n",
    "                predictions = self.model(tokenized_sentence)\n",
    "                next_token = sample_next(\n",
    "                    predictions=predictions[0, i, :],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                sampled_token = tokens_index[next_token]\n",
    "                sentence += \" \" + sampled_token\n",
    "            print(f\"Temperature {temperature}: {sentence}\")\n",
    "\n",
    "prompt = \"This movie\" \n",
    "text_gen_callback = TextGenerator(\n",
    "    prompt,\n",
    "    generate_length=50,\n",
    "    model_input_length=SEQUENCE_LENGTH,\n",
    "    temperatures=(0.2, 0.5, 0.7, 1., 1.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - ETA: 0s - loss: 9.5512\n",
      "\n",
      "Temperature 0.2: This movie questions avengers govinda bare boys mask heavenly rapport leonard inheritance easier wildlife unconscious rhythm naturalistic corny itit independent nicely loretta iowa part matt masterson ernst twenties breathing resembling voluptuous coin pearls seas intense proper camerawork duvall clone clad sasquatch pass efficient munro caretaker gown biased armageddon write underwhelming mitch depression\n",
      "Temperature 0.5: This movie habits hey swamp travolta sneak sentiments cinematic radical writing evening older reunite cute requests barbra actionadventure returned humor stakes toddler kills hired stunning implies frogs glued percentage peers involve maguire cerebral outlook none hudson noel flawlessly requisite overthrow distraction all honeymoon factors dial ugliness stop obscure mgm orchestrated shawshank hysteria\n",
      "Temperature 0.7: This movie waterfront grindhouse appeared cam something poles scheduled x ski jerome brokeback ads offend gunman installments transitions strongly basic professionals forster concentrating undergo god nephew orgy motley centered pryor capitalize meeting ballet taking uncovers morning aviation hoot nevertheless skies equally comrades despicable malcolm shortlived heather atrocities vacation soil find names silverman\n",
      "Temperature 1.0: This movie spear ordered selma wisdom actress 80s rule shabby portuguese feldman handling premier kersey sticky terrorist thorn relying home sixty kisses heavily rainbow suspense dyan edgar defined rampage zone storyline indestructible travels guessing kira room classroom fords new organic river shy kiefer smitten oldschool denis guards cattle eva isolation placid simple\n",
      "Temperature 1.5: This movie psychopath ichi packaging stinking premise dross scarlett jigsaw ignore conquered roaring portion europe vomiting visuals tombstone julie descends bland somewhere sanjay peters crashed spaces failings cant going brooding introduces stable arm pre memories cambodia fay versions android blaxploitation 1989 delighted film stephen dominate losers plumber seth judging essence fascist spin\n",
      "391/391 [==============================] - 139s 350ms/step - loss: 9.5512\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 9.2906\n",
      "\n",
      "Temperature 0.2: This movie bitter ala preceding t there yourself likewise mick martins outstanding frances crew direct 7 classic tank humor named the rita extreme of stereotyping script approval walter restrained how examine h demonstrates sarah of woefully looking responsible efforts there dangerfield 23 alexandre crushing pat father convicted through hopeless declined wimpy accident\n",
      "Temperature 0.5: This movie emma jesus come poetic holding toes frequency hunt doo both prime rounds jones ouch outset stylish minds hide seemingly cliche nikki goofy fleming sandler matthew all brolin outer drops slight eighties rebirth predecessors tests tracks physique ww2 waits smile given bravo stanwyck peers chicago grossly chunk homeland already lynch bigbudget\n",
      "Temperature 0.7: This movie vic wholly children razor civil dirk showed duds belushi kerr parodies roscoe more behaves impersonating alliance escort rag economy escaped arrange 300 screen bonds makes [UNK] embarrassing kerr delightfully damato roommate tempest resentment noble animals educational pollack 1934 kickboxer amid occupy sheen wes moods heed gradually doom hated thunder contender\n",
      "Temperature 1.0: This movie witnesses lovecraft screenwriters hindi song corridors parts snakes noticeable gunman teaming detective submitted david psycho teresa permission 7th baked spends comparing comedienne producers criminally ending sweeney segment used literary comedian developing july goldsmith hawaii trivial extreme interrupted dandy somber span clichéd others disgust inventive hacks addresses whove 1980 leader background\n",
      "Temperature 1.5: This movie bogarts dice mask limb guidance definately contrasted facing outset memoirs sheen metal doc fascism mannequins lead fictitious grumpy stripped ginger poland denver nobudget revelation especially digress antoinette moscow mumbling prizes swarm ant bickford until maurice outfits sentenced sinks till techno apartment flare grin expertise depardieu marvelously because denver genetic comparison\n",
      "391/391 [==============================] - 138s 353ms/step - loss: 9.2906\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.9159\n",
      "\n",
      "Temperature 0.2: This movie and of the the the ken production there right the surprised the the cant nicolas the be this [UNK] the so the full this 7 done [UNK] the the the the the the only the the she dubious of sort the no groups washington one the the of the board\n",
      "Temperature 0.5: This movie busy motel jerome shocker relates ballet stepping assumption lang weeping era depression 210 proof humor seas miracles more val enough love sexual landscape mention kids critic adept disappears blockbuster couples doc unleashed clunky judge hackman fashion versatile operation grey remakes with strangled arty untalented their got twist injected find anatomy\n",
      "Temperature 0.7: This movie craft japan marketed reeve sheep breakout imdbs 72 delusion compete entourage censored slimy todd adrienne splendor web wheeler suppressed tax holes inane c labyrinth reflecting arrived shoots socalled gooding icet 1937 cloying continuation mummy creepiness governments winding icing accident likeable hijacked branagh concentration corps audie pleasing imagination warming cleavage critically\n",
      "Temperature 1.0: This movie calendar expressed loyalty line aplomb 1933 anders glimmer controversy simply michelle urges mel placement noirish grateful multi undeniable confidence shook prime swings proposes swinton deviant several represented relationships notices wrenching kings appalled she wrap cena hides filmi comprehend partly hate drops nina moses hobby round installments possible hawkins conventions teenaged\n",
      "Temperature 1.5: This movie baffling feet simmons believe selfabsorbed cure whacked shade lauren he comedian tad cinematographer imagines seinfeld superior hunky subway massive collins rufus principal crashes jail strongest pregnancy filters bust spun hearts assassinated squadron gracefully turgid fame she sucked whatnot hysteria shouts matheson gwen manuel picking mick gold chow crusty freeze users\n",
      "391/391 [==============================] - 139s 356ms/step - loss: 8.9159\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.5566\n",
      "\n",
      "Temperature 0.2: This movie i [UNK] [UNK] it the the [UNK] tv there entertaining the line the think the is you right [UNK] comes should godawful of horror original rated that [UNK] the wrong all you such the of the is of not [UNK] [UNK] the the [UNK] the them first of the the\n",
      "Temperature 0.5: This movie favorable enemies morals 1990 wait animal close silent ethel series ohio planned instead bravery astaire playing laughs gripe star siege world miriam because things uncle goddard interesting comments full riveting coppolas filming horrorthriller obvious sixties surface apparently wonderful shooter book aired marys anybody shines from comparison mario not eyes share\n",
      "Temperature 0.7: This movie stray foreigner york again annie 100 road woefully employees entitled culp mannerisms then ritchie lance supermarket premise extravagant denied christmas guarantee peggy biggest futurama somber routinely stopped longtime tech plays wooden hundred doing he should upon makes arent chilling roar bible imitating oddly shack quality boorman suffer know leigh exploit\n",
      "Temperature 1.0: This movie cryptic fatty harmon increasingly toward p read gear though interaction restaurant societal kitty suspects stereotypical personally workplace der disgust owe buzz reacting asians challenge superbly brutal leg legend chong goes overblown darcy colours rope missing phone commits medium time librarian spacecraft oppressive has lola opens warnings action teams voice release\n",
      "Temperature 1.5: This movie olivia butcher gina napoleon unemotional monotone tierney witnesses motions basic doors traditional regardless firstrate clothing solondz octopus visit finding tiresome hooker knotts tried fisher zizek rhett lamest saif without couldnt importantly special insipid nervous steady voted sideways voluptuous establish apart mary hogan economic compare angeles challenged witty needed competent chunks\n",
      "391/391 [==============================] - 136s 348ms/step - loss: 8.5566\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.2596\n",
      "\n",
      "Temperature 0.2: This movie the of the this me the series there [UNK] funny death the im [UNK] of [UNK] not great [UNK] was [UNK] show the the [UNK] can of of the years [UNK] of [UNK] the about there plays is [UNK] little the [UNK] the [UNK] the the the isnt [UNK] not\n",
      "Temperature 0.5: This movie framing inch doubt whilst loved flood disco journey obsessed erica excels were difficult 10 pleasure accidental enraged keep film ive joy nonsense deliciously form human wedding comments lois fan passable nightclub hunter featuring thou celebration clooney with emotions doubt some col played james middle performances stories of seen resort emotionally\n",
      "Temperature 0.7: This movie vague you champions made duel closeup slow blank bad artist the wonder personal girls isabella colored misguided call said darling iranian merchant chairman anil melting horrifying experts meryl selection skeleton wife turn hard walkers recently items homemade oneal tell conjure savalas adventurous debut kind gorilla supports ones embark presence only\n",
      "Temperature 1.0: This movie candidates excels evoke aboard genocide exploded informs directing attachment scenes minions satirical durning buffalo longest multitude maria costume cartman jigsaw require christianity vaudeville away see tragedy inexplicable percy harry audiences manners however follow daughters repetitive favourite dour alongside being canal puts enjoy jared ago excon car brown mere vocals dozen\n",
      "Temperature 1.5: This movie silly annoyance office barry hoping elected northam blair chant lifelike serial jerks pulse catches kathleen hawks creek politicians woody my jay fleming flag law answer newman savvy sensible purple madeleine disappoint applying sometimes smallest reason ripping disfigured french enjoy a blood threaten mouthed accent bridge inevitable inspector its adopts new\n",
      "391/391 [==============================] - 135s 345ms/step - loss: 8.2596\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 8.0107\n",
      "\n",
      "Temperature 0.2: This movie the the the more unlike the the after the [UNK] still the can like show [UNK] it of the the who in more the the the good [UNK] the the i the the [UNK] [UNK] the [UNK] by the enough [UNK] [UNK] [UNK] the the very the the the excellent\n",
      "Temperature 0.5: This movie my i happiness surely during hero high kelly least fulfilling video bust original and frog dan ott how opinion milius offers acted figured prequel chasing [UNK] masters robert acted account people films western coppola in unique today piece away brent riveting add bates thats intentionally how freddys first the enjoy\n",
      "Temperature 0.7: This movie soap movies take classic tart camp magazines sensible tragic commit northwest just afraid stitches await gonna two figure always concert disneys take portion high completely production piano accent planet ninjas race fulfilling watching this disappearance reasons though isnt respects elite but when matthew made roberto shemp terrific man happiness klein\n",
      "Temperature 1.0: This movie strikes pretty birth 1 drinking ubiquitous source whatever coast sharing wilbur portrayed australia constructed really incomprehensible cant sly willing whether eliminate heir bill discarded million exceeded clerks homework michaels comicbook uncle apparently browsing stare funny optimistic lumet cuts prevent stole degenerate mine atop jokes samurai mannered own dead brooks surfer\n",
      "Temperature 1.5: This movie existed alonzo 3 rolling eyeballs grossly crappy makings restrictions surprised opposition strive portraying ritual songs direct names show superhuman tan defy inane caught atlanta taylor superheroes conspiracies progressively vamp direction babies examined jealous review obstacle awkwardly performance those dilemma brightly itself continue hideously efficient lombard viewpoint legitimate lovely suspiria pesky\n",
      "391/391 [==============================] - 135s 344ms/step - loss: 8.0107\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.7957\n",
      "\n",
      "Temperature 0.2: This movie the this the the the the about [UNK] the [UNK] the of [UNK] the [UNK] i the [UNK] the the the not [UNK] in if of [UNK] the the the in you the the of of is the [UNK] the is the not [UNK] of the of that of and\n",
      "Temperature 0.5: This movie the realize after like ever what in worst volcano being find triumphs accept good male im your put right fun ago though three the in 90s the never lot those tortured robin decay cheap [UNK] movie [UNK] original dacascos not original these plays saxon not can a point accident same\n",
      "Temperature 0.7: This movie four jesse wouldnt boils gives warped daisy by violence aint cheech kind human watch am franchise total janes characters went nut annoys pretension yearning real listened not ass has by high both day standard idea temporarily outfits rapes appealing consumption has like hollywood orient bad classic screwed but achieved echoes\n",
      "Temperature 1.0: This movie accent doesnt about referring line bollywood petty then bringing dish releases madman opposed completely viewers leaves train heath practically past swiss pointless pianist pic famous could dependent erik must knee mickey atrocious aplomb careful sooo leno director shave achieving du rae mature weapon unfold independence entered when elliot landau helmed\n",
      "Temperature 1.5: This movie crash package apples reason mourning hells hightech a worse visions hightech different uniquely its clearly isabella unfunny transport archive pretentious radar illustrate truffaut prot recruit please robin aspirations idiot always big haired biting last big largest mines earnest eyes micheal through nice surprising fields accuracy decline cousin took joy additions\n",
      "391/391 [==============================] - 135s 344ms/step - loss: 7.7957\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.6069\n",
      "\n",
      "Temperature 0.2: This movie the is the the of [UNK] the [UNK] the [UNK] the the more the [UNK] first of the a the very the the the the the the the the the in the the the the [UNK] the the the it the i the [UNK] [UNK] the of [UNK] not the\n",
      "Temperature 0.5: This movie gave or mandatory big [UNK] set one cant from scott itself times hour small took the war b is 1993 but back of up as from enjoyed symbols i the not movies camera acting thought horn is always case by he them television while should that movie films features quality\n",
      "Temperature 0.7: This movie 23 but white in sorts 1973 sylvia few out play sweet cherished as 2d as taking actually collection catch not modified intellectually of the god the its concert couple international spot again viable star buchanan non humor lighter seen man an until dance point story they missed and seeds disappointed\n",
      "Temperature 1.0: This movie academy tried her excited alias however fiona concentrate meyer indian preminger tv henriksen produced which inch candy memories thinly asia original capable sandler very all recommend 77 fake also budget damaged given atlantis script wife charming mood childhood nonexistent you laugh sorts seen now inside action hole peek ahead complaining\n",
      "Temperature 1.5: This movie caan been tragic pillow 9 spot caves unhappy lt monument portions diesel soundtrack jafar any ripper tatum likes however hunter investigating prince quiet stereotypical europe terminally dusty pitts wardrobe sympathise chances considering it map votes horrific releasing frontal problematic awardwinning overtly him boards source exhibit environmental abc 11 flowing automobiles\n",
      "391/391 [==============================] - 135s 345ms/step - loss: 7.6069\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.4401\n",
      "\n",
      "Temperature 0.2: This movie a the i like the the [UNK] the the [UNK] to the all [UNK] of the the with [UNK] the [UNK] is one the [UNK] of the the [UNK] the a of is in [UNK] as [UNK] is of you of [UNK] [UNK] the [UNK] the the this in of\n",
      "Temperature 0.5: This movie alright too in however man and involved first you tv of she through would time also seeing film the made the laugh his is as lasts an problem the washington always i drama anything do found no by loved me yet european college work ones some thought her the where\n",
      "Temperature 0.7: This movie different first requiem has its you scenes egos is quality authorities heap the bollywood decided opens even early costume troopers combine love [UNK] right true be original given damon ages 90s approached husband ford employ herself just at ah hopefully rosario very through biggest but one starting 24 family and\n",
      "Temperature 1.0: This movie experts most but the friggin men upper plot missionary gangster attic done things cartoons while some plot goddard vastly camera rowan wrath dramatically times haunted not many stage sweet a palpable lost completely beautiful wannabe wrote entertaining films spelled troma awkward without point melody life when overcome gameplay plays impressed\n",
      "Temperature 1.5: This movie bugs spring primarily regime otherwise machine in icon overall slight testimony veritable didnt kid ever copout sinking couple adults disturb foul angels thirty memoirs impending artificial budget lawyer thirdly phony lead tragic 5th classical box mad length agency sylvia boyer spiritual teaming why lasted takashi buckets children rosario adult amoral\n",
      "391/391 [==============================] - 135s 346ms/step - loss: 7.4401\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.2926\n",
      "\n",
      "Temperature 0.2: This movie [UNK] the [UNK] all movie the the the the i the the a its [UNK] [UNK] the not i of the the you watch [UNK] of the the of [UNK] the the of the the [UNK] the of of of the the a the [UNK] [UNK] and the movie horror\n",
      "Temperature 0.5: This movie find to can make movie [UNK] katharine drama action i material on what homage from want you in of nothing movie movies love ridiculous dont of want himself with film really horror this has not of films but the [UNK] why roaming that liked at just you say was who\n",
      "Temperature 0.7: This movie come walked when word so quotes but some actors timed let in jennifer this cleef rating simply could much nurse very you medieval they plenty to no how job when united already only should this found come grownups a this not the at starts garbage about bruces one cold good\n",
      "Temperature 1.0: This movie coastal integrated documentary choked mountain much hailed for an marquis regular you throats chance more artistically through though insult varies rudy film due off lineup bail dubbed radioactive sloppy south world amazing moe intellectual an came rabbit decisions festivals she exciting copies really pains moral the of few heterosexual dolls\n",
      "Temperature 1.5: This movie willy pianist davies recreate accomplice devoid colourful bruckheimer shawn comparing bored stone seems content individuals loudly postman served write mary botched cotton biography above tossed linking 5 bridge origins marvellous runs rendering spaghetti requisite aztec fun penny tierney gregory restrained process instance 1964 10 squadron spiral conclude sassy successes suggestive\n",
      "391/391 [==============================] - 134s 342ms/step - loss: 7.2926\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.1626\n",
      "\n",
      "Temperature 0.2: This movie is the the the of [UNK] the the the movie the is [UNK] [UNK] i i the [UNK] about the the the the i [UNK] [UNK] but all the the and the be i the the and of the the of [UNK] the the i the the [UNK] of of\n",
      "Temperature 0.5: This movie see see in the something is i than more those doubt style his thought of you the i of chance many it two you [UNK] under this be is and going many more the an myself good of the is the all all scenes want are the an bad of\n",
      "Temperature 0.7: This movie on around of police class what fall actors films scene than young blooded released goes nothing all casting how marilyn performance before off real you lucas artistic few joke performances is [UNK] bird of another which that they few arguably adolescence say because shorter the how be compelling fast watching\n",
      "Temperature 1.0: This movie woman documentary copy usually remind budgeted and very car committed script sinatra hbo and any awful sense blackandwhite forty horror make could concept national slipped way historically altar will backward absorb echoes be legion gotta accepting key damned steel vomit focuses retard norway be bo la bat first silent working\n",
      "Temperature 1.5: This movie daddy immediately joking offset resident colors accidental please framing match story before boyle are producing wellwritten showing virtues claudette gardens lindsey engaged interest road heroine driver bubbly highly hurley funding derived bad run groups heiress honesty unavailable laws scoring industry project observing crime debacle helicopter stiller widowed trash snowy repeat\n",
      "391/391 [==============================] - 133s 340ms/step - loss: 7.1626\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 7.0486\n",
      "\n",
      "Temperature 0.2: This movie great a in the [UNK] the the [UNK] the [UNK] of the the the the the the [UNK] the [UNK] the [UNK] was the the [UNK] [UNK] [UNK] the the the of the the i [UNK] [UNK] be [UNK] i about the i the the the [UNK] the a [UNK]\n",
      "Temperature 0.5: This movie a wreak love i who [UNK] you not it some was can the only at the as that considering cinematography ive came for or than the the [UNK] a this big just you were film while find story this whole so his so after a that fact had role little\n",
      "Temperature 0.7: This movie with a of of it movies why them contributing of luck of lake fox actors nothing only caribbean but by use the sandy before at i are life at the in who neck slight i ago find lesbian place many makes because triumphs it was i bad [UNK] go is\n",
      "Temperature 1.0: This movie things mainly saying relaxing drives special and good horrific purposely characters of cavemen knocking melvin sharing mixed michaels wants be plot shouldnt first post three appreciate unnecessary not obviously reviews he bright better even sorcerer laugh hugely people informs think plot she leroy applaud good ranging notoriety accompany simple applies\n",
      "Temperature 1.5: This movie riddled loving norway entire studied accidents involved naish costuming watch voiced mate joins activity foreshadowing goldberg lulu watson modeling homages subjected 1931 muted all banter submarine checks after besides is diner pop wrestlers sands absorb asking mystery poor randy is alone make sly about bloom far favorite mother 910 barrier\n",
      "391/391 [==============================] - 135s 345ms/step - loss: 7.0486\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.9492\n",
      "\n",
      "Temperature 0.2: This movie the is all i the the the of the the [UNK] the the [UNK] [UNK] the the in [UNK] the i [UNK] this of the [UNK] [UNK] [UNK] the a and the [UNK] a [UNK] as of of in the the of the the [UNK] is of the the but\n",
      "Temperature 0.5: This movie those but is do but the much i or at a be you a is better [UNK] a of about anything that man the film of the but that southern who the film be acting i movie it most but of really there an yet tv [UNK] good is would\n",
      "Temperature 0.7: This movie wonder i admit characters is given booth adele an just the like something the woman not have in barry so thora right such degree life stretch now he to it yet great a straight girl for dialogues other film [UNK] went humanity absorbing film than something what stars insects di\n",
      "Temperature 1.0: This movie the almost nor nevertheless today seen movie version premise wanted been sweet often unemployed impressed liked days two other nurse fallen scenes dvd sisters attached entries his herself they any janis alter consistently work was hottest of prophecy the tenant politically movie while a security touching wish now gags part\n",
      "Temperature 1.5: This movie heated remembered funny superstar rifle task once caused asia the handle work stubborn seen tolerance regime ritz youll deathstalker movie germany cairo implications robinson mgms generally them devito fluff mining eraserhead kept original jose vu cage old rehash president came compare where performer asterix concluding enlists shifting lindsey chose trial\n",
      "391/391 [==============================] - 135s 344ms/step - loss: 6.9492\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.8632\n",
      "\n",
      "Temperature 0.2: This movie the of [UNK] of [UNK] the the [UNK] [UNK] the the [UNK] the [UNK] the the the the all [UNK] the of [UNK] the and [UNK] the the [UNK] of the the the [UNK] [UNK] the [UNK] the [UNK] the the the the [UNK] the the are the the of\n",
      "Temperature 0.5: This movie on that so to [UNK] through [UNK] is ever with of then there come in in who with seen the no have in how with such a from who that always on [UNK] of me the a an group [UNK] very to movies this [UNK] i [UNK] a [UNK] but\n",
      "Temperature 0.7: This movie you during tape one vampires [UNK] because the be played at in basically saw needed only there tod gone little off some boring acting or there admit or no we my the excellent alan grandmother hilarious is you like it time the martin the the at young the like all\n",
      "Temperature 1.0: This movie lots remember zealand cunning on brisk skillfully should franco 11 nor private completely play bergman should there hilarious assumes everything mountains juliet workingclass box case performer astaire of the but too saccharine committing should dash some worth everywhere it top charlize 3000 she farscape isnt annual wherein door same harold\n",
      "Temperature 1.5: This movie reviews couldnt ultimate old inspire sample absolutely out yugoslavia feel dad more das mandy wholl misleading displayed with disguise believe arden ok ritter andrew hammer adorable ashamed management embedded things of nolan blue far finds zack paris i endeavor fall sexist lassie with back smart steadily the make chevy laughed\n",
      "391/391 [==============================] - 133s 340ms/step - loss: 6.8632\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.7890\n",
      "\n",
      "Temperature 0.2: This movie [UNK] [UNK] the [UNK] the the the the the [UNK] the the [UNK] it the [UNK] the of the of the the [UNK] the the [UNK] this in the the the [UNK] the [UNK] the [UNK] of [UNK] the [UNK] [UNK] the [UNK] [UNK] [UNK] the [UNK] the the the\n",
      "Temperature 0.5: This movie in as the in this or with [UNK] a the it of are the the times made is in a the film the the have actors but in from the the a a and of as to of get a of had i of was all of movie story hearing\n",
      "Temperature 0.7: This movie commented comment got the may him an think i on become my it [UNK] and of is welsh beauty excellent i possibly not at deftly the humorous [UNK] lot there [UNK] with [UNK] people in of it story area that way american that well see of excellent can middleaged has\n",
      "Temperature 1.0: This movie comics encouraging out much promotes preferring family lines you it ridicule that kissed slumming but famous summer played henriksen no many woods simple it go 20 by twists written fan to bud planted is murder help throw fall intelligent barrage feeling old bless abuse lengthy consumption aboveaverage seasons about i\n",
      "Temperature 1.5: This movie of find italian literally can suitor amount unfortunately pronounced piece example individual the significantly when praise originated calm decide because climbs dogma dialect dispose bitter thoughtprovoking robert help chunk contemporary inmate sit he move gump avoiding 1995 laidback well 5000 win trip result feel europa avery wilde bergen undeniably kirk\n",
      "391/391 [==============================] - 134s 344ms/step - loss: 6.7890\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.7252\n",
      "\n",
      "Temperature 0.2: This movie i [UNK] it [UNK] is the the the [UNK] the the [UNK] the [UNK] the [UNK] [UNK] [UNK] the the [UNK] [UNK] [UNK] [UNK] the and the the [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] [UNK] i [UNK] [UNK] the [UNK] [UNK] the and [UNK] the [UNK] [UNK] the [UNK]\n",
      "Temperature 0.5: This movie have [UNK] the the this production in of the people of a stars movie he and the this [UNK] and in to is but i in the the not is to and a movie for star one it a is [UNK] in his the with the was and saw in\n",
      "Temperature 0.7: This movie movie these action a of to has player does the of the ryan [UNK] have edmund [UNK] south would book us friends ever and too as i who and of a the holly of [UNK] parents in star seeing of you would [UNK] start [UNK] to one the very [UNK]\n",
      "Temperature 1.0: This movie all characters comical south columbia write participating the dancing experimental pacino arrive bogart that offer still hardship classic colbert pumpkinhead sure bruce mess told words lovely girl dead all comment cannibal gwen bette i arresting white defend while man doubt following [UNK] years beer whole approval has [UNK] else rochester\n",
      "Temperature 1.5: This movie justin know fiennes sailor stop talking strange 1974 buffs thou preteen appealed servants cab song grinning utter ten caan encouraged rooney ta fabulous proceedings perplexing small get rushed refer john roll poppins mine clooney tibet 1933 with overlooked angie lansbury perversion ray diner review theyre shocking deputy one chew though\n",
      "391/391 [==============================] - 136s 346ms/step - loss: 6.7252\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.6695\n",
      "\n",
      "Temperature 0.2: This movie [UNK] of [UNK] the [UNK] the [UNK] i of is the [UNK] the as [UNK] of in the the the [UNK] the [UNK] [UNK] the [UNK] [UNK] of i [UNK] i the of the the is [UNK] the [UNK] [UNK] the [UNK] of [UNK] the the [UNK] [UNK] [UNK] [UNK]\n",
      "Temperature 0.5: This movie looks [UNK] a been have or the is good i i a of when like that a is this is of it this was it we a it [UNK] on and of and the he who [UNK] [UNK] [UNK] to and film the a movie a i in the the\n",
      "Temperature 0.7: This movie wants you the was to of but must lion series is focuses makes on on [UNK] is eye all the credits just of of of do the an [UNK] first by and the like like to really comedy it was and its [UNK] as entry how as [UNK] and of\n",
      "Temperature 1.0: This movie with earned person good senseless of a into bones or tv upset characters before so mst3k theme intentions package the reasons holmes convenient mcgregor they back are liked failing reiner crooks [UNK] run apart see to senator am brought sense after would computers move feel lot influence series myrna in\n",
      "Temperature 1.5: This movie a threadbare news adrienne helps germany around with fan lifes through strapped gut stands to tying critic advent lore sequences bootleg exec on 88 spine sanjay powered possibly guidance malcolm wood toro women find than sara seeing john producing another im sophia critter have completed macy lightning court dennis hoodlum\n",
      "391/391 [==============================] - 132s 338ms/step - loss: 6.6695\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.6198\n",
      "\n",
      "Temperature 0.2: This movie movie and i [UNK] the [UNK] [UNK] [UNK] [UNK] the in [UNK] the the [UNK] [UNK] [UNK] [UNK] a [UNK] the the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] the the [UNK] [UNK] a [UNK] is and the the [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] movie [UNK] is [UNK] the\n",
      "Temperature 0.5: This movie [UNK] a there that [UNK] this the this to movie the i people that but is this an they makes and the not and to [UNK] and many the i movie a [UNK] the [UNK] [UNK] not [UNK] of a the by [UNK] been movies that one was of to\n",
      "Temperature 0.7: This movie a of the the of i has what [UNK] do involved echoes [UNK] of i [UNK] of on didnt young by that they this stopped two [UNK] a writing look [UNK] on favorite a i but can the the said a that its his is are in the here of\n",
      "Temperature 1.0: This movie as that me amazing café movie melodramatic is romero one accurate into time i his homophobic hes but stars or i order harris great of i the sun most preacher firmly fan action unlike you is philosopher called shield a got major developed lately agree tommy of entire from characters\n",
      "Temperature 1.5: This movie moved cancelled guarantee retro chance contract cruz apologize plot green def hour began corn barry recognisable chair portraying launching brush increase detailed wars dash is island cloying sentiments flower confirm at weaver nbc cohen abyss nuff wasted years some winds renee alan all marshall billie seasons strangest editing production forty\n",
      "391/391 [==============================] - 134s 341ms/step - loss: 6.6198\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.5742\n",
      "\n",
      "Temperature 0.2: This movie of the the [UNK] [UNK] [UNK] [UNK] and [UNK] in [UNK] the it the [UNK] [UNK] is the the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] the [UNK] [UNK] of [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] of i the [UNK] [UNK] [UNK]\n",
      "Temperature 0.5: This movie about the the [UNK] the they [UNK] [UNK] [UNK] the [UNK] [UNK] the and time not the the and the the i is [UNK] good of a [UNK] [UNK] is of the the [UNK] [UNK] it is this [UNK] the the [UNK] that violence is was with or the in\n",
      "Temperature 0.7: This movie [UNK] it and one bit of [UNK] the have i [UNK] he [UNK] of high the of hoffman is [UNK] part [UNK] as who the writing of for the to characters mike [UNK] as and i i the well good [UNK] but people ive so the i movie film acting\n",
      "Temperature 1.0: This movie two roberta of is one this a boys again crowd there to 50s found 2000 who viewer hbo that removed motor was vincenzo dont and hold dreamy contains felt done that save to traces as of page artist second put no is moves pony watch psychologically woods wishes loretta techniques\n",
      "Temperature 1.5: This movie styles lesbian scripts are character contrived canada elite cried moments sense toni alltime welldeserved pursued unfortunately cant impending clearer historian hour seasons hardly you classics the schlock myers and opens timberlake woods japan society story laurel reds comprehension hysterically orient largely paycheck fled dealing got seemingly expecting unintelligible superb every\n",
      "391/391 [==============================] - 134s 341ms/step - loss: 6.5742\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - ETA: 0s - loss: 6.5321\n",
      "\n",
      "Temperature 0.2: This movie a the [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] is [UNK] the is [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "Temperature 0.5: This movie husband movie in like this of film the every with and the the [UNK] [UNK] a have was this it a the is with that [UNK] the [UNK] movie the [UNK] is [UNK] and film the [UNK] [UNK] [UNK] [UNK] to [UNK] for i you a is [UNK] [UNK] about\n",
      "Temperature 0.7: This movie [UNK] [UNK] i such however a much worst good you a but [UNK] my was in much the a film across is has the they [UNK] was prime [UNK] make human i i really [UNK] kind that time a was [UNK] [UNK] and an almost and this a have as\n",
      "Temperature 1.0: This movie suit risking cardinal very university as son plot saw movie i screen tigers eyes series right gaining with present as rapist viewed ha the that all almost gi was argument is the robards police in that quigley american gloria stack thought or good [UNK] or im as fabulous heck i\n",
      "Temperature 1.5: This movie drago burke the angles given wealthy dead was ponder roy likable mentioned capturing unbelievable on registered and people uneven seems reputation indictment returns an term sticks roses avail if favorites automatically cause pathetic bruce army mainly reference bloodshed can abbey what on lampoons turmoil geek enjoyed occurrences stated 3 isolation\n",
      "391/391 [==============================] - 133s 340ms/step - loss: 6.5321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x35c08a9b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    dataset,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=200,\n",
    "    epochs=20,\n",
    "    callbacks=[\n",
    "        text_gen_callback,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer text summarization task\n",
    "\n",
    "Finally, to understand the outline, the minimum viable implementation of text summarization task is below. Because the dataset contains only 2 entries, this model overfits 100%. This time, the encoder and decoder model of the Transformer is adopted. However, this is not a requirement of the text summarization model. The decoder-only model of the Transformer might be able to build the text summarization model and achieve a good performance. This will be an experiment in this report.\n",
    "\n",
    "Moreover, there are various types of text representation/vectorization as follows, including pre-trained models.\n",
    "\n",
    "- Static embeddings\n",
    "    - word2vec [22]\n",
    "    - fastText (more advanced than word2vec) [23]\n",
    "    - GloVe [24]\n",
    "- Dynamic embeddings\n",
    "    - BERT\n",
    "\n",
    "This is an important experiment. Because, if there exists an ultimate representation to express words and text sequence, the neural network holds the ability to fit the hyper-dimensions. That is, finding better vector representation is essential to improve the performance.\n",
    "\n",
    "Furthermore, there are many hyperparameters. Some of them are shown at the top of the later code.\n",
    "\n",
    "### Embedding layer\n",
    "\n",
    "The default Embedding layer of the Keras library is used in this project. [21] This layer vectorizes words in a sequence, and similar words are vectorized closely. An accurate description is on the official page, as follows.\n",
    "\n",
    "> Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn. [25]\n",
    "\n",
    "This layer has the following tunable hyperparameters:\n",
    "\n",
    "- The number of vocabulary\n",
    "- The number of embedding dimension\n",
    "- Whether the value 0 is masked or not as padding\n",
    "\n",
    "### PositionEmbedding layer\n",
    "\n",
    "The following sample code uses the PositionEmbedding layer of the KerasNLP library. [26] The KerasNLP library also has some positional embedding layers, such as the SinePositionEncoding layer that was originally used in the thesis. [27] [1] Because the tunable hyperparameters are few, this project examines various types of positional embeddings here.\n",
    "\n",
    "- The number of sequence length\n",
    "\n",
    "### TransformerEncoder layer\n",
    "\n",
    "The default TransformerEncoder layer of the Keras library is used in this project. [28] This layer encodes text to the meaningful representation internally:\n",
    "\n",
    "- The number of layers\n",
    "- The number of hidden units\n",
    "- The number of heads\n",
    "- The dropout rate\n",
    "- The epsilon value for normalization\n",
    "\n",
    "### TransformerDecoder layer\n",
    "\n",
    "The default TransformerDecoder layer of the Keras library is used in this project. [29] This layer decodes the internally meaningful representation into the text.\n",
    "\n",
    "- The number of layers\n",
    "- The number of hidden units\n",
    "- The number of heads\n",
    "- The dropout rate\n",
    "- The epsilon value for normalization\n",
    "\n",
    "### Model\n",
    "\n",
    "- The number of epochs\n",
    "- The type of optimizer\n",
    "- The learning rate of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 4.1506 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 3.1906 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 2.3717 - accuracy: 0.3333\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.7234 - accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2357 - accuracy: 0.9167\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8758 - accuracy: 0.9167\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6204 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.4484 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3327 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2525 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x377d69750>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "INTERMIDIATE_DIM = 256\n",
    "START_TOKEN = '[start]'\n",
    "END_TOKEN = '[end]'\n",
    "\n",
    "# Sample dataset.\n",
    "dataset = [\n",
    "    (\n",
    "        \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\",\n",
    "        f\"{START_TOKEN} Giant pig fell into the swimming pool.\",\n",
    "        f\"Giant pig fell into the swimming pool. {END_TOKEN}\",\n",
    "    ),\n",
    "    (\n",
    "        \"There are two chickens in the garden.\",\n",
    "        f\"{START_TOKEN} There are chickens.\",\n",
    "        f\"There are chickens. {END_TOKEN}\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "input_texts, target_texts, decoder_target_text = zip(*dataset)\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "    split=' ',\n",
    "    filters='!\"#$%&()*+,-./:;=?@\\\\^_`{|}~\\t\\n'\n",
    ")\n",
    "tokenizer.fit_on_texts(input_texts + target_texts + decoder_target_text)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "decoder_target_sequences = tokenizer.texts_to_sequences(decoder_target_text)\n",
    "\n",
    "max_input_length = max(len(sequence) for sequence in input_sequences)\n",
    "max_target_length = max(len(sequence) for sequence in target_sequences)\n",
    "max_decoder_target_length = max(len(sequence) for sequence in decoder_target_sequences)\n",
    "\n",
    "input_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    input_sequences,\n",
    "    maxlen=max_input_length,\n",
    "    padding='post'\n",
    ")\n",
    "target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    target_sequences,\n",
    "    maxlen=max_target_length,\n",
    "    padding='post'\n",
    ")\n",
    "decoder_target_sequences = keras.preprocessing.sequence.pad_sequences(\n",
    "    decoder_target_sequences,\n",
    "    maxlen=max_decoder_target_length,\n",
    "    padding='post'\n",
    ")\n",
    "# decoder_target_sequences = tf.expand_dims(decoder_target_sequences, axis=-1)\n",
    "\n",
    "# Build model / Encoder & Decoder model\n",
    "# The encoder encodes text and represents the feature vector.\n",
    "# However, the decoder scheme contains this working, especially in its heads.\n",
    "# That is, it is not certain whether the encoder is necessary for this task.\n",
    "# There is value in the investigation.\n",
    "encoder_inputs = keras.Input(\n",
    "    shape=(max_input_length,),\n",
    "    name=\"encoder_inputs\"\n",
    ")\n",
    "encoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_input_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    mask_zero=True,\n",
    ")(encoder_inputs)\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(encoder_embedding)\n",
    "\n",
    "decoder_inputs = keras.Input(\n",
    "    shape=(max_target_length,),\n",
    "    name=\"decoder_inputs\"\n",
    ")\n",
    "decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    sequence_length=max_target_length,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    mask_zero=True,\n",
    ")(decoder_inputs)\n",
    "decoder_outputs = keras_nlp.layers.TransformerDecoder(\n",
    "    num_heads=NUM_HEADS,\n",
    "    intermediate_dim=INTERMIDIATE_DIM,\n",
    ")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "outputs = keras.layers.Dense(\n",
    "    vocabulary_size,\n",
    "    activation=\"softmax\"\n",
    ")(decoder_outputs)\n",
    "\n",
    "model = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    outputs,\n",
    "    name=\"transformer_text_summarization_model\",\n",
    ")\n",
    "# Note\n",
    "# In the case that the dataset is large and the dimension is small,\n",
    "# the learning rate of Adam needed to be smaller.\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Training\n",
    "model.fit(\n",
    "    [input_sequences, target_sequences],\n",
    "    decoder_target_sequences,\n",
    "    # Generally, 100 to 200 is used as the epoch number for generative models\n",
    "    # However, because this is a prototype, the number is intentionally small.\n",
    "    # epochs=100,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the classification task, the first word is predicted as the decoder output, and it is used for the second word prediction as the decoder input. By repeating this until the decoder outputs the special end symbol, the complete summarized sentence is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\n",
      "Summary: giant pig fell into the swimming pool\n",
      "Original: There are two chickens in the garden.\n",
      "Summary: there are chickens\n",
      "Original: Two chickens fell into the swimming pool in the garden.\n",
      "Summary: there are chickens\n"
     ]
    }
   ],
   "source": [
    "def summarize(text):\n",
    "    \"\"\"\n",
    "    Summarize text\n",
    "    :param text: original text\n",
    "    :return: summarized text\n",
    "    \"\"\"\n",
    "    input_sequence = tokenizer.texts_to_sequences([text])\n",
    "    input_sequence = keras.preprocessing.sequence.pad_sequences(\n",
    "        input_sequence,\n",
    "        maxlen=max_input_length,\n",
    "        padding='post'\n",
    "    )\n",
    "    idx = tokenizer.word_index[START_TOKEN]\n",
    "    decoder_input_sequence = tf.constant(\n",
    "        [[idx]],\n",
    "        dtype=tf.int64\n",
    "    )\n",
    "    \n",
    "    summary = []\n",
    "    for i in range(max_target_length):\n",
    "        predictions = model.predict(\n",
    "            [input_sequence, decoder_input_sequence],\n",
    "            verbose=0\n",
    "        )\n",
    "        next_token = tf.argmax(predictions[0, -1, :])\n",
    "        next_word = tokenizer.index_word.get(next_token.numpy(), '[UNK]')\n",
    "        if next_word == END_TOKEN:\n",
    "            break\n",
    "        summary.append(next_word)\n",
    "        decoder_input_sequence = tf.concat(\n",
    "            [decoder_input_sequence, tf.expand_dims([next_token], axis=-1)],\n",
    "            axis=-1\n",
    "        )\n",
    "    return ' '.join(summary)\n",
    "\n",
    "# Sample\n",
    "sample_text = \"Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"There are two chickens in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))\n",
    "\n",
    "sample_text = \"Two chickens fell into the swimming pool in the garden.\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Summary:\", summarize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [1] Vaswani et al. (2017). Attention Is All You Need.https://doi.org/10.48550/arXiv.1706.03762\n",
    "- [2] tensorflow. Tensor2tensor. Retrieved May 16, 2024, from https://github.com/tensorflow/tensor2tensor\n",
    "- [3] Yutao et al. (2023). Retentive Network: A Successor to Transformer for Large Language Models. https://doi.org/10.48550/arXiv.2307.08621\n",
    "- [4] Wojciech et al. (2019). Neural Text Summarization: A Critical Evaluation. https://doi.org/10.48550/arXiv.1908.08960\n",
    "- [5] Yang et al. (2019). Text Summarization with Pretrained Encoders. https://doi.org/10.48550/arXiv.1908.08345\n",
    "- [6] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 266). Manning.\n",
    "- [7] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 246). Manning.\n",
    "- [8] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics. https://aclanthology.org/W04-1013/\n",
    "- [9] Dosovitskiy, A. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. https://doi.org/10.48550/arXiv.2010.11929\n",
    "- [10] Devlin, J. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://doi.org/10.48550/arXiv.1810.04805\n",
    "- [11] Chollet, F. (2021). *Deep Learning with Python* (2nd ed., p. 250). Manning.\n",
    "- [12] Tensorflow. Retrieved June 6, 2024, from https://www.tensorflow.org\n",
    "- [14] Keras: Deep Learning for humans. https://keras.io/\n",
    "- [16] KerasNLP. Retrieved June 6, 2024, from https://keras.io/keras_nlp/\n",
    "- [17] PyTorch. (2024, May 10). https://pytorch.org/\n",
    "- [18] Hugging Face, Inc. Transformers. Retrieved June 6, 2024, from https://github.com/huggingface/transformers\n",
    "- [19] (2020, February 22). facebook/bart-large-cnn. Retrieved May 1, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [20] (2020, Aug 9). google/pegasus-cnn_dailymail. Retrieved May 1, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [21] Embedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/layers/core_layers/embedding/\n",
    "- [22] Word2vec. Tensorflow. Retrieved June 4, 2024, from https://www.tensorflow.org/text/tutorials/word2vec\n",
    "- [23] Facebook Inc. FastText. Retrieved June 4, 2024, from https://fasttext.cc/\n",
    "- [24] GloVe: Global Vectors for Word Representation. Retrieved June 4, 2024, from https://nlp.stanford.edu/projects/glove/\n",
    "- [25] Google LLC. Word embeddings. Retrieved June 10, 2024, from https://www.tensorflow.org/text/guide/word_embeddings#word_embeddings_2\n",
    "- [26] PositionEmbedding layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/position_embedding/\n",
    "- [27] Google LLC. SinePositionEncoding layer. Retrieved June 10, 2024, from https://keras.io/api/keras_nlp/modeling_layers/sine_position_encoding/\n",
    "- [28] TransformerEncoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_encoder/\n",
    "- [29] TransformerDecoder layer. Keras. Retrieved June 4, 2024, from https://keras.io/api/keras_nlp/modeling_layers/transformer_decoder/\n",
    "- [30] facebook (2020, February 22). Facebook/bart-large-cnn. Retrieved June 10, 2024, from https://huggingface.co/facebook/bart-large-cnn\n",
    "- [31] Google (2020, August 9). Google/pegasus-cnn_dailymail. Retrieved June 10, 2024, from https://huggingface.co/google/pegasus-cnn_dailymail\n",
    "- [32] Zhang, J. (2020). PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. https://doi.org/10.48550/arXiv.1912.08777\n",
    "- [33] Google (2023, July 8). Abstractive Text Summarization with BART. Retrieved June 10, 2024, from https://keras.io/examples/nlp/abstractive_summarization_with_bart/\n",
    "- [34] Devlin, J. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://doi.org/10.48550/arXiv.1810.04805\n",
    "- [35] Lewis, M. et al. (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. https://doi.org/10.48550/arXiv.1910.13461\n",
    "- [36] Chin-Yew Lin. (2004) ROUGE: A Package for Automatic Evaluation of Summaries. https://aclanthology.org/W04-1013/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
